{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a97d3321-4427-4837-8269-048dc64686f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from feature.scalers import ranged_scaler\n",
    "from feature.engineering import *\n",
    "from datetime import datetime, timedelta\n",
    "#from mpge.rca import mpge_root_cause_diagnosis\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from CARAT.model_utils import *\n",
    "from CARAT.model import CausalGraphVAE\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, GATConv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch.distributions import Normal, Laplace, RelaxedOneHotCategorical\n",
    "from torchdiffeq import odeint  # For continuous-time normalizing flows\n",
    "from CARAT.components import *\n",
    "from CARAT.model_utils import *\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from utils.utils import set_seed\n",
    "from utils.utils import logger\n",
    "\n",
    "import warnings\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "def gxn():\n",
    "    warnings.warn(\"RuntimeWarning\", RuntimeWarning)\n",
    "\n",
    "RuntimeWarning\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "\n",
    "# Or if you are using > Python 3.11:\n",
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    fxn()\n",
    "    gxn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68ef600a-406e-456a-98ac-8388b03ccc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f4205996dd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "torch.backends.cudnn.benchmark=False\n",
    "torch.autograd.profiler.profile(enabled=False)\n",
    "torch.autograd.profiler.emit_nvtx(enabled=False)\n",
    "torch.autograd.set_detect_anomaly(mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6981e0a1-e1c0-409d-ab6a-cf7d34bf1793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000000, 20)\n"
     ]
    }
   ],
   "source": [
    "cats_df = pl.read_csv(\"data/data.csv\", separator=\",\")  \n",
    "print(cats_df.shape)\n",
    "metadata = pl.read_csv('data/metadata.csv',separator=',')\n",
    "potential_causes = metadata['root_cause'].unique().to_list()\n",
    "\n",
    "for col in cats_df.columns:\n",
    "    unique_vals = cats_df[col].n_unique()\n",
    "    data_type = cats_df[col].dtype\n",
    "    bad_dtypes = [pl.Date,pl.Datetime,pl.Utf8]\n",
    "    if ((unique_vals >= 50) & (data_type not in bad_dtypes) ):\n",
    "        cats_df = cats_df.with_columns(ranged_scaler(cats_df[col]))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df0f5a4d-1cf3-4a87-a3fb-751607296c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_df = cats_df.with_columns(\n",
    "    pl.col('timestamp').str.to_datetime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    pl.Series(\"entity_id\",range(len(cats_df)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48b9af15-9315-41d4-86fe-907430746a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000000, 21)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_rows_list = metadata.rows(named=True)\n",
    "cats_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52959448-4391-42ad-a59e-e0f732b1971f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping column timestamp (not a float column)\n",
      "Column: aimp | Final ADF: -2211.6294 | p-value: 0.0000 | Diffs: 0\n",
      "Column: amud | Final ADF: -8.0091 | p-value: 0.0000 | Diffs: 0\n",
      "Column: arnd | Final ADF: -18.1248 | p-value: 0.0000 | Diffs: 0\n",
      "Column: asin1 | Final ADF: -5.7565 | p-value: 0.0000 | Diffs: 1\n",
      "Column: asin2 | Final ADF: -39.8976 | p-value: 0.0000 | Diffs: 3\n",
      "Column: adbr | Final ADF: -111.5305 | p-value: 0.0000 | Diffs: 0\n",
      "Column: adfl | Final ADF: -217.2031 | p-value: 0.0000 | Diffs: 0\n",
      "Column: bed1 | Final ADF: -35.0214 | p-value: 0.0000 | Diffs: 0\n",
      "Column: bed2 | Final ADF: -44.5328 | p-value: 0.0000 | Diffs: 0\n",
      "Column: bfo1 | Final ADF: -8.5614 | p-value: 0.0000 | Diffs: 0\n",
      "Column: bfo2 | Final ADF: -13.3015 | p-value: 0.0000 | Diffs: 0\n",
      "Column: bso1 | Final ADF: -2164.1026 | p-value: 0.0000 | Diffs: 1\n",
      "Column: bso2 | Final ADF: -13.5707 | p-value: 0.0000 | Diffs: 0\n",
      "Column: bso3 | Final ADF: -14.7270 | p-value: 0.0000 | Diffs: 0\n",
      "Column: ced1 | Final ADF: -727.3289 | p-value: 0.0000 | Diffs: 1\n",
      "Column: cfo1 | Final ADF: -6.7640 | p-value: 0.0000 | Diffs: 0\n",
      "Column: cso1 | Final ADF: -7.8526 | p-value: 0.0000 | Diffs: 0\n",
      "Column: y | Final ADF: -51.2989 | p-value: 0.0000 | Diffs: 0\n",
      "Column: category | Final ADF: -53.9463 | p-value: 0.0000 | Diffs: 0\n",
      "⚠️ Skipping column entity_id (not a float column)\n"
     ]
    }
   ],
   "source": [
    "cats_df = make_stationary(cats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6b4cd0e-cacc-4d93-b0e9-a0cdca28b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_df = cats_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e03dcda-4108-4fe2-9ec3-8ccbdeaf85f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['ced1']\", \"['cso1']\", \"['cfo1']\"]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata['affected'].unique().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc297da3-97f8-4077-898e-cc23b8c2e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_causes = metadata['root_cause'].unique().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8be12f8-8ca1-49a7-81aa-5e05325023a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aimp</th>\n",
       "      <th>amud</th>\n",
       "      <th>arnd</th>\n",
       "      <th>asin1</th>\n",
       "      <th>asin2</th>\n",
       "      <th>adbr</th>\n",
       "      <th>adfl</th>\n",
       "      <th>bed1</th>\n",
       "      <th>bed2</th>\n",
       "      <th>bfo1</th>\n",
       "      <th>bfo2</th>\n",
       "      <th>bso1</th>\n",
       "      <th>bso2</th>\n",
       "      <th>bso3</th>\n",
       "      <th>ced1</th>\n",
       "      <th>cfo1</th>\n",
       "      <th>cso1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:00:06</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.444480</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>-7.999823e-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.32802</td>\n",
       "      <td>-0.369237</td>\n",
       "      <td>-0.738163</td>\n",
       "      <td>-0.767181</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>-0.507953</td>\n",
       "      <td>-0.716059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100401</td>\n",
       "      <td>-0.186461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:00:07</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.446078</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>-7.999823e-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.32802</td>\n",
       "      <td>-0.369237</td>\n",
       "      <td>-0.738163</td>\n",
       "      <td>-0.767181</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>-0.507953</td>\n",
       "      <td>-0.716059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100408</td>\n",
       "      <td>-0.186406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:00:08</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.447166</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>-8.000267e-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.32802</td>\n",
       "      <td>-0.369237</td>\n",
       "      <td>-0.738163</td>\n",
       "      <td>-0.767181</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>-0.507953</td>\n",
       "      <td>-0.716059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100416</td>\n",
       "      <td>-0.186345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:00:09</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.442843</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>-7.999823e-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.32802</td>\n",
       "      <td>-0.369237</td>\n",
       "      <td>-0.738163</td>\n",
       "      <td>-0.767181</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>-0.507953</td>\n",
       "      <td>-0.716059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100426</td>\n",
       "      <td>-0.186278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:00:10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.441320</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>-8.000045e-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.32802</td>\n",
       "      <td>-0.369237</td>\n",
       "      <td>-0.738163</td>\n",
       "      <td>-0.767181</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-0.507953</td>\n",
       "      <td>-0.716059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100438</td>\n",
       "      <td>-0.186205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     aimp      amud      arnd    asin1         asin2  adbr  \\\n",
       "timestamp                                                                    \n",
       "2023-01-01 00:00:06   0.0  0.142857 -0.444480  0.00002 -7.999823e-12   0.0   \n",
       "2023-01-01 00:00:07   0.0  0.142857 -0.446078  0.00002 -7.999823e-12   0.0   \n",
       "2023-01-01 00:00:08   0.0  0.142857 -0.447166  0.00002 -8.000267e-12   0.0   \n",
       "2023-01-01 00:00:09   0.0  0.142857 -0.442843  0.00002 -7.999823e-12   0.0   \n",
       "2023-01-01 00:00:10   0.0  0.142857 -0.441320  0.00002 -8.000045e-12   0.0   \n",
       "\n",
       "                     adfl     bed1      bed2      bfo1      bfo2      bso1  \\\n",
       "timestamp                                                                    \n",
       "2023-01-01 00:00:06   0.0 -0.32802 -0.369237 -0.738163 -0.767181  0.000072   \n",
       "2023-01-01 00:00:07   0.0 -0.32802 -0.369237 -0.738163 -0.767181  0.000083   \n",
       "2023-01-01 00:00:08   0.0 -0.32802 -0.369237 -0.738163 -0.767181  0.000094   \n",
       "2023-01-01 00:00:09   0.0 -0.32802 -0.369237 -0.738163 -0.767181  0.000104   \n",
       "2023-01-01 00:00:10   0.0 -0.32802 -0.369237 -0.738163 -0.767181  0.000113   \n",
       "\n",
       "                         bso2      bso3  ced1      cfo1      cso1  \n",
       "timestamp                                                          \n",
       "2023-01-01 00:00:06 -0.507953 -0.716059   0.0  0.100401 -0.186461  \n",
       "2023-01-01 00:00:07 -0.507953 -0.716059   0.0  0.100408 -0.186406  \n",
       "2023-01-01 00:00:08 -0.507953 -0.716059   0.0  0.100416 -0.186345  \n",
       "2023-01-01 00:00:09 -0.507953 -0.716059   0.0  0.100426 -0.186278  \n",
       "2023-01-01 00:00:10 -0.507953 -0.716059   0.0  0.100438 -0.186205  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_df=cats_df.set_index('timestamp')\n",
    "cats_df = cats_df.drop(['y','category','entity_id'],axis=1)\n",
    "cats_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c816fec8-c143-4f7e-a78c-1b1b18f32a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4999994, 17)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6b15ba7-0bab-44e8-95aa-c6b534da9748",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = cats_df[0:1000000]\n",
    "test_df = cats_df[1000000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7eaa704-a473-421e-acaf-0aff6efe4fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af75df75-c4c5-47d3-8f48-748b82fc6160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda:1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22ef7544-f092-4051-9ee2-f0894d2a42c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab4ff270-6892-43e6-8571-b623acdd105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_df = train_df.drop('time',axis=1)\n",
    "except:\n",
    "    None\n",
    "try:\n",
    "    test_df = test_df.drop('time',axis=1)\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfc22130-f5dd-4ef8-8d1b-e29c4f26b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(test_df.columns)\n",
    "non_causal_columns = list(set(cols).difference(set(potential_causes)))\n",
    "causal_indices = [train_df.columns.get_loc(col) for col in potential_causes]\n",
    "non_causal_indices = [train_df.columns.get_loc(col) for col in non_causal_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbc701ea-9858-4664-80b2-2caf4f8cbafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_tensor(tensor):\n",
    "  \"\"\"Scales a PyTorch tensor to the range [0, 1].\n",
    "\n",
    "  Args:\n",
    "    tensor: The input tensor.\n",
    "\n",
    "  Returns:\n",
    "    A new tensor with values scaled to [0, 1].\n",
    "  \"\"\"\n",
    "  min_val = tensor.min()\n",
    "  max_val = tensor.max()\n",
    "  scaled_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "  return scaled_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eebb432e-a758-4045-ad93-c5ad7cb19bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining on nominal data...\n",
      "Epoch 1: Loss = 2174905900929436928.0000\n",
      "Recon Loss =1423468.1250, KL Loss = 3.9480, Sparsity Loss = 502.9736, Lagrangian Loss = 165498850990096384.0000, Likelihood Loss = 0.0000\n",
      "Epoch 251: Loss = 630838877434675.2500\n",
      "Recon Loss =1671.8445, KL Loss = 6.1807, Sparsity Loss = 434.0810, Lagrangian Loss = 627157835448320.0000, Likelihood Loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "TIME_STEPS = 3\n",
    "BATCH_SIZE = 100000\n",
    "hidden_dim = 500\n",
    "latent_dim = 8\n",
    "device = torch.device(\"cuda:1\")\n",
    "num_nodes = 17\n",
    "\n",
    "dataset_nominal = TimeSeriesDataset(train_df, time_steps=TIME_STEPS)\n",
    "dataloader_nominal = DataLoader(dataset_nominal, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = CausalGraphVAE(input_dim=train_df.shape[1], hidden_dim=hidden_dim,\n",
    "                        latent_dim=latent_dim, num_nodes=train_df.shape[1],\n",
    "                        time_steps=TIME_STEPS, prior_adj=None).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "# Train on nominal data\n",
    "print(\"Pretraining on nominal data...\")\n",
    "model.train_model(dataloader_nominal, optimizer, num_epochs=500, patience=20)\n",
    "\n",
    "# Extract learned adjacency\n",
    "prior_adj = model.causal_graph.edge_score_now.clone().detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d35de27-37ce-4711-9b1f-c052549cdbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_adj = model.causal_graph.edge_score_now.clone().detach()\n",
    "scaled_prior = scale_tensor(prior_adj)\n",
    "for i, row in enumerate(scaled_prior):\n",
    "    for j, column in enumerate(row):\n",
    "        if (j in non_causal_indices) and (i in causal_indices) & (i!=j):\n",
    "            continue\n",
    "        else:\n",
    "            scaled_prior[i,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1969b33-7fd1-471c-b21a-3b6c42c157ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aimp</th>\n",
       "      <th>amud</th>\n",
       "      <th>arnd</th>\n",
       "      <th>asin1</th>\n",
       "      <th>asin2</th>\n",
       "      <th>adbr</th>\n",
       "      <th>adfl</th>\n",
       "      <th>bed1</th>\n",
       "      <th>bed2</th>\n",
       "      <th>bfo1</th>\n",
       "      <th>bfo2</th>\n",
       "      <th>bso1</th>\n",
       "      <th>bso2</th>\n",
       "      <th>bso3</th>\n",
       "      <th>ced1</th>\n",
       "      <th>cfo1</th>\n",
       "      <th>cso1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aimp</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amud</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arnd</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asin1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asin2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adbr</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adfl</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bed1</th>\n",
       "      <td>0.200303</td>\n",
       "      <td>0.501797</td>\n",
       "      <td>0.491774</td>\n",
       "      <td>0.419287</td>\n",
       "      <td>0.369055</td>\n",
       "      <td>0.571580</td>\n",
       "      <td>0.568413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.473623</td>\n",
       "      <td>0.437199</td>\n",
       "      <td>0.313898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bed2</th>\n",
       "      <td>0.556672</td>\n",
       "      <td>0.408943</td>\n",
       "      <td>0.527579</td>\n",
       "      <td>0.538709</td>\n",
       "      <td>0.503766</td>\n",
       "      <td>0.475597</td>\n",
       "      <td>0.712675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.432411</td>\n",
       "      <td>0.603195</td>\n",
       "      <td>0.403706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bfo1</th>\n",
       "      <td>0.482994</td>\n",
       "      <td>0.502200</td>\n",
       "      <td>0.506608</td>\n",
       "      <td>0.623707</td>\n",
       "      <td>0.580026</td>\n",
       "      <td>0.670484</td>\n",
       "      <td>0.550955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.528722</td>\n",
       "      <td>0.506777</td>\n",
       "      <td>0.446574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bfo2</th>\n",
       "      <td>0.665232</td>\n",
       "      <td>0.337531</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.544953</td>\n",
       "      <td>0.442819</td>\n",
       "      <td>0.477483</td>\n",
       "      <td>0.499796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.382937</td>\n",
       "      <td>0.394501</td>\n",
       "      <td>0.383408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bso1</th>\n",
       "      <td>0.589416</td>\n",
       "      <td>0.564248</td>\n",
       "      <td>0.687663</td>\n",
       "      <td>0.581543</td>\n",
       "      <td>0.700280</td>\n",
       "      <td>0.623523</td>\n",
       "      <td>0.557805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.680969</td>\n",
       "      <td>0.474592</td>\n",
       "      <td>0.659170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bso2</th>\n",
       "      <td>0.696138</td>\n",
       "      <td>0.333983</td>\n",
       "      <td>0.381504</td>\n",
       "      <td>0.465704</td>\n",
       "      <td>0.356810</td>\n",
       "      <td>0.345442</td>\n",
       "      <td>0.280331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.436259</td>\n",
       "      <td>0.509088</td>\n",
       "      <td>0.300020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bso3</th>\n",
       "      <td>0.731007</td>\n",
       "      <td>0.395056</td>\n",
       "      <td>0.391311</td>\n",
       "      <td>0.484995</td>\n",
       "      <td>0.521578</td>\n",
       "      <td>0.477630</td>\n",
       "      <td>0.150121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.415162</td>\n",
       "      <td>0.479072</td>\n",
       "      <td>0.488996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ced1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cfo1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cso1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           aimp      amud      arnd     asin1     asin2      adbr      adfl  \\\n",
       "aimp   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "amud   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "arnd   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "asin1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "asin2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "adbr   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "adfl   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "bed1   0.200303  0.501797  0.491774  0.419287  0.369055  0.571580  0.568413   \n",
       "bed2   0.556672  0.408943  0.527579  0.538709  0.503766  0.475597  0.712675   \n",
       "bfo1   0.482994  0.502200  0.506608  0.623707  0.580026  0.670484  0.550955   \n",
       "bfo2   0.665232  0.337531  0.353983  0.544953  0.442819  0.477483  0.499796   \n",
       "bso1   0.589416  0.564248  0.687663  0.581543  0.700280  0.623523  0.557805   \n",
       "bso2   0.696138  0.333983  0.381504  0.465704  0.356810  0.345442  0.280331   \n",
       "bso3   0.731007  0.395056  0.391311  0.484995  0.521578  0.477630  0.150121   \n",
       "ced1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "cfo1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "cso1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       bed1  bed2  bfo1  bfo2  bso1  bso2  bso3      ced1      cfo1      cso1  \n",
       "aimp    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "amud    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "arnd    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "asin1   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "asin2   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "adbr    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "adfl    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "bed1    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.473623  0.437199  0.313898  \n",
       "bed2    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.432411  0.603195  0.403706  \n",
       "bfo1    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.528722  0.506777  0.446574  \n",
       "bfo2    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.382937  0.394501  0.383408  \n",
       "bso1    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.680969  0.474592  0.659170  \n",
       "bso2    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.436259  0.509088  0.300020  \n",
       "bso3    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.415162  0.479072  0.488996  \n",
       "ced1    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "cfo1    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "cso1    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b1ff0832-99a0-4823-94c9-999d95ab3947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, GATConv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch.distributions import Normal, Laplace, RelaxedOneHotCategorical\n",
    "from torchdiffeq import odeint  # For continuous-time normalizing flows\n",
    "from CARAT.model_utils import *\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def scale_tensor(tensor):\n",
    "  \"\"\"Scales a PyTorch tensor to the range [0, 1].\n",
    "\n",
    "  Args:\n",
    "    tensor: The input tensor.\n",
    "\n",
    "  Returns:\n",
    "    A new tensor with values scaled to [0, 1].\n",
    "  \"\"\"\n",
    "  min_val = tensor.min()\n",
    "  max_val = tensor.max()\n",
    "  scaled_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "  return scaled_tensor\n",
    "\n",
    "class FourierTimeEmbedding(nn.Module):\n",
    "    \"\"\"Encodes time indices using sinusoidal embeddings for better temporal representation.\"\"\"\n",
    "    def __init__(self, embedding_dim,time_steps, max_time=100):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_time = max_time\n",
    "        \n",
    "        # Create frequency bands (log-spaced)\n",
    "        self.freqs = torch.exp(torch.linspace(0, time_steps, embedding_dim//2,dtype=torch.float32)).to(device)\n",
    "\n",
    "    def forward(self, time_indices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            time_indices: Tensor of shape [batch_size, time_steps] containing integer time indices\n",
    "        Returns:\n",
    "            Embedded time representations of shape [batch_size, time_steps, embedding_dim]\n",
    "        \"\"\"\n",
    "        time_indices = time_indices.unsqueeze(-1)  # Shape [batch_size, time_steps, 1]\n",
    "        sinusoidal_in = time_indices * self.freqs  # Shape [batch_size, time_steps, embedding_dim//2]\n",
    "        time_embedding = torch.cat([torch.sin(sinusoidal_in), torch.cos(sinusoidal_in)], dim=-1)\n",
    "        return time_embedding  # Shape [batch_size, time_steps, embedding_dim]\n",
    "\n",
    "class TemporalRealNVPFlow(nn.Module):\n",
    "    \"\"\"Time-Adaptive Normalizing Flow for Latent Confounders.\"\"\"\n",
    "    def __init__(self, latent_dim,input_dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Linear(latent_dim // 2, latent_dim // 2,dtype=torch.float32)\n",
    "        self.translate = nn.Linear(latent_dim // 2, latent_dim // 2,dtype=torch.float32)\n",
    "        self.time_embedding = FourierTimeEmbedding(latent_dim,input_dim)\n",
    "        self.temporal_gate = nn.GRU(latent_dim, latent_dim//2, batch_first=True,dtype=torch.float32)  # Time-aware updates\n",
    "        self.time_projection = nn.Linear(input_dim,1,dtype=torch.float32)\n",
    "    def forward(self, z, time_context):\n",
    "        z1, z2 = z.chunk(2, dim=1)  # Split into two parts\n",
    "        s = torch.sigmoid(self.scale(z1))\n",
    "        t = self.translate(z1)\n",
    "        z2 = s * z2 + t\n",
    "        time_embed = self.time_embedding(time_context)\n",
    "        # Temporal adjustment to confounders\n",
    "        time_out, _ = self.temporal_gate(time_embed)\n",
    "        time_out = self.time_projection(time_out.permute(0,2,1)).squeeze(2)\n",
    "        z2 = z2 + time_out#.squeeze(1)  # Adjust for temporal shift\n",
    "        \n",
    "        return torch.cat([z1, z2], dim=1)\n",
    "\n",
    "class TemporalCausalGraph(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a Temporal Causal Graph (TCG) with:\n",
    "    - Time-dependent adjacency matrix (instantaneous + delayed effects)\n",
    "    - Adaptive normalizing flow for non-stationarity handling\n",
    "    \"\"\"\n",
    "    def __init__(self, num_nodes, hidden_dim, latent_dim,time_steps=10, prior_adj=None, mixed_data=False, num_regimes=3):\n",
    "        super(TemporalCausalGraph, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.time_steps = time_steps\n",
    "        self.mixed_data = mixed_data  # Support for categorical + continuous\n",
    "\n",
    "        self.pos_embedding = nn.Embedding(time_steps, hidden_dim,dtype=torch.float32)\n",
    "\n",
    "        # Learnable adjacency matrices (instantaneous + delayed)\n",
    "        self.edge_score_now = nn.Parameter(torch.randn(num_nodes, num_nodes,device=device))\n",
    "        self.edge_score_lag = nn.Parameter(torch.randn(num_nodes, num_nodes,device=device))\n",
    "        self.prior_adj = prior_adj if prior_adj is not None else torch.zeros(num_nodes, num_nodes,device=device)\n",
    "\n",
    "        # Direct adjacency learning\n",
    "        self.dropout = nn.Dropout(p=0.3)  # Dropout rate 30%\n",
    "        self.x_projection1 = nn.Sequential(\n",
    "            nn.Linear(num_nodes, hidden_dim, dtype=torch.float32),\n",
    "            nn.ReLU(),\n",
    "            self.dropout\n",
    "        )\n",
    "      \n",
    "        self.self_attention = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=4, dim_feedforward=hidden_dim, dtype=torch.float32,dropout=0.2),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.x_projection2 = nn.Linear(hidden_dim,num_nodes,dtype=torch.float32)\n",
    "\n",
    "        # Latent Confounders Z (Time-Aware)\n",
    "        \n",
    "        self.temporal_flow = TemporalRealNVPFlow(latent_dim,time_steps)\n",
    "\n",
    "        # Temporal Graph Attention Network\n",
    "        self.gnn = GATv2Conv(num_nodes, hidden_dim, heads=4, concat=True, dropout=0.2).to(torch.float32)\n",
    "\n",
    "        # Mapping from latent space to num_nodes\n",
    "        self.latent_to_nodes = nn.Linear(latent_dim, num_nodes,dtype=torch.float32)\n",
    "\n",
    "        # Likelihood Models\n",
    "        self.gaussian_likelihood = nn.Sequential(\n",
    "            nn.Dropout(0.2),  # Dropout before final layer\n",
    "            nn.Linear(hidden_dim * 4, self.num_nodes*2, dtype=torch.float32)\n",
    "        )  # Mean, Log-Variance\n",
    "        if mixed_data:\n",
    "            self.categorical_likelihood = nn.Linear(hidden_dim, num_nodes,dtype=torch.float32)  # Gumbel-Softmax Output\n",
    "\n",
    "    def forward(self, X, time_context,Z):\n",
    "        \"\"\" Learns causal graph over time and performs inference \"\"\"\n",
    "        # Compute adjacency matrices\n",
    "        x = self.x_projection1(X)\n",
    "        pos_indices = torch.arange(self.time_steps, device=X.device)  # [time_steps]\n",
    "        X_permuted = x.permute(1, 0, 2)  # [time_steps, batch_size, num_nodes]\n",
    "        pos_embedding = self.pos_embedding(pos_indices).unsqueeze(1) \n",
    "        \n",
    "        X_transformed = self.self_attention(X_permuted + pos_embedding)\n",
    "        x = self.x_projection2(X_transformed)\n",
    "\n",
    "\n",
    "        weights_schedule = generate_decreasing_weights(3,start=0.2)\n",
    "        lag_mats = []\n",
    "        for i in range(0,x.shape[0]):\n",
    "            if i ==0:\n",
    "                lag_mats.append( replace_zero(torch.sigmoid(self.edge_score_now* weights_schedule[i])  +self.prior_adj + F.normalize(F.gumbel_softmax(torch.einsum('bk,bj->kj', x[i,:,:], x[i,:,:]) * weights_schedule[i],tau=100,eps=1e-16,))).fill_diagonal_(-1)) \n",
    "            else:\n",
    "                lag_mats.append( replace_zero(torch.sigmoid(self.edge_score_lag* weights_schedule[i])  +self.prior_adj + F.normalize(F.gumbel_softmax(torch.einsum('bk,bj->kj', x[i,:,:], x[i,:,:]) * weights_schedule[i],tau=100,eps=1e-16,))).fill_diagonal_(-1)) \n",
    "        \n",
    "        adj_now = scale_tensor(lag_mats[0] )  # Amplify signal\n",
    "        if x.shape[0] >1:\n",
    "            adj_lag = scale_tensor(torch.sum(torch.stack(lag_mats[1:]), dim=0))\n",
    "        else:\n",
    "            adj_lag = scale_tensor(lag_mats[1])\n",
    "        adj_mat = adj_now * 0.5 + adj_lag * 0.5\n",
    "       \n",
    "        # Encode latent confounders with time-awareness\n",
    "        #Z = self.latent_Z + torch.randn_like(self.latent_Z) * 0.1\n",
    "        \"\"\"Z = self.temporal_flow(Z.to(device), time_context.to(device))  # Apply time-adaptive normalizing flow\n",
    "        Z = self.latent_to_nodes(Z)  # Map latent space to num_nodes\n",
    "\n",
    "        # Temporal graph attention\n",
    "        edge_index,edge_weights = dense_to_sparse( adj_mat)\n",
    "        X_emb = self.gnn(Z, edge_index)\n",
    "        X_emb = X_emb.view(X_emb.shape[0], -1)\n",
    "\n",
    "        # Likelihood computation\n",
    "        mean_logvar = self.gaussian_likelihood(X_emb)\n",
    "        mean, log_var = torch.split(mean_logvar, mean_logvar.shape[-1] // 2, dim=-1)\n",
    "        log_var = torch.clamp(log_var, -5, 2)  # Stabilization\n",
    "        \n",
    "        likelihood = Laplace(mean, torch.exp(0.5 * log_var))\"\"\"\n",
    "        likelihood = 0\n",
    "        \n",
    "        return adj_now, adj_lag, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a10d062f-0ea5-4117-a77a-f71acef68294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, GATConv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch.distributions import Normal, Laplace, RelaxedOneHotCategorical\n",
    "from torchdiffeq import odeint  # For continuous-time normalizing flows\n",
    "from CARAT.components import *\n",
    "from CARAT.model_utils import *\n",
    "import pandas as pd\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CausalGraphVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_nodes, time_steps=10, prior_adj=None):\n",
    "        super(CausalGraphVAE, self).__init__()\n",
    "        self.time_steps = time_steps\n",
    "        self.num_nodes = num_nodes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.prior_adj = prior_adj\n",
    "        self.causal_graph = TemporalCausalGraph(num_nodes, hidden_dim, latent_dim, time_steps, prior_adj)\n",
    "        self.alpha = torch.tensor(0.0, dtype=torch.float32, requires_grad=False, device=device)\n",
    "        self.rho = torch.tensor(1.0, dtype=torch.float32, requires_grad=False, device=device)\n",
    "\n",
    "        # Temporal-aware Encoder and Decoder\n",
    "        self.encoder_rnn = nn.GRU(input_dim, hidden_dim, batch_first=True,dtype=torch.float32, device=device)\n",
    "        self.mu_layer = nn.Linear(hidden_dim, latent_dim,dtype=torch.float32, device=device)\n",
    "        self.logvar_layer = nn.Linear(hidden_dim, latent_dim,dtype=torch.float32, device=device)\n",
    "        self.decoder_rnn = nn.GRU(latent_dim, hidden_dim, batch_first=True,dtype=torch.float32, device=device)\n",
    "        self.decoder_fc = nn.Linear(hidden_dim, input_dim,dtype=torch.float32, device=device)\n",
    "\n",
    "    def encode(self, X, time_context):\n",
    "        X_enc, _ = self.encoder_rnn(X)\n",
    "        mu, logvar = self.mu_layer(X_enc[:, -1, :]), self.logvar_layer(X_enc[:, -1, :])\n",
    "        Z = mu + torch.randn_like(logvar) * torch.exp(0.5 * logvar)\n",
    "        adj_now, adj_lag, likelihood = self.causal_graph(X, time_context,Z)  # Use temporal causal graph\n",
    "        return mu, logvar, adj_now, adj_lag, likelihood\n",
    "\n",
    "    def decode(self, Z, adj_now, adj_lag):\n",
    "        Z_expanded = Z.unsqueeze(1).repeat(1, self.time_steps, 1)\n",
    "        X_dec, _ = self.decoder_rnn(Z_expanded)\n",
    "        X_dec = self.decoder_fc(X_dec)\n",
    "        return X_dec\n",
    "\n",
    "    def forward(self, X, time_context):\n",
    "        X = X.to(torch.float32)  # Convert input X to float32\n",
    "        time_context = time_context.to(torch.float32)  # Convert time context to float32\n",
    "\n",
    "        mu, logvar, adj_now, adj_lag, likelihood = self.encode(X, time_context)\n",
    "        Z = mu + torch.randn_like(logvar) * torch.exp(0.5 * logvar)\n",
    "        recon_X = self.decode(Z, adj_now, adj_lag)\n",
    "        return recon_X, mu, logvar, adj_now, adj_lag, likelihood\n",
    "\n",
    "    def infer_causal_effect(self, X_data, T_data, target_variable, labels, non_causal_indices=[]):\n",
    "        \"\"\"Infers the top causal factors for a given target variable using counterfactual analysis.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            target_idx = labels.index(target_variable, 0)\n",
    "        except:\n",
    "            return target_variable + \" not in index\"\n",
    "    \n",
    "        self.eval()\n",
    "        n_samples = X_data.shape[0]\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            _, _, adj_now, adj_lag, _ = self.encode(X_data, T_data)\n",
    "            adj_matrix = adj_now * 0.5 + adj_lag * 0.5\n",
    "    \n",
    "            # Compute edge strengths\n",
    "            causal_strengths = adj_matrix[:, target_idx].cpu().detach().numpy()\n",
    "            edge_strengths = {labels[i]: float(np.round(val.item(), 6)) for i, val in enumerate(causal_strengths) if i not in non_causal_indices}\n",
    "\n",
    "            instantaneous_strengths = adj_now[:, target_idx].cpu().detach().numpy()\n",
    "            instantaneous_edge_strengths = {}\n",
    "            for i, val in enumerate(instantaneous_strengths):\n",
    "                if i not in non_causal_indices:\n",
    "                    instantaneous_edge_strengths[labels[i]] = float(np.round(val.item(),6))\n",
    "            \n",
    "            lagged_strengths = adj_lag[:, target_idx].cpu().detach().numpy()\n",
    "            lagged_edge_strengths = {}\n",
    "            for i, val in enumerate(lagged_strengths):\n",
    "                if i not in non_causal_indices:\n",
    "                    lagged_edge_strengths[labels[i]] = float(np.round(val.item(),6))\n",
    "\n",
    "            \n",
    "            counterfactual_results_positive = {}\n",
    "            counterfactual_results_negative = {}\n",
    "    \n",
    "            for i in range(self.num_nodes):\n",
    "                if i == target_idx or i in non_causal_indices:\n",
    "                    continue\n",
    "                \n",
    "                label = labels[i]\n",
    "    \n",
    "                # Positive intervention: set cause variable to 1\n",
    "                intervention_pos = X_data.clone()\n",
    "                intervention_pos[:, :, i] = 1  \n",
    "                mu, logvar, adj_now_int, adj_lag_int, likelihood = self.encode(intervention_pos, T_data)\n",
    "                Z = mu + torch.randn_like(logvar, dtype=torch.float32, device=device) * torch.exp(0.5 * logvar)\n",
    "                recon_X = self.decode(Z, adj_now_int, adj_lag_int)\n",
    "    \n",
    "                # Compute normalized deviation for positive intervention\n",
    "                counterfactual_results_positive[label] = torch.abs((X_data[:, :, target_idx] - recon_X[:, :, target_idx]) / (X_data[:, :, target_idx] + 1e-6)).mean().item()\n",
    "    \n",
    "                # Negative intervention: set cause variable to -1\n",
    "                intervention_neg = X_data.clone()\n",
    "                intervention_neg[:, :, i] = -1\n",
    "                mu, logvar, adj_now_int, adj_lag_int, likelihood = self.encode(intervention_neg, T_data)\n",
    "                Z = mu + torch.randn_like(logvar, dtype=torch.float32, device=device) * torch.exp(0.5 * logvar)\n",
    "                recon_X = self.decode(Z, adj_now_int, adj_lag_int)\n",
    "    \n",
    "                # Compute normalized deviation for negative intervention\n",
    "                counterfactual_results_negative[label] = torch.abs((X_data[:, :, target_idx] - recon_X[:, :, target_idx]) / (X_data[:, :, target_idx] + 1e-6)).mean().item()\n",
    "    \n",
    "            # Compute composite counterfactual score\n",
    "            counterfactual_rankings = {key: counterfactual_results_positive[key] + counterfactual_results_negative[key] for key in counterfactual_results_positive}\n",
    "    \n",
    "            # Sort by impact score\n",
    "            sorted_keys = sorted(counterfactual_rankings, key=counterfactual_rankings.get, reverse=True)\n",
    "            counterfactual_rankings = {key: counterfactual_rankings[key] for key in sorted_keys}\n",
    "            sorted_keys = sorted(edge_strengths, key=edge_strengths.get, reverse=True)\n",
    "            top_causes = {key: edge_strengths[key] for key in sorted_keys}\n",
    "            sorted_keys = sorted(instantaneous_edge_strengths, key=instantaneous_edge_strengths.get, reverse=True)\n",
    "            instantaneous_effects = {key: instantaneous_edge_strengths[key] for key in sorted_keys}\n",
    "            sorted_keys = sorted(lagged_edge_strengths, key=lagged_edge_strengths.get, reverse=True)\n",
    "            lagged_effects = {key: lagged_edge_strengths[key] for key in sorted_keys}\n",
    "            \n",
    "                \n",
    "        return top_causes, counterfactual_rankings, instantaneous_effects, lagged_effects\n",
    "\n",
    "    def loss_function(self, recon_X, X, mu, logvar, likelihood, adj_now, adj_lag, epoch, max_epochs, rho_max=30.0, alpha_max=15.0, lambda_prior=5.0):\n",
    "        \"\"\"Loss function including reconstruction, KL divergence, DAG penalty, and prior regularization\"\"\"\n",
    "        recon_loss = F.mse_loss(recon_X, X, reduction='sum')\n",
    "        beta = min(1.0, epoch / max_epochs)\n",
    "        kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "        # Regularization on adjacency matrix (pretrained `prior_adj` should remain mostly intact)\n",
    "        #prior_loss = lambda_prior * (torch.norm(adj_now - self.prior_adj, p=1) + torch.norm(adj_lag - self.prior_adj, p=1))\n",
    "    \n",
    "        sparsity_loss = torch.norm(adj_now, p=1) + torch.norm(adj_lag, p=1)\n",
    "        h_value = (notears_constraint(adj_now) + notears_constraint(adj_lag))\n",
    "    \n",
    "        self.rho = min(rho_max, 1.0 + (epoch / max_epochs) ** 2 * 15.0)\n",
    "        self.alpha = min(alpha_max, (epoch / max_epochs) ** 2 * 5.0)\n",
    "    \n",
    "        #likelihood_loss = (-likelihood.log_prob(X[:, 0, :]).mean()) + 5.0\n",
    "        likelihood_loss = 0\n",
    "    \n",
    "        lagrangian_loss = (self.alpha * h_value + 0.5 * self.rho * (h_value ** 2)) / (self.num_nodes ** 2)\n",
    "\n",
    "\n",
    "        return recon_loss , kl_loss , sparsity_loss , lagrangian_loss, likelihood_loss\n",
    "\n",
    "    def train_model(self, dataloader, optimizer, num_epochs=100, patience=10,BATCH_SIZE=64,rho_max=30.0,alpha_max=15.0):\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for batch_idx, (X_batch, time_batch) in enumerate(dataloader):\n",
    "                if X_batch.shape[0] < X_batch.shape[2]:\n",
    "                    continue\n",
    "                optimizer.zero_grad()\n",
    "                recon_X, mu, logvar, adj_now, adj_lag, likelihood = self.forward(X_batch, time_batch)\n",
    "                recon_loss , kl_loss , sparsity_loss , lagrangian_loss, likelihood_loss = self.loss_function(recon_X, X_batch, mu, logvar,likelihood, adj_now, \n",
    "                                                                                            adj_lag, epoch, num_epochs,rho_max,alpha_max)\n",
    "                loss = recon_loss + kl_loss + sparsity_loss + lagrangian_loss # + likelihood_loss\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            self.adj_mat = scale_tensor(adj_now * 0.5 + adj_lag * 0.5)\n",
    "\n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            if epoch % 250 == 0:\n",
    "                print(f\"Epoch {epoch + 1}: Loss = {avg_loss:.4f}\")\n",
    "                print(f\"Recon Loss ={recon_loss:.4f}, KL Loss = {kl_loss:.4f}, Sparsity Loss = {sparsity_loss:.4f}, Lagrangian Loss = {lagrangian_loss:.4f}, Likelihood Loss = {likelihood_loss:.4f}\") \n",
    "\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered. Last Epoch: \" + str(epoch) )\n",
    "                    print(f\"Recon Loss ={recon_loss:.4f}, KL Loss = {kl_loss:.4f}, Sparsity Loss = {sparsity_loss:.4f}, Lagrangian Loss = {lagrangian_loss:.4f}, Likelihood Loss = {likelihood_loss:.4f}\") \n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "06f878d7-d080-4f20-bd9a-10f33666d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataframe, time_steps=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): Time-series data\n",
    "            time_steps (int): Number of past time steps to consider\n",
    "        \"\"\"\n",
    "        self.data = torch.tensor(dataframe.values, dtype=torch.float32)\n",
    "        self.time_steps = time_steps\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.time_steps\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            X: Past `time_steps` values (shape: [time_steps, num_features])\n",
    "            y: Next step prediction target (shape: [num_features])\n",
    "        \"\"\"\n",
    "        X = self.data[idx : idx + self.time_steps, :].to(device)\n",
    "        time_context = torch.arange(self.time_steps).float().to(device)  # Time indexing\n",
    "        \n",
    "        return X, time_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3c7d362f-6a8a-4a3c-b03f-3839ca30f4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 10:53:59,097 INFO -- Model: 0\n",
      "/opt/data/home/j.lowhorn/miniconda3/envs/rca/lib/python3.11/site-packages/statsmodels/tsa/stattools.py:702: RuntimeWarning: invalid value encountered in divide\n",
      "  acf = avf[: nlags + 1] / avf[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly: cfo1\n",
      "Epoch 1: Loss = 209954475.7500\n",
      "Recon Loss =70.2668, KL Loss = 0.8316, Sparsity Loss = 465.2918, Lagrangian Loss = 40883288.0000, Likelihood Loss = 0.0000\n",
      "Epoch 251: Loss = 86057.8305\n",
      "Recon Loss =1.2512, KL Loss = 0.7205, Sparsity Loss = 399.6961, Lagrangian Loss = 97666.7578, Likelihood Loss = 0.0000\n",
      "Epoch 501: Loss = 55293.7184\n",
      "Recon Loss =1.0454, KL Loss = 0.4710, Sparsity Loss = 390.7825, Lagrangian Loss = 55512.2188, Likelihood Loss = 0.0000\n",
      "Epoch 751: Loss = 26403.0002\n",
      "Recon Loss =0.3202, KL Loss = 0.5269, Sparsity Loss = 379.0124, Lagrangian Loss = 26120.6387, Likelihood Loss = 0.0000\n",
      "Epoch 1001: Loss = 10829.5494\n",
      "Recon Loss =0.7400, KL Loss = 0.5056, Sparsity Loss = 364.8170, Lagrangian Loss = 10475.9697, Likelihood Loss = 0.0000\n",
      "Epoch 1251: Loss = 4441.0924\n",
      "Recon Loss =0.2602, KL Loss = 0.4937, Sparsity Loss = 349.4449, Lagrangian Loss = 4091.5305, Likelihood Loss = 0.0000\n",
      "Epoch 1501: Loss = 1978.1325\n",
      "Recon Loss =0.4989, KL Loss = 0.5233, Sparsity Loss = 333.0504, Lagrangian Loss = 1642.8674, Likelihood Loss = 0.0000\n",
      "Epoch 1751: Loss = 1020.0928\n",
      "Recon Loss =0.2683, KL Loss = 0.5173, Sparsity Loss = 315.3649, Lagrangian Loss = 702.9038, Likelihood Loss = 0.0000\n",
      "Epoch 2001: Loss = 617.7231\n",
      "Recon Loss =0.2004, KL Loss = 0.5930, Sparsity Loss = 295.8778, Lagrangian Loss = 320.1967, Likelihood Loss = 0.0000\n",
      "Epoch 2251: Loss = 434.4161\n",
      "Recon Loss =0.2984, KL Loss = 0.5351, Sparsity Loss = 274.8067, Lagrangian Loss = 158.0416, Likelihood Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 11:07:39,785 INFO -- Edge Accuracy = 0.00, Instantaneous Accuracy = 0.00, Lagged Accuracy = 0.00, Counterfactual Accuracy = 100.00,  Blended Accuracy = 100.00  \n",
      "2025-03-13 11:07:39,785 INFO -- Model: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly: cfo1\n",
      "Epoch 1: Loss = 25958299.9646\n",
      "Recon Loss =29.2614, KL Loss = 0.6443, Sparsity Loss = 413.3897, Lagrangian Loss = 353027.3438, Likelihood Loss = 0.0000\n",
      "Epoch 251: Loss = 474.4332\n",
      "Recon Loss =1.9683, KL Loss = 0.9438, Sparsity Loss = 321.6603, Lagrangian Loss = 147.3307, Likelihood Loss = 0.0000\n",
      "Epoch 501: Loss = 145.4575\n",
      "Recon Loss =1.0885, KL Loss = 0.8156, Sparsity Loss = 133.1582, Lagrangian Loss = 9.4888, Likelihood Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 11:30:28,950 INFO -- Edge Accuracy = 50.00, Instantaneous Accuracy = 50.00, Lagged Accuracy = 50.00, Counterfactual Accuracy = 50.00,  Blended Accuracy = 50.00  \n",
      "2025-03-13 11:30:28,951 INFO -- Model: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered. Last Epoch: 590\n",
      "Recon Loss =0.9875, KL Loss = 0.8161, Sparsity Loss = 132.3716, Lagrangian Loss = 10.8946, Likelihood Loss = 0.0000\n",
      "Anomaly: cso1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/data/home/j.lowhorn/miniconda3/envs/rca/lib/python3.11/site-packages/statsmodels/tsa/stattools.py:702: RuntimeWarning: invalid value encountered in divide\n",
      "  acf = avf[: nlags + 1] / avf[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 128432882.1354\n",
      "Recon Loss =100.1272, KL Loss = 0.7426, Sparsity Loss = 415.8172, Lagrangian Loss = 769229.9375, Likelihood Loss = 0.0000\n",
      "Epoch 251: Loss = 81708.0378\n",
      "Recon Loss =1.7975, KL Loss = 1.1985, Sparsity Loss = 390.3911, Lagrangian Loss = 85855.2344, Likelihood Loss = 0.0000\n",
      "Epoch 501: Loss = 18691.4451\n",
      "Recon Loss =1.4517, KL Loss = 1.1636, Sparsity Loss = 371.4758, Lagrangian Loss = 19345.7793, Likelihood Loss = 0.0000\n",
      "Epoch 751: Loss = 4028.3089\n",
      "Recon Loss =1.8988, KL Loss = 1.0930, Sparsity Loss = 348.0166, Lagrangian Loss = 3900.5134, Likelihood Loss = 0.0000\n",
      "Epoch 1001: Loss = 1092.8484\n",
      "Recon Loss =2.5953, KL Loss = 1.2599, Sparsity Loss = 320.7953, Lagrangian Loss = 829.9745, Likelihood Loss = 0.0000\n",
      "Epoch 1251: Loss = 476.8621\n",
      "Recon Loss =1.3981, KL Loss = 1.1670, Sparsity Loss = 287.9541, Lagrangian Loss = 214.4134, Likelihood Loss = 0.0000\n",
      "Epoch 1501: Loss = 308.7171\n",
      "Recon Loss =1.5023, KL Loss = 1.0676, Sparsity Loss = 239.4283, Lagrangian Loss = 85.2576, Likelihood Loss = 0.0000\n",
      "Epoch 1751: Loss = 229.5882\n",
      "Recon Loss =1.1588, KL Loss = 1.1362, Sparsity Loss = 173.3656, Lagrangian Loss = 67.7128, Likelihood Loss = 0.0000\n",
      "Epoch 2001: Loss = 207.0888\n",
      "Recon Loss =1.7239, KL Loss = 1.5598, Sparsity Loss = 141.0588, Lagrangian Loss = 75.0886, Likelihood Loss = 0.0000\n",
      "Epoch 2251: Loss = 198.7406\n",
      "Recon Loss =1.1414, KL Loss = 1.0606, Sparsity Loss = 134.0233, Lagrangian Loss = 74.5224, Likelihood Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 11:50:32,253 INFO -- Edge Accuracy = 33.33, Instantaneous Accuracy = 33.33, Lagged Accuracy = 33.33, Counterfactual Accuracy = 33.33,  Blended Accuracy = 33.33  \n",
      "2025-03-13 11:50:32,253 INFO -- Model: 3\n",
      "/opt/data/home/j.lowhorn/miniconda3/envs/rca/lib/python3.11/site-packages/statsmodels/tsa/stattools.py:702: RuntimeWarning: invalid value encountered in divide\n",
      "  acf = avf[: nlags + 1] / avf[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly: ced1\n",
      "Epoch 1: Loss = 210437159.5714\n",
      "Recon Loss =93.2553, KL Loss = 0.3928, Sparsity Loss = 445.7740, Lagrangian Loss = 8849002.0000, Likelihood Loss = 0.0000\n",
      "Epoch 251: Loss = 143552.0000\n",
      "Recon Loss =1.4146, KL Loss = 0.6759, Sparsity Loss = 395.6437, Lagrangian Loss = 166953.6875, Likelihood Loss = 0.0000\n",
      "Epoch 501: Loss = 107155.6562\n",
      "Recon Loss =0.3918, KL Loss = 0.4801, Sparsity Loss = 389.9657, Lagrangian Loss = 124433.1250, Likelihood Loss = 0.0000\n",
      "Epoch 751: Loss = 72575.3281\n",
      "Recon Loss =0.5963, KL Loss = 0.4241, Sparsity Loss = 382.3292, Lagrangian Loss = 84183.7891, Likelihood Loss = 0.0000\n",
      "Epoch 1001: Loss = 44358.2182\n",
      "Recon Loss =0.6485, KL Loss = 0.4478, Sparsity Loss = 373.3398, Lagrangian Loss = 51311.7383, Likelihood Loss = 0.0000\n",
      "Epoch 1251: Loss = 25801.5198\n",
      "Recon Loss =0.1829, KL Loss = 0.4177, Sparsity Loss = 363.6976, Lagrangian Loss = 29693.4922, Likelihood Loss = 0.0000\n",
      "Epoch 1501: Loss = 14853.7497\n",
      "Recon Loss =0.5717, KL Loss = 0.4281, Sparsity Loss = 353.7200, Lagrangian Loss = 16950.0508, Likelihood Loss = 0.0000\n",
      "Epoch 1751: Loss = 8624.0544\n",
      "Recon Loss =0.1989, KL Loss = 0.4410, Sparsity Loss = 343.4767, Lagrangian Loss = 9704.1484, Likelihood Loss = 0.0000\n",
      "Epoch 2001: Loss = 4876.3142\n",
      "Recon Loss =0.2160, KL Loss = 0.4516, Sparsity Loss = 333.0869, Lagrangian Loss = 5348.9976, Likelihood Loss = 0.0000\n",
      "Epoch 2251: Loss = 2729.6261\n",
      "Recon Loss =0.2141, KL Loss = 0.4643, Sparsity Loss = 324.0479, Lagrangian Loss = 2856.9268, Likelihood Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 11:59:27,314 INFO -- Edge Accuracy = 25.00, Instantaneous Accuracy = 25.00, Lagged Accuracy = 25.00, Counterfactual Accuracy = 50.00,  Blended Accuracy = 50.00  \n",
      "2025-03-13 11:59:27,315 INFO -- Model: 4\n",
      "/opt/data/home/j.lowhorn/miniconda3/envs/rca/lib/python3.11/site-packages/statsmodels/tsa/stattools.py:702: RuntimeWarning: invalid value encountered in divide\n",
      "  acf = avf[: nlags + 1] / avf[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly: cfo1\n",
      "Epoch 1: Loss = 300613802.5714\n",
      "Recon Loss =150.2297, KL Loss = 0.3903, Sparsity Loss = 452.7309, Lagrangian Loss = 18160962.0000, Likelihood Loss = 0.0000\n",
      "Epoch 251: Loss = 181155.0580\n",
      "Recon Loss =2.1790, KL Loss = 1.4071, Sparsity Loss = 396.0241, Lagrangian Loss = 210631.1406, Likelihood Loss = 0.0000\n",
      "Epoch 501: Loss = 137378.3750\n",
      "Recon Loss =1.0847, KL Loss = 1.1319, Sparsity Loss = 390.5897, Lagrangian Loss = 159648.8281, Likelihood Loss = 0.0000\n",
      "Epoch 751: Loss = 93125.6183\n",
      "Recon Loss =0.6888, KL Loss = 1.0500, Sparsity Loss = 382.9973, Lagrangian Loss = 108113.2109, Likelihood Loss = 0.0000\n",
      "Epoch 1001: Loss = 56770.0190\n",
      "Recon Loss =0.6413, KL Loss = 0.8919, Sparsity Loss = 374.0209, Lagrangian Loss = 65757.6797, Likelihood Loss = 0.0000\n",
      "Epoch 1251: Loss = 32891.1557\n",
      "Recon Loss =0.7276, KL Loss = 1.1700, Sparsity Loss = 364.4062, Lagrangian Loss = 37950.9102, Likelihood Loss = 0.0000\n",
      "Epoch 1501: Loss = 18829.7369\n",
      "Recon Loss =0.3520, KL Loss = 0.9282, Sparsity Loss = 354.4710, Lagrangian Loss = 21581.6543, Likelihood Loss = 0.0000\n",
      "Epoch 1751: Loss = 10853.2755\n",
      "Recon Loss =0.4384, KL Loss = 1.0207, Sparsity Loss = 344.2899, Lagrangian Loss = 12299.7480, Likelihood Loss = 0.0000\n",
      "Epoch 2001: Loss = 6073.4893\n",
      "Recon Loss =0.4794, KL Loss = 0.9402, Sparsity Loss = 333.9834, Lagrangian Loss = 6742.2524, Likelihood Loss = 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 62\u001b[0m\n\u001b[1;32m     56\u001b[0m fine_tuned \u001b[38;5;241m=\u001b[39m CausalGraphVAE(input_dim\u001b[38;5;241m=\u001b[39mnum_nodes, hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim,\n\u001b[1;32m     57\u001b[0m                        latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, num_nodes\u001b[38;5;241m=\u001b[39mnum_nodes,\n\u001b[1;32m     58\u001b[0m                        time_steps\u001b[38;5;241m=\u001b[39mTIME_STEPS,prior_adj\u001b[38;5;241m=\u001b[39mscaled_prior)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     59\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(fine_tuned\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)  \u001b[38;5;66;03m# L2 Regularization\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[43mfine_tuned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrho_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43malpha_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m causes, counterfactual_rankings, instantaneous_effects, lagged_effects \u001b[38;5;241m=\u001b[39m fine_tuned\u001b[38;5;241m.\u001b[39minfer_causal_effect(X_data\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device),T_data\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device),anomaly,cols,non_causal_indices\u001b[38;5;241m=\u001b[39mcurr_non_causal)\n\u001b[1;32m     66\u001b[0m causes_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(causes, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcauses\u001b[39m\u001b[38;5;124m'\u001b[39m],orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[78], line 169\u001b[0m, in \u001b[0;36mCausalGraphVAE.train_model\u001b[0;34m(self, dataloader, optimizer, num_epochs, patience, BATCH_SIZE, rho_max, alpha_max)\u001b[0m\n\u001b[1;32m    165\u001b[0m recon_loss , kl_loss , sparsity_loss , lagrangian_loss, likelihood_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(recon_X, X_batch, mu, logvar,likelihood, adj_now, \n\u001b[1;32m    166\u001b[0m                                                                             adj_lag, epoch, num_epochs,rho_max,alpha_max)\n\u001b[1;32m    167\u001b[0m loss \u001b[38;5;241m=\u001b[39m recon_loss \u001b[38;5;241m+\u001b[39m kl_loss \u001b[38;5;241m+\u001b[39m sparsity_loss \u001b[38;5;241m+\u001b[39m lagrangian_loss \u001b[38;5;66;03m# + likelihood_loss\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    171\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/rca/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rca/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rca/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cols = list(test_df.columns)\n",
    "non_causal_columns = list(set(cols).difference(set(potential_causes)))\n",
    "causal_indices = [train_df.columns.get_loc(col) for col in potential_causes]\n",
    "non_causal_indices = [train_df.columns.get_loc(col) for col in non_causal_columns]\n",
    "\n",
    "num_nodes = len(train_df.columns)\n",
    "\n",
    "new_metadata = []\n",
    "A0 = get_adjacency(cols,causal_indices,non_causal_indices,num_nodes)    \n",
    "BATCH_SIZE = 64\n",
    "hidden_dim=64\n",
    "latent_dim=8\n",
    "\n",
    "set_seed()\n",
    "edge_correct = 0\n",
    "instantaneous_correct = 0\n",
    "lagged_correct = 0\n",
    "counterfactual_correct = 0 \n",
    "root_rank_correct = 0\n",
    "total_correct = 0\n",
    "total_checked = 0\n",
    "incorrect = []\n",
    "\n",
    "for i, row in enumerate(cats_rows_list):\n",
    "    total_checked +=1 \n",
    "    logger.info('Model: '+ str(i))\n",
    "    anomaly = eval(row['affected'])[0]\n",
    "    print('Anomaly: ' + anomaly)\n",
    "    anomaly_time = datetime.strptime(row['start_time'],\"%Y-%m-%d %H:%M:%S\")\n",
    "    #start_time = datetime.strptime(row['start_time'],\"%Y-%m-%d %H:%M:%S\")\n",
    "    end_time = datetime.strptime(row['end_time'],\"%Y-%m-%d %H:%M:%S\")\n",
    "    root_cause = row['root_cause']\n",
    "    #start_len = mod_df.shape[0]\n",
    "    #if start_len >1000:\n",
    "        #start_len = 1000\n",
    "    start_len = 50\n",
    "    start_time = anomaly_time- timedelta(seconds=start_len)\n",
    "    finish_time = end_time + timedelta(seconds=start_len)\n",
    "    mod_df = test_df[(test_df.index>= start_time) & (test_df.index<= finish_time)]\n",
    "    mod_df = mod_df[['aimp', 'amud', 'arnd', 'asin1', 'asin2', 'adbr', 'adfl', 'bed1',\n",
    "       'bed2', 'bfo1', 'bfo2', 'bso1', 'bso2', 'bso3', 'ced1', 'cfo1', 'cso1']]\n",
    "\n",
    "    \"\"\"\n",
    "    FIND THE OPTIMAL NUMBER OF LAGS\n",
    "    \"\"\"\n",
    "    TIME_STEPS = most_frequent(find_optimal_lags_for_dataframe(mod_df))+1\n",
    "\n",
    "    dataset = TimeSeriesDataset(mod_df, time_steps=TIME_STEPS)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    X_data = torch.empty(0,device=device)\n",
    "    T_data = torch.empty(0,device=device)\n",
    "    for batch_idx, (X_batch, time_batch) in enumerate(dataloader):\n",
    "        X_data = torch.cat((X_data[:batch_idx], X_batch, X_data[batch_idx:]))\n",
    "        T_data = torch.cat((T_data[:batch_idx], time_batch, T_data[batch_idx:]))\n",
    "    \n",
    "    fine_tuned = CausalGraphVAE(input_dim=num_nodes, hidden_dim=hidden_dim,\n",
    "                           latent_dim=latent_dim, num_nodes=num_nodes,\n",
    "                           time_steps=TIME_STEPS,prior_adj=scaled_prior).to(device)\n",
    "    optimizer = torch.optim.AdamW(fine_tuned.parameters(), lr=0.01, weight_decay=1e-2)  # L2 Regularization\n",
    "\n",
    "    \n",
    "    fine_tuned.train_model(dataloader, optimizer, num_epochs=2500, patience=50,BATCH_SIZE=BATCH_SIZE,rho_max=10.0,alpha_max=2.0)\n",
    "\n",
    "    causes, counterfactual_rankings, instantaneous_effects, lagged_effects = fine_tuned.infer_causal_effect(X_data.to(torch.float32).to(device),T_data.to(torch.float32).to(device),anomaly,cols,non_causal_indices=curr_non_causal)\n",
    "    \n",
    "    causes_df = pd.DataFrame.from_dict(causes, columns=['causes'],orient='index')\n",
    "    causes_df=(causes_df-causes_df.min())/(causes_df.max()-causes_df.min())\n",
    "    isntantaneous_df = pd.DataFrame.from_dict(instantaneous_effects, columns=['instantaneous'],orient='index')\n",
    "    isntantaneous_df=(isntantaneous_df-isntantaneous_df.min())/(isntantaneous_df.max()-isntantaneous_df.min())\n",
    "    lagged_df = pd.DataFrame.from_dict(lagged_effects, columns=['lagged'],orient='index')\n",
    "    lagged_df=(lagged_df-lagged_df.min())/(lagged_df.max()-lagged_df.min())\n",
    "    \n",
    "    counterfactual_df = pd.DataFrame.from_dict(counterfactual_rankings, columns=['counterfactuals'],orient='index')\n",
    "    counterfactual_df=(counterfactual_df-counterfactual_df.min())/(counterfactual_df.max()-counterfactual_df.min())\n",
    "    \n",
    "    total_score = pd.concat([causes_df, isntantaneous_df,lagged_df,counterfactual_df], axis=1)\n",
    "    total_score['causal_strength'] = total_score.mean(axis=1)\n",
    "    total_score=total_score.sort_values(by='causal_strength',ascending=False)\n",
    "    \n",
    "    edge_cause_1 = causes_df.sort_values(by='causes',ascending=False)[0:3].index[0]\n",
    "    edge_cause_2 = causes_df.sort_values(by='causes',ascending=False)[0:3].index[1]\n",
    "    edge_cause_3 = causes_df.sort_values(by='causes',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    instant_cause_1 = isntantaneous_df.sort_values(by='instantaneous',ascending=False)[0:3].index[0]\n",
    "    instant_cause_2 = isntantaneous_df.sort_values(by='instantaneous',ascending=False)[0:3].index[1]\n",
    "    instant_cause_3 = isntantaneous_df.sort_values(by='instantaneous',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    lag_cause_1 = lagged_df.sort_values(by='lagged',ascending=False)[0:3].index[0]\n",
    "    lag_cause_2 = lagged_df.sort_values(by='lagged',ascending=False)[0:3].index[1]\n",
    "    lag_cause_3 = lagged_df.sort_values(by='lagged',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    counterfactual_cause_1 = counterfactual_df.sort_values(by='counterfactuals',ascending=False)[0:3].index[0]\n",
    "    counterfactual_cause_2 = counterfactual_df.sort_values(by='counterfactuals',ascending=False)[0:3].index[1]\n",
    "    counterfactual_cause_3 = counterfactual_df.sort_values(by='counterfactuals',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    total_score_cause_1=total_score.sort_values(by='causal_strength',ascending=False)[0:3].index[0]\n",
    "    total_score_cause_2=total_score.sort_values(by='causal_strength',ascending=False)[0:3].index[1]\n",
    "    total_score_cause_3=total_score.sort_values(by='causal_strength',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    if root_cause == edge_cause_1:\n",
    "        row['edge_cause_1'] = 1\n",
    "    if root_cause == edge_cause_1:\n",
    "        row['edge_cause_2'] = 1\n",
    "    if root_cause == edge_cause_1:\n",
    "        row['edge_cause_3'] = 1\n",
    "    \n",
    "    if root_cause == counterfactual_cause_1:\n",
    "        row['counterfactual_cause_1'] = 1\n",
    "    if root_cause == counterfactual_cause_2:\n",
    "        row['counterfactual_cause_2'] = 1\n",
    "    if root_cause == counterfactual_cause_3:\n",
    "        row['counterfactual_cause_3'] = 1\n",
    "    \n",
    "    \n",
    "    if root_cause == total_score_cause_1:\n",
    "        row['total_score_cause_1'] = 1\n",
    "    if root_cause == total_score_cause_2:\n",
    "        row['total_score_cause_2'] = 1\n",
    "    if root_cause == total_score_cause_3:\n",
    "        row['total_score_cause_3'] = 1\n",
    "    \n",
    "    if root_cause == instant_cause_1:\n",
    "        row['instant_cause_1'] = 1\n",
    "    if root_cause == instant_cause_2:\n",
    "        row['instant_cause_2'] = 1\n",
    "    if root_cause == instant_cause_3:\n",
    "        row['instant_cause_3'] = 1\n",
    "    \n",
    "    if root_cause == lag_cause_1:\n",
    "        row['lag_cause_1'] = 1\n",
    "    if root_cause == lag_cause_2:\n",
    "        row['lag_cause_2'] = 1\n",
    "    if root_cause == lag_cause_3:\n",
    "        row['lag_cause_3'] = 1\n",
    "    \n",
    "    new_metadata.append(row)\n",
    "    \n",
    "    if root_cause in [total_score_cause_1 , total_score_cause_2 , total_score_cause_3]:\n",
    "        total_correct+=1\n",
    "    if root_cause in [edge_cause_1 , edge_cause_2 , edge_cause_3]:\n",
    "        edge_correct+=1\n",
    "    if root_cause in [counterfactual_cause_1 , counterfactual_cause_2 , counterfactual_cause_3]:\n",
    "        counterfactual_correct+=1\n",
    "    if root_cause in [instant_cause_1 , instant_cause_2 , instant_cause_3]:\n",
    "        instantaneous_correct+=1\n",
    "    if root_cause in [lag_cause_1 , lag_cause_2 , lag_cause_3]:\n",
    "        lagged_correct+=1\n",
    "    \n",
    "    total_accuracy = total_correct/total_checked* 100\n",
    "    edge_accuracy = edge_correct/total_checked* 100\n",
    "    cf_accuracy = counterfactual_correct/total_checked* 100\n",
    "    instant_accuracy = instantaneous_correct/total_checked* 100\n",
    "    lag_accuracy = lagged_correct/total_checked* 100\n",
    "    \n",
    "    if root_cause not in [total_score_cause_1 , total_score_cause_2 , total_score_cause_3,edge_cause_1 , edge_cause_2 , edge_cause_3 ]:\n",
    "        incorrect.append(i) \n",
    "    logger.info(f\"Edge Accuracy = {edge_accuracy:.2f}, Instantaneous Accuracy = {instant_accuracy:.2f}, Lagged Accuracy = {lag_accuracy:.2f}, Counterfactual Accuracy = {cf_accuracy:.2f},  Blended Accuracy = {total_accuracy:.2f}  \") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da074e-a5dd-456e-83b3-d94c64f41043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rca",
   "language": "python",
   "name": "rca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
