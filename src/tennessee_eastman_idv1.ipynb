{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3559a8bd-d3ef-4fc1-9d11-9adbc07d2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from utils.utils import set_seed\n",
    "set_seed()\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from torch_geometric.nn import GATv2Conv, GATConv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch.distributions import Normal, Laplace, RelaxedOneHotCategorical\n",
    "from torchdiffeq import odeint  # For continuous-time normalizing flows\n",
    "\n",
    "from feature.scalers import ranged_scaler\n",
    "from feature.engineering import *\n",
    "from CARAT.model_utils import *\n",
    "from CARAT.model import CausalGraphVAE\n",
    "from CARAT.components import *\n",
    "from utils.utils import set_seed, logger\n",
    "\n",
    "# Torch settings\n",
    "#torch.use_deterministic_algorithms(False)\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "#torch.autograd.profiler.profile(enabled=False)\n",
    "#torch.autograd.profiler.emit_nvtx(enabled=False)\n",
    "#torch.autograd.set_detect_anomaly(mode=False)\n",
    "\n",
    "# Environment variables\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "# Set device\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "data = []\n",
    "with open('data/TEP/idv1/y.dat', 'r') as file:\n",
    "    for line in file:\n",
    "        columns = line.strip().split(\"\\t\")\n",
    "        data.append([float(col.strip()) for col in columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bcf9903-62ed-4eec-9a22-28d013172a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = len(data[0])\n",
    "col_names = []\n",
    "for i in  range(0,vars):\n",
    "    col_names.append('x_'+str(i+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4186cd19-2804-4acc-9e80-651792a44849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from feature.scalers import ranged_scaler\n",
    "df = pl.DataFrame(data,schema=col_names)\n",
    "for col in df.columns:\n",
    "    df = df.with_columns(ranged_scaler(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb09efe-a50e-4f3f-b8d7-33e501d19436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: x_1 | Final ADF: -5.4811 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_2 | Final ADF: -3.6982 | p-value: 0.0003 | Diffs: 0\n",
      "Column: x_3 | Final ADF: -3.6538 | p-value: 0.0003 | Diffs: 0\n",
      "Column: x_4 | Final ADF: -3.3276 | p-value: 0.0010 | Diffs: 0\n",
      "Column: x_5 | Final ADF: -28.9652 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_6 | Final ADF: -27.3830 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_7 | Final ADF: -5.1194 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_8 | Final ADF: -30.8786 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_9 | Final ADF: -3.0565 | p-value: 0.0024 | Diffs: 0\n",
      "Column: x_10 | Final ADF: -5.8785 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_11 | Final ADF: -5.5060 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_12 | Final ADF: -27.0496 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_13 | Final ADF: -5.5245 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_14 | Final ADF: -29.4428 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_15 | Final ADF: -23.7157 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_16 | Final ADF: -5.1058 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_17 | Final ADF: -30.4505 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_18 | Final ADF: -4.5867 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_19 | Final ADF: -24.7012 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_20 | Final ADF: -7.6420 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_21 | Final ADF: -9.3515 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_22 | Final ADF: -7.0943 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_23 | Final ADF: -24.8033 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_24 | Final ADF: -24.7994 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_25 | Final ADF: -27.1527 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_26 | Final ADF: -3.2823 | p-value: 0.0012 | Diffs: 0\n",
      "Column: x_27 | Final ADF: -30.0469 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_28 | Final ADF: -27.5087 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_29 | Final ADF: -22.6222 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_30 | Final ADF: -20.6208 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_31 | Final ADF: -21.2602 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_32 | Final ADF: -4.1043 | p-value: 0.0001 | Diffs: 0\n",
      "Column: x_33 | Final ADF: -27.7522 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_34 | Final ADF: -24.9966 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_35 | Final ADF: -26.3901 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_36 | Final ADF: -30.5467 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_37 | Final ADF: -3.3945 | p-value: 0.0008 | Diffs: 0\n",
      "Column: x_38 | Final ADF: -17.4584 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_39 | Final ADF: -3.0542 | p-value: 0.0025 | Diffs: 0\n",
      "Column: x_40 | Final ADF: -20.1681 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_41 | Final ADF: -21.8081 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_42 | Final ADF: -15.5148 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_43 | Final ADF: -11.3392 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_44 | Final ADF: -8.3272 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_45 | Final ADF: -6.6655 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_46 | Final ADF: -4.5653 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_47 | Final ADF: -3.8511 | p-value: 0.0001 | Diffs: 1\n",
      "Column: x_48 | Final ADF: -6.2267 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_49 | Final ADF: -4.0029 | p-value: 0.0001 | Diffs: 1\n",
      "Column: x_50 | Final ADF: -5.5598 | p-value: 0.0000 | Diffs: 1\n",
      "Column: x_51 | Final ADF: -5.9882 | p-value: 0.0000 | Diffs: 1\n"
     ]
    }
   ],
   "source": [
    "df = make_stationary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f176e5-8328-47d3-898c-620aec152f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d5211d2-8b0d-48f1-9863-9986f68f65b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random dates, trivial for this exercise\n",
    "\n",
    "start_date = '2023-03-01'  # Define the start date\n",
    "date_range = pd.date_range(start=start_date, periods=df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e78764d-6061-4347-a15b-42915ce98352",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lags = most_frequent(find_optimal_lags_for_dataframe(df))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c41f2494-6c22-46f8-921d-dc3dd99a8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time']=date_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b131b6f5-0728-4a5f-a866-7b964d369d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='None', ylabel='x_44'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(data=df,y='x_44',x=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e39ac1b-36cc-429c-809f-01b016584bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['x_1', 'x_4','x_7','x_13','x_16','x_18','x_19','x_44','x_50']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ba3bdf6-cc0e-4f60-bd0d-ef5be924498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = df[200:]\n",
    "bad = df[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b1ffb3d-85b0-4e90-be11-2e218b5f69b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['x_1', 'x_4', 'x_7', 'x_13', 'x_16', 'x_18', 'x_19', 'x_44', 'x_50'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d3dc758-21df-44c7-bc12-ceb0dc3e36da",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = df.drop('time',axis=1)\n",
    "except:\n",
    "    None\n",
    "\n",
    "try:\n",
    "    normal = normal.drop('time',axis=1)\n",
    "except:\n",
    "    None\n",
    "try:\n",
    "    bad = bad.drop('time',axis=1)\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47a52bc7-6830-4a25-8c76-7d167da5d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edb92727-92ff-4c95-ab54-be1c0a279212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_1', 'x_4', 'x_7', 'x_13', 'x_16', 'x_18', 'x_19', 'x_44', 'x_50']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645900e2-8fb0-4eb4-895b-f1d6c4c63c58",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44039614-1138-4a1b-8aaa-3244219d7554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define the seed value\n",
    "seed = 42\n",
    "\n",
    "# Set seed for PyTorch\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Set seed for CUDA (if using GPUs)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "\n",
    "# Set seed for Python's random module\n",
    "random.seed(seed)\n",
    "\n",
    "# Set seed for NumPy\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Ensure deterministic behavior for PyTorch operations\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "344bfeb6-d2fd-4476-adda-89829a6bc1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 301.9690\n",
      "Recon Loss = 320.1398, KL Loss = 0.0000, Lagrangian Loss = 29.4586\n",
      "Epoch 51: Loss = 15.1541\n",
      "Recon Loss = 15.0322, KL Loss = 0.7678, Lagrangian Loss = 6.3340\n",
      "Epoch 101: Loss = 5.9774\n",
      "Recon Loss = 5.4800, KL Loss = 0.5173, Lagrangian Loss = 2.7307\n",
      "Epoch 151: Loss = 3.7469\n",
      "Recon Loss = 3.4642, KL Loss = 0.3201, Lagrangian Loss = 1.9046\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_13</th>\n",
       "      <th>x_16</th>\n",
       "      <th>x_18</th>\n",
       "      <th>x_19</th>\n",
       "      <th>x_44</th>\n",
       "      <th>x_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x_1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.463701</td>\n",
       "      <td>0.331602</td>\n",
       "      <td>0.357082</td>\n",
       "      <td>0.332337</td>\n",
       "      <td>0.446658</td>\n",
       "      <td>0.325793</td>\n",
       "      <td>0.330286</td>\n",
       "      <td>0.322070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_4</th>\n",
       "      <td>0.280336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356729</td>\n",
       "      <td>0.386473</td>\n",
       "      <td>0.404197</td>\n",
       "      <td>0.305971</td>\n",
       "      <td>0.384953</td>\n",
       "      <td>0.407404</td>\n",
       "      <td>0.302840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_7</th>\n",
       "      <td>0.322337</td>\n",
       "      <td>0.387143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.415536</td>\n",
       "      <td>0.332255</td>\n",
       "      <td>0.468466</td>\n",
       "      <td>0.347451</td>\n",
       "      <td>0.317093</td>\n",
       "      <td>0.378922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_13</th>\n",
       "      <td>0.284771</td>\n",
       "      <td>0.357019</td>\n",
       "      <td>0.285732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314734</td>\n",
       "      <td>0.350673</td>\n",
       "      <td>0.400621</td>\n",
       "      <td>0.369002</td>\n",
       "      <td>0.323369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_16</th>\n",
       "      <td>0.283832</td>\n",
       "      <td>0.420236</td>\n",
       "      <td>0.353251</td>\n",
       "      <td>0.299617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.318161</td>\n",
       "      <td>0.317730</td>\n",
       "      <td>0.315012</td>\n",
       "      <td>0.312234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_18</th>\n",
       "      <td>0.371586</td>\n",
       "      <td>0.340347</td>\n",
       "      <td>0.283001</td>\n",
       "      <td>0.410161</td>\n",
       "      <td>0.308738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311795</td>\n",
       "      <td>0.422890</td>\n",
       "      <td>0.392977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_19</th>\n",
       "      <td>0.311184</td>\n",
       "      <td>0.412615</td>\n",
       "      <td>0.289547</td>\n",
       "      <td>0.305193</td>\n",
       "      <td>0.327827</td>\n",
       "      <td>0.336896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288606</td>\n",
       "      <td>0.415722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_44</th>\n",
       "      <td>0.369653</td>\n",
       "      <td>0.419239</td>\n",
       "      <td>0.358901</td>\n",
       "      <td>0.412631</td>\n",
       "      <td>0.405392</td>\n",
       "      <td>0.334310</td>\n",
       "      <td>0.399950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.318081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_50</th>\n",
       "      <td>0.298846</td>\n",
       "      <td>0.308027</td>\n",
       "      <td>0.315350</td>\n",
       "      <td>0.326999</td>\n",
       "      <td>0.404068</td>\n",
       "      <td>0.388763</td>\n",
       "      <td>0.302975</td>\n",
       "      <td>0.319219</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x_1       x_4       x_7      x_13      x_16      x_18      x_19  \\\n",
       "x_1   0.000000  0.463701  0.331602  0.357082  0.332337  0.446658  0.325793   \n",
       "x_4   0.280336  0.000000  0.356729  0.386473  0.404197  0.305971  0.384953   \n",
       "x_7   0.322337  0.387143  0.000000  0.415536  0.332255  0.468466  0.347451   \n",
       "x_13  0.284771  0.357019  0.285732  0.000000  0.314734  0.350673  0.400621   \n",
       "x_16  0.283832  0.420236  0.353251  0.299617  0.000000  0.318161  0.317730   \n",
       "x_18  0.371586  0.340347  0.283001  0.410161  0.308738  0.000000  0.311795   \n",
       "x_19  0.311184  0.412615  0.289547  0.305193  0.327827  0.336896  0.000000   \n",
       "x_44  0.369653  0.419239  0.358901  0.412631  0.405392  0.334310  0.399950   \n",
       "x_50  0.298846  0.308027  0.315350  0.326999  0.404068  0.388763  0.302975   \n",
       "\n",
       "          x_44      x_50  \n",
       "x_1   0.330286  0.322070  \n",
       "x_4   0.407404  0.302840  \n",
       "x_7   0.317093  0.378922  \n",
       "x_13  0.369002  0.323369  \n",
       "x_16  0.315012  0.312234  \n",
       "x_18  0.422890  0.392977  \n",
       "x_19  0.288606  0.415722  \n",
       "x_44  0.000000  0.318081  \n",
       "x_50  0.319219  0.000000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TIME_STEPS = 7\n",
    "BATCH_SIZE = 25\n",
    "hidden_dim = 64\n",
    "latent_dim = 8\n",
    "dataset_nominal = TimeSeriesDataset(normal, device=device, time_steps=TIME_STEPS)\n",
    "dataloader_nominal = DataLoader(dataset_nominal, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = CausalGraphVAE(input_dim=normal.shape[1], hidden_dim=hidden_dim,\n",
    "                        latent_dim=latent_dim, num_nodes=normal.shape[1],device=device,\n",
    "                        time_steps=TIME_STEPS, prior_adj=None,instantaneous_weight=0.1).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4, amsgrad =True)\n",
    "\n",
    "# Train on nominal data\n",
    "#print(\"Pretraining on nominal data...\")\n",
    "model.train_model(dataloader_nominal, optimizer, num_epochs=200, patience=20,\n",
    "                  BATCH_SIZE=BATCH_SIZE,rho_max=4,alpha_max=2)\n",
    "\n",
    "# Extract learned adjacency\n",
    "prior_adj = model.causal_graph.adj_mat.clone().detach()\n",
    "pd.DataFrame((prior_adj).cpu().detach().numpy(),index=cols,columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2890e7fc-e0e9-4420-84f2-8fa5c1907520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_stage_training(model, dataloader, num_epochs_stage1=100, num_epochs_stage2=100, \n",
    "                       patience_stage1=10, patience_stage2=10, lr_stage1=1e-3, lr_stage2=5e-4,\n",
    "                       batch_size=64, rho_max=30.0, alpha_max=15.0):\n",
    "    \"\"\"\n",
    "    Two-stage training process:\n",
    "    1. Train the reconstruction parts of CausalGraphVAE first\n",
    "    2. Freeze encoder/decoder and train only the TemporalCausalGraph\n",
    "    \n",
    "    Args:\n",
    "        model: CausalGraphVAE model\n",
    "        dataloader: Data loader for training\n",
    "        num_epochs_stage1: Number of epochs for stage 1 training\n",
    "        num_epochs_stage2: Number of epochs for stage 2 training\n",
    "        patience_stage1: Early stopping patience for stage 1\n",
    "        patience_stage2: Early stopping patience for stage 2\n",
    "        lr_stage1: Learning rate for stage 1\n",
    "        lr_stage2: Learning rate for stage 2\n",
    "        batch_size: Batch size\n",
    "        rho_max: Maximum rho value for acyclicity constraint\n",
    "        alpha_max: Maximum alpha value for acyclicity constraint\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "    print(\"=== Starting Stage 1: Training Reconstruction Network ===\")\n",
    "    \n",
    "    # Stage 1: Train for reconstruction\n",
    "    # Define the optimizer for stage 1\n",
    "    optimizer_stage1 = optim.Adam(model.parameters(), lr=lr_stage1)\n",
    "    scheduler_stage1 = ReduceLROnPlateau(optimizer_stage1, 'min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs_stage1):\n",
    "        total_recon_loss = 0\n",
    "        total_kl_loss = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_idx, (X_batch, time_batch) in enumerate(dataloader):\n",
    "            if X_batch.shape[0] < X_batch.shape[2]:\n",
    "                continue\n",
    "                \n",
    "            optimizer_stage1.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            recon_X, mu, logvar, adj_now, adj_lag = model(X_batch, time_batch)\n",
    "            \n",
    "            # Focus only on reconstruction and KL loss\n",
    "            recon_loss = torch.nn.functional.mse_loss(recon_X, X_batch, reduction='sum')\n",
    "            beta = min(1.0, epoch / (num_epochs_stage1 * 0.3))  # Gradually increase KL weight\n",
    "            kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()) * beta\n",
    "            \n",
    "            # Total loss (ignoring graph constraints in stage 1)\n",
    "            loss = recon_loss + kl_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_stage1.step()\n",
    "            \n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_kl_loss += kl_loss.item()\n",
    "            total_samples += X_batch.size(0)\n",
    "        \n",
    "        avg_recon_loss = total_recon_loss / len(dataloader)\n",
    "        avg_kl_loss = total_kl_loss / len(dataloader)\n",
    "        avg_loss = avg_recon_loss + avg_kl_loss\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Stage 1 - Epoch {epoch + 1}: Recon Loss = {avg_recon_loss:.4f}, KL Loss = {avg_kl_loss:.4f}\")\n",
    "        \n",
    "        scheduler_stage1.step(avg_loss)\n",
    "        \n",
    "        # Early stopping based on reconstruction loss\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience_stage1:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                print(f\"Final Stage 1 - Recon Loss = {avg_recon_loss:.4f}, KL Loss = {avg_kl_loss:.4f}\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "    print(\"\\n=== Starting Stage 2: Training Causal Graph Discovery ===\")\n",
    "    \n",
    "    # Stage 2: Freeze encoder/decoder, train only the causal graph components\n",
    "    # Make sure alpha and rho are stored as simple Python values, not tensors requiring gradients\n",
    "    model.rho = float(model.rho)\n",
    "    model.alpha = float(model.alpha)\n",
    "    \n",
    "    # Freeze parameters of encoder and decoder components\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(x in name for x in ['enocder_projection', 'encoder_norm', 'encoder_transformer', \n",
    "                                   'mu_layer', 'logvar_layer', 'decoder_projection', \n",
    "                                   'decoder_norm', 'decoder_transformer', 'decoder_fc',\n",
    "                                   'pos_embedding']):\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Only optimize parameters of the causal graph component\n",
    "    causal_graph_params = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            causal_graph_params.append(param)\n",
    "    \n",
    "    optimizer_stage2 = optim.Adam(causal_graph_params, lr=lr_stage2)\n",
    "    scheduler_stage2 = ReduceLROnPlateau(optimizer_stage2, 'min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs_stage2):\n",
    "        total_lagrangian_loss = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_idx, (X_batch, time_batch) in enumerate(dataloader):\n",
    "            if X_batch.shape[0] < X_batch.shape[2]:\n",
    "                continue\n",
    "                \n",
    "            optimizer_stage2.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            recon_X, mu, logvar, adj_now, adj_lag = model(X_batch, time_batch)\n",
    "            \n",
    "            # Focus on graph structure loss\n",
    "            h_now = model.improved_acyclicity_constraint(adj_now)\n",
    "            h_lag = model.improved_acyclicity_constraint(adj_lag)\n",
    "            h_value = h_now * model.instantaneous_weight + h_lag * model.lag_weight\n",
    "            \n",
    "            # Ensure h_value is a tensor\n",
    "            if not isinstance(h_value, torch.Tensor):\n",
    "                h_value = torch.tensor(h_value, device=device)\n",
    "            \n",
    "            # Adaptive weights for DAG constraint\n",
    "            # Update these values as numbers, not tensors\n",
    "            current_rho = min(float(rho_max), 1.0 + (epoch / num_epochs_stage2) ** 2)\n",
    "            current_alpha = min(float(alpha_max), (epoch / num_epochs_stage2) ** 2)\n",
    "            \n",
    "            # Store for logging purposes only\n",
    "            model.rho = current_rho\n",
    "            model.alpha = current_alpha\n",
    "            \n",
    "            # Sparsity penalty\n",
    "            sparsity_loss = torch.norm(adj_now, p=1) * model.instantaneous_weight + torch.norm(adj_lag, p=1) * model.lag_weight\n",
    "            \n",
    "            # Lagrangian loss - use scalar values for alpha and rho\n",
    "            lagrangian_loss = current_alpha * h_value + 0.5 * current_rho * (h_value ** 2) + 0.01 * sparsity_loss\n",
    "            \n",
    "            lagrangian_loss.backward()\n",
    "            optimizer_stage2.step()\n",
    "            \n",
    "            total_lagrangian_loss += lagrangian_loss.item()\n",
    "            total_samples += X_batch.size(0)\n",
    "        \n",
    "        avg_lagrangian_loss = total_lagrangian_loss / len(dataloader)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Stage 2 - Epoch {epoch + 1}: Lagrangian Loss = {avg_lagrangian_loss:.4f}\")\n",
    "            # Safely extract values, handling both tensor and float types\n",
    "            h_value_scalar = h_value.item() if hasattr(h_value, 'item') else h_value\n",
    "            rho_scalar = model.rho.item() if hasattr(model.rho, 'item') else model.rho\n",
    "            alpha_scalar = model.alpha.item() if hasattr(model.alpha, 'item') else model.alpha\n",
    "            print(f\"h_value = {h_value_scalar:.6f}, Rho = {rho_scalar:.2f}, Alpha = {alpha_scalar:.2f}\")\n",
    "        \n",
    "        scheduler_stage2.step(avg_lagrangian_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_lagrangian_loss < best_loss:\n",
    "            best_loss = avg_lagrangian_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience_stage2:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                print(f\"Final Stage 2 - Lagrangian Loss = {avg_lagrangian_loss:.4f}\")\n",
    "                h_value_scalar = h_value.item() if hasattr(h_value, 'item') else h_value\n",
    "                print(f\"Final h_value = {h_value_scalar:.6f}\")\n",
    "                break\n",
    "    \n",
    "    # Unfreeze all parameters for any future use\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    print(\"Two-stage training completed!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0393fb88-1824-44e9-ae93-8fa9cc369af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bad = TimeSeriesDataset(bad, device=device, time_steps=TIME_STEPS)\n",
    "dataloader_bad = DataLoader(dataset_bad, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "fine_tuned = CausalGraphVAE(input_dim=bad.shape[1], hidden_dim=hidden_dim,\n",
    "                        latent_dim=latent_dim, num_nodes=bad.shape[1],device=device,\n",
    "                        time_steps=TIME_STEPS, prior_adj=prior_adj,instantaneous_weight=0.2).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab125afa-eaba-4c0a-b0d6-c05ae4d46c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 559.5884, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 91.7314, KL Loss = 0.0687\n",
      "Stage 1 - Epoch 21: Recon Loss = 63.0323, KL Loss = 0.1927\n",
      "Stage 1 - Epoch 31: Recon Loss = 49.0818, KL Loss = 0.3347\n",
      "Stage 1 - Epoch 41: Recon Loss = 40.2408, KL Loss = 0.4916\n",
      "Stage 1 - Epoch 51: Recon Loss = 32.5007, KL Loss = 0.6407\n",
      "Stage 1 - Epoch 61: Recon Loss = 27.5611, KL Loss = 0.7706\n",
      "Stage 1 - Epoch 71: Recon Loss = 23.0593, KL Loss = 0.8891\n",
      "Stage 1 - Epoch 81: Recon Loss = 19.4304, KL Loss = 0.9321\n",
      "Stage 1 - Epoch 91: Recon Loss = 16.6849, KL Loss = 0.8826\n",
      "Stage 1 - Epoch 101: Recon Loss = 14.6488, KL Loss = 0.8315\n",
      "Stage 1 - Epoch 111: Recon Loss = 12.9135, KL Loss = 0.7964\n",
      "Stage 1 - Epoch 121: Recon Loss = 10.9726, KL Loss = 0.7623\n",
      "Stage 1 - Epoch 131: Recon Loss = 10.1085, KL Loss = 0.7320\n",
      "Stage 1 - Epoch 141: Recon Loss = 8.7383, KL Loss = 0.7073\n",
      "Stage 1 - Epoch 151: Recon Loss = 8.0766, KL Loss = 0.6693\n",
      "Stage 1 - Epoch 161: Recon Loss = 7.3837, KL Loss = 0.6346\n",
      "Stage 1 - Epoch 171: Recon Loss = 6.8476, KL Loss = 0.6036\n",
      "Stage 1 - Epoch 181: Recon Loss = 6.2510, KL Loss = 0.5609\n",
      "Stage 1 - Epoch 191: Recon Loss = 5.7992, KL Loss = 0.5346\n",
      "Stage 1 - Epoch 201: Recon Loss = 5.5141, KL Loss = 0.5014\n",
      "Stage 1 - Epoch 211: Recon Loss = 5.1501, KL Loss = 0.4778\n",
      "Stage 1 - Epoch 221: Recon Loss = 5.0801, KL Loss = 0.4601\n",
      "Stage 1 - Epoch 231: Recon Loss = 5.1516, KL Loss = 0.4582\n",
      "Stage 1 - Epoch 241: Recon Loss = 4.9554, KL Loss = 0.4496\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 137.0262\n",
      "h_value = 16.379250, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 85.2198\n",
      "h_value = 12.913825, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 48.8617\n",
      "h_value = 9.667816, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 28.1618\n",
      "h_value = 7.276791, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 18.0162\n",
      "h_value = 5.816253, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 12.2886\n",
      "h_value = 4.802490, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 9.0695\n",
      "h_value = 4.017685, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 6.9228\n",
      "h_value = 3.547064, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 5.4986\n",
      "h_value = 3.011800, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 4.7185\n",
      "h_value = 2.748045, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 4.0862\n",
      "h_value = 2.521351, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 3.5982\n",
      "h_value = 2.213475, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 3.2460\n",
      "h_value = 2.029782, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 3.0213\n",
      "h_value = 1.898825, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 2.8551\n",
      "h_value = 1.805768, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 2.7027\n",
      "h_value = 1.663108, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 2.6232\n",
      "h_value = 1.625505, Rho = 1.41, Alpha = 0.41\n",
      "Stage 2 - Epoch 171: Lagrangian Loss = 2.4724\n",
      "h_value = 1.523467, Rho = 1.46, Alpha = 0.46\n",
      "Stage 2 - Epoch 181: Lagrangian Loss = 2.4166\n",
      "h_value = 1.431223, Rho = 1.52, Alpha = 0.52\n",
      "Stage 2 - Epoch 191: Lagrangian Loss = 2.3381\n",
      "h_value = 1.386148, Rho = 1.58, Alpha = 0.58\n",
      "Stage 2 - Epoch 201: Lagrangian Loss = 2.2469\n",
      "h_value = 1.325064, Rho = 1.64, Alpha = 0.64\n",
      "Stage 2 - Epoch 211: Lagrangian Loss = 2.2623\n",
      "h_value = 1.216916, Rho = 1.71, Alpha = 0.71\n",
      "Stage 2 - Epoch 221: Lagrangian Loss = 2.2882\n",
      "h_value = 1.188915, Rho = 1.77, Alpha = 0.77\n",
      "Early stopping triggered at epoch 224\n",
      "Final Stage 2 - Lagrangian Loss = 2.2727\n",
      "Final h_value = 1.155300\n",
      "Two-stage training completed!\n"
     ]
    }
   ],
   "source": [
    "mod = two_stage_training(fine_tuned, dataloader_bad, num_epochs_stage1=250, num_epochs_stage2=250, \n",
    "                       patience_stage1=10, patience_stage2=10, lr_stage1=1e-4, lr_stage2=5e-4,\n",
    "                       batch_size=64, rho_max=4.0, alpha_max=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7d0f3ea-fe7a-4ced-972a-c71fc1aab1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = torch.empty(0,device=device)\n",
    "T_data = torch.empty(0,device=device)\n",
    "for batch_idx, (X_batch, time_batch) in enumerate(dataloader_bad):\n",
    "    X_data = torch.cat((X_data[:batch_idx], X_batch, X_data[batch_idx:]))\n",
    "    T_data = torch.cat((T_data[:batch_idx], time_batch, T_data[batch_idx:]))\n",
    "\n",
    "causes= mod.infer_causal_effect(X_data,T_data,'x_4',cols,non_causal_indices=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93c64341-4720-4007-b314-ae1cb148b530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>causes</th>\n",
       "      <th>instantaneous</th>\n",
       "      <th>lagged</th>\n",
       "      <th>counterfactuals</th>\n",
       "      <th>causal_strength</th>\n",
       "      <th>RootRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x_1</th>\n",
       "      <td>0.968134</td>\n",
       "      <td>0.846212</td>\n",
       "      <td>0.993025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951842</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_18</th>\n",
       "      <td>0.951594</td>\n",
       "      <td>0.920713</td>\n",
       "      <td>0.926674</td>\n",
       "      <td>0.721191</td>\n",
       "      <td>0.880043</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_44</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955807</td>\n",
       "      <td>0.189421</td>\n",
       "      <td>0.786307</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_13</th>\n",
       "      <td>0.942407</td>\n",
       "      <td>0.880390</td>\n",
       "      <td>0.935178</td>\n",
       "      <td>0.271856</td>\n",
       "      <td>0.757458</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_16</th>\n",
       "      <td>0.982514</td>\n",
       "      <td>0.872790</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.081940</td>\n",
       "      <td>0.734311</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_19</th>\n",
       "      <td>0.966350</td>\n",
       "      <td>0.862052</td>\n",
       "      <td>0.981535</td>\n",
       "      <td>0.009714</td>\n",
       "      <td>0.704913</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_7</th>\n",
       "      <td>0.945484</td>\n",
       "      <td>0.870608</td>\n",
       "      <td>0.945270</td>\n",
       "      <td>0.056109</td>\n",
       "      <td>0.704368</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_50</th>\n",
       "      <td>0.959366</td>\n",
       "      <td>0.903166</td>\n",
       "      <td>0.948160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.702673</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        causes  instantaneous    lagged  counterfactuals  causal_strength  \\\n",
       "x_1   0.968134       0.846212  0.993025         1.000000         0.951842   \n",
       "x_18  0.951594       0.920713  0.926674         0.721191         0.880043   \n",
       "x_44  1.000000       1.000000  0.955807         0.189421         0.786307   \n",
       "x_13  0.942407       0.880390  0.935178         0.271856         0.757458   \n",
       "x_16  0.982514       0.872790  1.000000         0.081940         0.734311   \n",
       "x_19  0.966350       0.862052  0.981535         0.009714         0.704913   \n",
       "x_7   0.945484       0.870608  0.945270         0.056109         0.704368   \n",
       "x_50  0.959366       0.903166  0.948160         0.000000         0.702673   \n",
       "x_4   0.000000       0.000000  0.000000              NaN         0.000000   \n",
       "\n",
       "      RootRank  \n",
       "x_1      0.250  \n",
       "x_18     0.000  \n",
       "x_44     1.000  \n",
       "x_13     0.500  \n",
       "x_16     0.500  \n",
       "x_19     0.875  \n",
       "x_7      0.250  \n",
       "x_50     0.750  \n",
       "x_4      0.625  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5057eeb3-1ac1-4644-a2bd-9d6550a42207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 360.2719, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 88.5896, KL Loss = 0.0972\n",
      "Stage 1 - Epoch 21: Recon Loss = 57.4996, KL Loss = 0.2360\n",
      "Stage 1 - Epoch 31: Recon Loss = 43.0930, KL Loss = 0.3957\n",
      "Stage 1 - Epoch 41: Recon Loss = 34.0815, KL Loss = 0.5784\n",
      "Stage 1 - Epoch 51: Recon Loss = 27.2830, KL Loss = 0.7414\n",
      "Stage 1 - Epoch 61: Recon Loss = 23.1813, KL Loss = 0.8941\n",
      "Stage 1 - Epoch 71: Recon Loss = 19.1889, KL Loss = 0.9985\n",
      "Stage 1 - Epoch 81: Recon Loss = 15.9863, KL Loss = 1.0009\n",
      "Stage 1 - Epoch 91: Recon Loss = 13.5651, KL Loss = 0.9038\n",
      "Stage 1 - Epoch 101: Recon Loss = 11.4121, KL Loss = 0.8354\n",
      "Stage 1 - Epoch 111: Recon Loss = 10.0564, KL Loss = 0.7726\n",
      "Stage 1 - Epoch 121: Recon Loss = 8.6766, KL Loss = 0.6982\n",
      "Stage 1 - Epoch 131: Recon Loss = 7.8787, KL Loss = 0.6704\n",
      "Stage 1 - Epoch 141: Recon Loss = 7.0884, KL Loss = 0.6045\n",
      "Stage 1 - Epoch 151: Recon Loss = 6.4861, KL Loss = 0.5710\n",
      "Stage 1 - Epoch 161: Recon Loss = 5.8075, KL Loss = 0.5362\n",
      "Stage 1 - Epoch 171: Recon Loss = 5.5504, KL Loss = 0.4929\n",
      "Stage 1 - Epoch 181: Recon Loss = 5.1826, KL Loss = 0.4537\n",
      "Stage 1 - Epoch 191: Recon Loss = 4.6380, KL Loss = 0.4352\n",
      "Stage 1 - Epoch 201: Recon Loss = 4.4321, KL Loss = 0.3900\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.2777, KL Loss = 0.3630\n",
      "Stage 1 - Epoch 221: Recon Loss = 4.0375, KL Loss = 0.3380\n",
      "Stage 1 - Epoch 231: Recon Loss = 3.9678, KL Loss = 0.3159\n",
      "Stage 1 - Epoch 241: Recon Loss = 3.6999, KL Loss = 0.2985\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 17.9979\n",
      "h_value = 5.901423, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 11.1652\n",
      "h_value = 4.626593, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 6.8357\n",
      "h_value = 3.586357, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 4.2932\n",
      "h_value = 2.821551, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 2.9713\n",
      "h_value = 2.348176, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 2.2260\n",
      "h_value = 1.960941, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 1.7156\n",
      "h_value = 1.682186, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 1.3896\n",
      "h_value = 1.471280, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 1.1958\n",
      "h_value = 1.294429, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 1.0741\n",
      "h_value = 1.191080, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 0.9694\n",
      "h_value = 1.089855, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 0.8996\n",
      "h_value = 1.012205, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.8535\n",
      "h_value = 0.930100, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.8306\n",
      "h_value = 0.892632, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.7926\n",
      "h_value = 0.829223, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.7685\n",
      "h_value = 0.766031, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 0.7663\n",
      "h_value = 0.727252, Rho = 1.41, Alpha = 0.41\n",
      "Stage 2 - Epoch 171: Lagrangian Loss = 0.7612\n",
      "h_value = 0.689790, Rho = 1.46, Alpha = 0.46\n",
      "Early stopping triggered at epoch 173\n",
      "Final Stage 2 - Lagrangian Loss = 0.7699\n",
      "Final h_value = 0.695910\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 18:24:44,935 INFO -- Edge Accuracy = 0.00, Instantaneous Accuracy = 0.00, Lagged Accuracy = 0.00, Counterfactual Accuracy = 0.00,  Blended Accuracy = 0.00,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 445.4907, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 86.8348, KL Loss = 0.0790\n",
      "Stage 1 - Epoch 21: Recon Loss = 54.0634, KL Loss = 0.2096\n",
      "Stage 1 - Epoch 31: Recon Loss = 43.1174, KL Loss = 0.3645\n",
      "Stage 1 - Epoch 41: Recon Loss = 33.9958, KL Loss = 0.5336\n",
      "Stage 1 - Epoch 51: Recon Loss = 28.4221, KL Loss = 0.7268\n",
      "Stage 1 - Epoch 61: Recon Loss = 23.6444, KL Loss = 0.9093\n",
      "Stage 1 - Epoch 71: Recon Loss = 20.7065, KL Loss = 1.0670\n",
      "Stage 1 - Epoch 81: Recon Loss = 17.6079, KL Loss = 1.1281\n",
      "Stage 1 - Epoch 91: Recon Loss = 15.8473, KL Loss = 1.0901\n",
      "Stage 1 - Epoch 101: Recon Loss = 13.6993, KL Loss = 1.0622\n",
      "Stage 1 - Epoch 111: Recon Loss = 12.2719, KL Loss = 0.9954\n",
      "Stage 1 - Epoch 121: Recon Loss = 10.7028, KL Loss = 0.9274\n",
      "Stage 1 - Epoch 131: Recon Loss = 9.6279, KL Loss = 0.8503\n",
      "Stage 1 - Epoch 141: Recon Loss = 8.8764, KL Loss = 0.8178\n",
      "Stage 1 - Epoch 151: Recon Loss = 7.9729, KL Loss = 0.7617\n",
      "Stage 1 - Epoch 161: Recon Loss = 7.6360, KL Loss = 0.7171\n",
      "Stage 1 - Epoch 171: Recon Loss = 6.7971, KL Loss = 0.6924\n",
      "Stage 1 - Epoch 181: Recon Loss = 6.3318, KL Loss = 0.6478\n",
      "Stage 1 - Epoch 191: Recon Loss = 5.9038, KL Loss = 0.6116\n",
      "Stage 1 - Epoch 201: Recon Loss = 5.5926, KL Loss = 0.5721\n",
      "Stage 1 - Epoch 211: Recon Loss = 5.2004, KL Loss = 0.5494\n",
      "Stage 1 - Epoch 221: Recon Loss = 5.0094, KL Loss = 0.5136\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.6503, KL Loss = 0.4842\n",
      "Stage 1 - Epoch 241: Recon Loss = 4.3729, KL Loss = 0.4445\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 25.6293\n",
      "h_value = 7.037995, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 14.9560\n",
      "h_value = 5.348574, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 9.3870\n",
      "h_value = 4.241698, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 6.2437\n",
      "h_value = 3.445651, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 4.4468\n",
      "h_value = 2.853662, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 3.3416\n",
      "h_value = 2.438220, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 2.6030\n",
      "h_value = 2.106608, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 2.1153\n",
      "h_value = 1.856089, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 1.7860\n",
      "h_value = 1.669738, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 1.5516\n",
      "h_value = 1.484422, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 1.3957\n",
      "h_value = 1.341341, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 1.2562\n",
      "h_value = 1.234378, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 1.1764\n",
      "h_value = 1.153352, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 1.1124\n",
      "h_value = 1.063691, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 1.0531\n",
      "h_value = 1.003335, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 1.0104\n",
      "h_value = 0.921215, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 0.9858\n",
      "h_value = 0.885033, Rho = 1.41, Alpha = 0.41\n",
      "Stage 2 - Epoch 171: Lagrangian Loss = 0.9683\n",
      "h_value = 0.830123, Rho = 1.46, Alpha = 0.46\n",
      "Stage 2 - Epoch 181: Lagrangian Loss = 0.9645\n",
      "h_value = 0.796774, Rho = 1.52, Alpha = 0.52\n",
      "Early stopping triggered at epoch 185\n",
      "Final Stage 2 - Lagrangian Loss = 0.9669\n",
      "Final h_value = 0.770632\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 18:32:20,787 INFO -- Edge Accuracy = 50.00, Instantaneous Accuracy = 50.00, Lagged Accuracy = 50.00, Counterfactual Accuracy = 0.00,  Blended Accuracy = 0.00,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 418.6414, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 75.9323, KL Loss = 0.0889\n",
      "Stage 1 - Epoch 21: Recon Loss = 52.6692, KL Loss = 0.2314\n",
      "Stage 1 - Epoch 31: Recon Loss = 41.1838, KL Loss = 0.3817\n",
      "Stage 1 - Epoch 41: Recon Loss = 34.3638, KL Loss = 0.5436\n",
      "Stage 1 - Epoch 51: Recon Loss = 27.9822, KL Loss = 0.7014\n",
      "Stage 1 - Epoch 61: Recon Loss = 23.4552, KL Loss = 0.8379\n",
      "Stage 1 - Epoch 71: Recon Loss = 19.6104, KL Loss = 0.9669\n",
      "Stage 1 - Epoch 81: Recon Loss = 16.5442, KL Loss = 0.9897\n",
      "Stage 1 - Epoch 91: Recon Loss = 14.3943, KL Loss = 0.9212\n",
      "Stage 1 - Epoch 101: Recon Loss = 12.1433, KL Loss = 0.8603\n",
      "Stage 1 - Epoch 111: Recon Loss = 10.6921, KL Loss = 0.8208\n",
      "Stage 1 - Epoch 121: Recon Loss = 9.3665, KL Loss = 0.7626\n",
      "Stage 1 - Epoch 131: Recon Loss = 8.3500, KL Loss = 0.7179\n",
      "Stage 1 - Epoch 141: Recon Loss = 7.3203, KL Loss = 0.6911\n",
      "Stage 1 - Epoch 151: Recon Loss = 6.7444, KL Loss = 0.6458\n",
      "Stage 1 - Epoch 161: Recon Loss = 6.0842, KL Loss = 0.5936\n",
      "Stage 1 - Epoch 171: Recon Loss = 5.4910, KL Loss = 0.5737\n",
      "Stage 1 - Epoch 181: Recon Loss = 5.1928, KL Loss = 0.5401\n",
      "Stage 1 - Epoch 191: Recon Loss = 4.9099, KL Loss = 0.5068\n",
      "Stage 1 - Epoch 201: Recon Loss = 4.6094, KL Loss = 0.4913\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.7753, KL Loss = 0.4691\n",
      "Stage 1 - Epoch 221: Recon Loss = 4.3834, KL Loss = 0.4570\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.3264, KL Loss = 0.4359\n",
      "Stage 1 - Epoch 241: Recon Loss = 4.3227, KL Loss = 0.4342\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 13.7535\n",
      "h_value = 5.137167, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 7.4876\n",
      "h_value = 3.746033, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 4.5845\n",
      "h_value = 2.935601, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 2.9394\n",
      "h_value = 2.326078, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 2.0506\n",
      "h_value = 1.882978, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 1.5511\n",
      "h_value = 1.625833, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 1.2184\n",
      "h_value = 1.392911, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 0.9899\n",
      "h_value = 1.216060, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 0.8078\n",
      "h_value = 1.037028, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 0.6660\n",
      "h_value = 0.886697, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 0.6005\n",
      "h_value = 0.809423, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 0.5516\n",
      "h_value = 0.719578, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.5261\n",
      "h_value = 0.675627, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.5046\n",
      "h_value = 0.626088, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.5016\n",
      "h_value = 0.600815, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.4989\n",
      "h_value = 0.561946, Rho = 1.36, Alpha = 0.36\n",
      "Early stopping triggered at epoch 153\n",
      "Final Stage 2 - Lagrangian Loss = 0.5041\n",
      "Final h_value = 0.566110\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 18:39:18,487 INFO -- Edge Accuracy = 66.67, Instantaneous Accuracy = 33.33, Lagged Accuracy = 66.67, Counterfactual Accuracy = 0.00,  Blended Accuracy = 0.00,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 329.9357, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 73.1241, KL Loss = 0.0785\n",
      "Stage 1 - Epoch 21: Recon Loss = 48.5599, KL Loss = 0.2099\n",
      "Stage 1 - Epoch 31: Recon Loss = 37.2587, KL Loss = 0.3683\n",
      "Stage 1 - Epoch 41: Recon Loss = 30.6069, KL Loss = 0.5202\n",
      "Stage 1 - Epoch 51: Recon Loss = 25.2421, KL Loss = 0.6905\n",
      "Stage 1 - Epoch 61: Recon Loss = 21.2271, KL Loss = 0.8424\n",
      "Stage 1 - Epoch 71: Recon Loss = 18.4473, KL Loss = 0.9531\n",
      "Stage 1 - Epoch 81: Recon Loss = 15.6853, KL Loss = 0.9959\n",
      "Stage 1 - Epoch 91: Recon Loss = 13.7008, KL Loss = 0.9730\n",
      "Stage 1 - Epoch 101: Recon Loss = 11.6455, KL Loss = 0.9002\n",
      "Stage 1 - Epoch 111: Recon Loss = 10.2372, KL Loss = 0.8491\n",
      "Stage 1 - Epoch 121: Recon Loss = 8.8571, KL Loss = 0.8125\n",
      "Stage 1 - Epoch 131: Recon Loss = 7.9064, KL Loss = 0.7372\n",
      "Stage 1 - Epoch 141: Recon Loss = 7.3727, KL Loss = 0.6903\n",
      "Stage 1 - Epoch 151: Recon Loss = 6.6499, KL Loss = 0.6477\n",
      "Stage 1 - Epoch 161: Recon Loss = 6.1707, KL Loss = 0.6024\n",
      "Stage 1 - Epoch 171: Recon Loss = 5.5499, KL Loss = 0.5562\n",
      "Stage 1 - Epoch 181: Recon Loss = 5.1784, KL Loss = 0.5335\n",
      "Stage 1 - Epoch 191: Recon Loss = 4.7471, KL Loss = 0.4929\n",
      "Stage 1 - Epoch 201: Recon Loss = 4.4797, KL Loss = 0.4563\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.2064, KL Loss = 0.4163\n",
      "Stage 1 - Epoch 221: Recon Loss = 4.0499, KL Loss = 0.4039\n",
      "Stage 1 - Epoch 231: Recon Loss = 3.8598, KL Loss = 0.3711\n",
      "Stage 1 - Epoch 241: Recon Loss = 3.6453, KL Loss = 0.3550\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 8.3320\n",
      "h_value = 3.995195, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 4.9729\n",
      "h_value = 3.107573, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 2.9145\n",
      "h_value = 2.320019, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 1.7996\n",
      "h_value = 1.782137, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 1.2096\n",
      "h_value = 1.451497, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 0.8866\n",
      "h_value = 1.192914, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 0.6993\n",
      "h_value = 1.018889, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 0.6028\n",
      "h_value = 0.901365, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 0.5290\n",
      "h_value = 0.808690, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 0.4862\n",
      "h_value = 0.742563, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 0.4709\n",
      "h_value = 0.678041, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 0.4527\n",
      "h_value = 0.637378, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.4498\n",
      "h_value = 0.595995, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.4542\n",
      "h_value = 0.578882, Rho = 1.27, Alpha = 0.27\n",
      "Early stopping triggered at epoch 132\n",
      "Final Stage 2 - Lagrangian Loss = 0.4561\n",
      "Final h_value = 0.581342\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 18:45:51,957 INFO -- Edge Accuracy = 50.00, Instantaneous Accuracy = 25.00, Lagged Accuracy = 50.00, Counterfactual Accuracy = 25.00,  Blended Accuracy = 25.00,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 479.5578, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 88.4638, KL Loss = 0.0784\n",
      "Stage 1 - Epoch 21: Recon Loss = 56.0121, KL Loss = 0.2037\n",
      "Stage 1 - Epoch 31: Recon Loss = 42.8122, KL Loss = 0.3563\n",
      "Stage 1 - Epoch 41: Recon Loss = 33.8958, KL Loss = 0.5126\n",
      "Stage 1 - Epoch 51: Recon Loss = 28.4487, KL Loss = 0.6694\n",
      "Stage 1 - Epoch 61: Recon Loss = 24.3181, KL Loss = 0.8325\n",
      "Stage 1 - Epoch 71: Recon Loss = 21.8442, KL Loss = 0.9623\n",
      "Stage 1 - Epoch 81: Recon Loss = 18.5060, KL Loss = 1.0016\n",
      "Stage 1 - Epoch 91: Recon Loss = 16.5845, KL Loss = 0.9689\n",
      "Stage 1 - Epoch 101: Recon Loss = 14.6055, KL Loss = 0.9414\n",
      "Stage 1 - Epoch 111: Recon Loss = 13.1133, KL Loss = 0.8802\n",
      "Stage 1 - Epoch 121: Recon Loss = 11.9605, KL Loss = 0.8515\n",
      "Stage 1 - Epoch 131: Recon Loss = 10.5525, KL Loss = 0.8014\n",
      "Stage 1 - Epoch 141: Recon Loss = 9.7964, KL Loss = 0.7658\n",
      "Stage 1 - Epoch 151: Recon Loss = 9.0374, KL Loss = 0.7370\n",
      "Stage 1 - Epoch 161: Recon Loss = 7.9941, KL Loss = 0.7096\n",
      "Stage 1 - Epoch 171: Recon Loss = 7.6638, KL Loss = 0.6618\n",
      "Stage 1 - Epoch 181: Recon Loss = 6.8936, KL Loss = 0.6358\n",
      "Stage 1 - Epoch 191: Recon Loss = 6.3624, KL Loss = 0.6053\n",
      "Stage 1 - Epoch 201: Recon Loss = 5.9999, KL Loss = 0.5729\n",
      "Stage 1 - Epoch 211: Recon Loss = 5.5010, KL Loss = 0.5502\n",
      "Stage 1 - Epoch 221: Recon Loss = 5.2197, KL Loss = 0.5221\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.9636, KL Loss = 0.5017\n",
      "Stage 1 - Epoch 241: Recon Loss = 4.7652, KL Loss = 0.4731\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 7.4537\n",
      "h_value = 3.768247, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 4.4901\n",
      "h_value = 2.916106, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 2.6957\n",
      "h_value = 2.230842, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 1.7082\n",
      "h_value = 1.746252, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 1.1921\n",
      "h_value = 1.411611, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 0.8928\n",
      "h_value = 1.184001, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 0.7276\n",
      "h_value = 1.031293, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 0.6206\n",
      "h_value = 0.916757, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 0.5445\n",
      "h_value = 0.822194, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 0.5051\n",
      "h_value = 0.762162, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 0.4737\n",
      "h_value = 0.691564, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 0.4571\n",
      "h_value = 0.647964, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.4477\n",
      "h_value = 0.600461, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.4449\n",
      "h_value = 0.583388, Rho = 1.27, Alpha = 0.27\n",
      "Early stopping triggered at epoch 138\n",
      "Final Stage 2 - Lagrangian Loss = 0.4540\n",
      "Final h_value = 0.565255\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 18:52:35,044 INFO -- Edge Accuracy = 60.00, Instantaneous Accuracy = 20.00, Lagged Accuracy = 60.00, Counterfactual Accuracy = 20.00,  Blended Accuracy = 20.00,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 454.0530, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 84.8633, KL Loss = 0.1435\n",
      "Stage 1 - Epoch 21: Recon Loss = 55.1841, KL Loss = 0.3252\n",
      "Stage 1 - Epoch 31: Recon Loss = 42.1972, KL Loss = 0.5154\n",
      "Stage 1 - Epoch 41: Recon Loss = 33.9135, KL Loss = 0.6852\n",
      "Stage 1 - Epoch 51: Recon Loss = 27.0199, KL Loss = 0.8544\n",
      "Stage 1 - Epoch 61: Recon Loss = 22.7342, KL Loss = 0.9974\n",
      "Stage 1 - Epoch 71: Recon Loss = 19.5948, KL Loss = 1.1006\n",
      "Stage 1 - Epoch 81: Recon Loss = 16.8573, KL Loss = 1.0964\n",
      "Stage 1 - Epoch 91: Recon Loss = 14.3855, KL Loss = 1.0195\n",
      "Stage 1 - Epoch 101: Recon Loss = 12.6534, KL Loss = 0.9517\n",
      "Stage 1 - Epoch 111: Recon Loss = 10.8839, KL Loss = 0.9156\n",
      "Stage 1 - Epoch 121: Recon Loss = 9.7287, KL Loss = 0.8615\n",
      "Stage 1 - Epoch 131: Recon Loss = 8.9849, KL Loss = 0.8263\n",
      "Stage 1 - Epoch 141: Recon Loss = 7.9064, KL Loss = 0.7757\n",
      "Stage 1 - Epoch 151: Recon Loss = 7.2223, KL Loss = 0.7430\n",
      "Stage 1 - Epoch 161: Recon Loss = 6.7424, KL Loss = 0.7047\n",
      "Stage 1 - Epoch 171: Recon Loss = 6.1123, KL Loss = 0.6557\n",
      "Stage 1 - Epoch 181: Recon Loss = 5.4903, KL Loss = 0.6334\n",
      "Stage 1 - Epoch 191: Recon Loss = 5.2671, KL Loss = 0.6115\n",
      "Stage 1 - Epoch 201: Recon Loss = 4.9845, KL Loss = 0.5848\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.7635, KL Loss = 0.5396\n",
      "Stage 1 - Epoch 221: Recon Loss = 4.5534, KL Loss = 0.5162\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.1989, KL Loss = 0.4829\n",
      "Stage 1 - Epoch 241: Recon Loss = 4.0672, KL Loss = 0.4398\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 43.9708\n",
      "h_value = 9.274572, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 30.2238\n",
      "h_value = 7.651717, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 19.4651\n",
      "h_value = 6.111857, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 12.5328\n",
      "h_value = 4.887743, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 8.0309\n",
      "h_value = 3.843827, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 5.4203\n",
      "h_value = 3.118062, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 3.8954\n",
      "h_value = 2.589776, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 2.9791\n",
      "h_value = 2.217202, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 2.3879\n",
      "h_value = 1.928904, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 1.9953\n",
      "h_value = 1.706750, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 1.7278\n",
      "h_value = 1.530964, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 1.5236\n",
      "h_value = 1.377607, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 1.3864\n",
      "h_value = 1.260256, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 1.2868\n",
      "h_value = 1.161879, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 1.2110\n",
      "h_value = 1.075935, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 1.1481\n",
      "h_value = 1.006054, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 1.1081\n",
      "h_value = 0.948018, Rho = 1.41, Alpha = 0.41\n",
      "Stage 2 - Epoch 171: Lagrangian Loss = 1.0766\n",
      "h_value = 0.887607, Rho = 1.46, Alpha = 0.46\n",
      "Stage 2 - Epoch 181: Lagrangian Loss = 1.0600\n",
      "h_value = 0.831283, Rho = 1.52, Alpha = 0.52\n",
      "Stage 2 - Epoch 191: Lagrangian Loss = 1.0372\n",
      "h_value = 0.787230, Rho = 1.58, Alpha = 0.58\n",
      "Stage 2 - Epoch 201: Lagrangian Loss = 1.0253\n",
      "h_value = 0.745960, Rho = 1.64, Alpha = 0.64\n",
      "Stage 2 - Epoch 211: Lagrangian Loss = 1.0348\n",
      "h_value = 0.725200, Rho = 1.71, Alpha = 0.71\n",
      "Early stopping triggered at epoch 211\n",
      "Final Stage 2 - Lagrangian Loss = 1.0348\n",
      "Final h_value = 0.725200\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 19:00:44,115 INFO -- Edge Accuracy = 50.00, Instantaneous Accuracy = 16.67, Lagged Accuracy = 50.00, Counterfactual Accuracy = 16.67,  Blended Accuracy = 16.67,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 417.2436, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 98.4922, KL Loss = 0.0964\n",
      "Stage 1 - Epoch 21: Recon Loss = 64.1883, KL Loss = 0.2477\n",
      "Stage 1 - Epoch 31: Recon Loss = 47.6524, KL Loss = 0.4295\n",
      "Stage 1 - Epoch 41: Recon Loss = 38.9305, KL Loss = 0.6054\n",
      "Stage 1 - Epoch 51: Recon Loss = 32.1280, KL Loss = 0.7784\n",
      "Stage 1 - Epoch 61: Recon Loss = 27.1870, KL Loss = 0.9359\n",
      "Stage 1 - Epoch 71: Recon Loss = 23.5774, KL Loss = 1.0494\n",
      "Stage 1 - Epoch 81: Recon Loss = 19.6746, KL Loss = 1.0764\n",
      "Stage 1 - Epoch 91: Recon Loss = 17.0526, KL Loss = 1.0081\n",
      "Stage 1 - Epoch 101: Recon Loss = 14.8462, KL Loss = 0.9342\n",
      "Stage 1 - Epoch 111: Recon Loss = 12.6771, KL Loss = 0.8916\n",
      "Stage 1 - Epoch 121: Recon Loss = 11.1522, KL Loss = 0.8382\n",
      "Stage 1 - Epoch 131: Recon Loss = 9.8706, KL Loss = 0.7966\n",
      "Stage 1 - Epoch 141: Recon Loss = 8.5577, KL Loss = 0.7394\n",
      "Stage 1 - Epoch 151: Recon Loss = 7.8317, KL Loss = 0.7034\n",
      "Stage 1 - Epoch 161: Recon Loss = 6.9074, KL Loss = 0.6540\n",
      "Stage 1 - Epoch 171: Recon Loss = 6.5472, KL Loss = 0.6077\n",
      "Stage 1 - Epoch 181: Recon Loss = 5.9055, KL Loss = 0.5736\n",
      "Stage 1 - Epoch 191: Recon Loss = 5.4859, KL Loss = 0.5285\n",
      "Stage 1 - Epoch 201: Recon Loss = 5.1267, KL Loss = 0.4917\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.9261, KL Loss = 0.4582\n",
      "Stage 1 - Epoch 221: Recon Loss = 4.6548, KL Loss = 0.4189\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.2534, KL Loss = 0.3850\n",
      "Stage 1 - Epoch 241: Recon Loss = 4.1706, KL Loss = 0.3550\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 32.5834\n",
      "h_value = 7.968393, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 21.9681\n",
      "h_value = 6.543335, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 13.2619\n",
      "h_value = 5.045257, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 7.5757\n",
      "h_value = 3.768054, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 4.3707\n",
      "h_value = 2.794388, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 2.7623\n",
      "h_value = 2.195660, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 1.9451\n",
      "h_value = 1.801453, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 1.4925\n",
      "h_value = 1.514840, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 1.2271\n",
      "h_value = 1.324669, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 1.0432\n",
      "h_value = 1.192991, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 0.9341\n",
      "h_value = 1.080249, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 0.8507\n",
      "h_value = 0.956865, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.8035\n",
      "h_value = 0.908767, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.7688\n",
      "h_value = 0.866189, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.7393\n",
      "h_value = 0.784998, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.7213\n",
      "h_value = 0.739977, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 0.7169\n",
      "h_value = 0.707412, Rho = 1.41, Alpha = 0.41\n",
      "Stage 2 - Epoch 171: Lagrangian Loss = 0.7181\n",
      "h_value = 0.660001, Rho = 1.46, Alpha = 0.46\n",
      "Early stopping triggered at epoch 172\n",
      "Final Stage 2 - Lagrangian Loss = 0.7175\n",
      "Final h_value = 0.665044\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 19:08:05,644 INFO -- Edge Accuracy = 42.86, Instantaneous Accuracy = 14.29, Lagged Accuracy = 42.86, Counterfactual Accuracy = 14.29,  Blended Accuracy = 14.29,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 354.9846, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 77.4396, KL Loss = 0.0766\n",
      "Stage 1 - Epoch 21: Recon Loss = 55.6482, KL Loss = 0.1942\n",
      "Stage 1 - Epoch 31: Recon Loss = 42.9880, KL Loss = 0.3272\n",
      "Stage 1 - Epoch 41: Recon Loss = 34.7011, KL Loss = 0.4686\n",
      "Stage 1 - Epoch 51: Recon Loss = 28.6754, KL Loss = 0.6105\n",
      "Stage 1 - Epoch 61: Recon Loss = 24.6345, KL Loss = 0.7322\n",
      "Stage 1 - Epoch 71: Recon Loss = 21.2621, KL Loss = 0.8519\n",
      "Stage 1 - Epoch 81: Recon Loss = 18.0621, KL Loss = 0.8755\n",
      "Stage 1 - Epoch 91: Recon Loss = 15.4283, KL Loss = 0.8561\n",
      "Stage 1 - Epoch 101: Recon Loss = 13.2682, KL Loss = 0.8126\n",
      "Stage 1 - Epoch 111: Recon Loss = 11.8936, KL Loss = 0.7707\n",
      "Stage 1 - Epoch 121: Recon Loss = 10.1547, KL Loss = 0.7411\n",
      "Stage 1 - Epoch 131: Recon Loss = 8.8210, KL Loss = 0.6914\n",
      "Stage 1 - Epoch 141: Recon Loss = 7.9436, KL Loss = 0.6572\n",
      "Stage 1 - Epoch 151: Recon Loss = 7.0032, KL Loss = 0.6268\n",
      "Stage 1 - Epoch 161: Recon Loss = 6.4327, KL Loss = 0.5913\n",
      "Stage 1 - Epoch 171: Recon Loss = 5.9456, KL Loss = 0.5431\n",
      "Stage 1 - Epoch 181: Recon Loss = 5.2455, KL Loss = 0.5023\n",
      "Stage 1 - Epoch 191: Recon Loss = 5.1517, KL Loss = 0.4665\n",
      "Stage 1 - Epoch 201: Recon Loss = 4.6108, KL Loss = 0.4429\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.4020, KL Loss = 0.4028\n",
      "Stage 1 - Epoch 221: Recon Loss = 4.2385, KL Loss = 0.3862\n",
      "Stage 1 - Epoch 231: Recon Loss = 3.9336, KL Loss = 0.3662\n",
      "Stage 1 - Epoch 241: Recon Loss = 3.9477, KL Loss = 0.3545\n",
      "Early stopping triggered at epoch 249\n",
      "Final Stage 1 - Recon Loss = 3.9551, KL Loss = 0.3558\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 27.7439\n",
      "h_value = 7.359056, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 18.7812\n",
      "h_value = 6.023679, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 11.4757\n",
      "h_value = 4.624295, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 6.9073\n",
      "h_value = 3.564020, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 4.0841\n",
      "h_value = 2.729123, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 2.6652\n",
      "h_value = 2.190055, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 1.9379\n",
      "h_value = 1.799536, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 1.5275\n",
      "h_value = 1.532267, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 1.2742\n",
      "h_value = 1.366679, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 1.0941\n",
      "h_value = 1.210515, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 0.9870\n",
      "h_value = 1.078821, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 0.9065\n",
      "h_value = 1.017931, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.8330\n",
      "h_value = 0.920298, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.8058\n",
      "h_value = 0.857050, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.7691\n",
      "h_value = 0.827114, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.7648\n",
      "h_value = 0.753216, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 0.7347\n",
      "h_value = 0.702258, Rho = 1.41, Alpha = 0.41\n",
      "Stage 2 - Epoch 171: Lagrangian Loss = 0.7436\n",
      "h_value = 0.679839, Rho = 1.46, Alpha = 0.46\n",
      "Early stopping triggered at epoch 171\n",
      "Final Stage 2 - Lagrangian Loss = 0.7436\n",
      "Final h_value = 0.679839\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 19:15:27,886 INFO -- Edge Accuracy = 50.00, Instantaneous Accuracy = 25.00, Lagged Accuracy = 50.00, Counterfactual Accuracy = 12.50,  Blended Accuracy = 12.50,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 306.5313, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 71.4965, KL Loss = 0.1191\n",
      "Stage 1 - Epoch 21: Recon Loss = 49.8482, KL Loss = 0.3320\n",
      "Stage 1 - Epoch 31: Recon Loss = 37.3830, KL Loss = 0.5847\n",
      "Stage 1 - Epoch 41: Recon Loss = 28.5664, KL Loss = 0.8428\n",
      "Stage 1 - Epoch 51: Recon Loss = 25.9857, KL Loss = 1.0746\n",
      "Stage 1 - Epoch 61: Recon Loss = 21.4128, KL Loss = 1.2545\n",
      "Stage 1 - Epoch 71: Recon Loss = 18.3149, KL Loss = 1.3771\n",
      "Stage 1 - Epoch 81: Recon Loss = 16.0584, KL Loss = 1.3631\n",
      "Stage 1 - Epoch 91: Recon Loss = 14.1812, KL Loss = 1.2490\n",
      "Stage 1 - Epoch 101: Recon Loss = 12.6801, KL Loss = 1.1635\n",
      "Stage 1 - Epoch 111: Recon Loss = 11.1113, KL Loss = 1.0869\n",
      "Stage 1 - Epoch 121: Recon Loss = 10.1432, KL Loss = 1.0192\n",
      "Stage 1 - Epoch 131: Recon Loss = 8.7555, KL Loss = 0.9746\n",
      "Stage 1 - Epoch 141: Recon Loss = 8.0574, KL Loss = 0.9161\n",
      "Stage 1 - Epoch 151: Recon Loss = 7.2656, KL Loss = 0.8647\n",
      "Stage 1 - Epoch 161: Recon Loss = 6.7933, KL Loss = 0.8116\n",
      "Stage 1 - Epoch 171: Recon Loss = 6.2005, KL Loss = 0.7688\n",
      "Stage 1 - Epoch 181: Recon Loss = 5.7577, KL Loss = 0.7316\n",
      "Stage 1 - Epoch 191: Recon Loss = 5.3786, KL Loss = 0.6857\n",
      "Stage 1 - Epoch 201: Recon Loss = 4.9615, KL Loss = 0.6279\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.8282, KL Loss = 0.5809\n",
      "Stage 1 - Epoch 221: Recon Loss = 4.4042, KL Loss = 0.5382\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.3332, KL Loss = 0.4983\n",
      "Stage 1 - Epoch 241: Recon Loss = 4.1032, KL Loss = 0.4545\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 12.2076\n",
      "h_value = 4.862966, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 8.4933\n",
      "h_value = 4.046273, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 6.1445\n",
      "h_value = 3.422112, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 4.5528\n",
      "h_value = 2.921996, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 3.4457\n",
      "h_value = 2.508814, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 2.7059\n",
      "h_value = 2.178673, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 2.1932\n",
      "h_value = 1.910730, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 1.8205\n",
      "h_value = 1.717093, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 1.5529\n",
      "h_value = 1.530688, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 1.3673\n",
      "h_value = 1.371787, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 1.2206\n",
      "h_value = 1.248639, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 1.1175\n",
      "h_value = 1.140537, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 1.0493\n",
      "h_value = 1.066512, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.9896\n",
      "h_value = 0.984536, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.9395\n",
      "h_value = 0.918958, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.9137\n",
      "h_value = 0.866449, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 0.8850\n",
      "h_value = 0.805980, Rho = 1.41, Alpha = 0.41\n",
      "Stage 2 - Epoch 171: Lagrangian Loss = 0.8749\n",
      "h_value = 0.771638, Rho = 1.46, Alpha = 0.46\n",
      "Stage 2 - Epoch 181: Lagrangian Loss = 0.8692\n",
      "h_value = 0.727473, Rho = 1.52, Alpha = 0.52\n",
      "Stage 2 - Epoch 191: Lagrangian Loss = 0.8594\n",
      "h_value = 0.702118, Rho = 1.58, Alpha = 0.58\n",
      "Early stopping triggered at epoch 200\n",
      "Final Stage 2 - Lagrangian Loss = 0.8737\n",
      "Final h_value = 0.670220\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 19:23:23,371 INFO -- Edge Accuracy = 44.44, Instantaneous Accuracy = 33.33, Lagged Accuracy = 44.44, Counterfactual Accuracy = 11.11,  Blended Accuracy = 11.11,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 314.2557, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 78.4705, KL Loss = 0.1260\n",
      "Stage 1 - Epoch 21: Recon Loss = 52.4903, KL Loss = 0.3114\n",
      "Stage 1 - Epoch 31: Recon Loss = 40.4963, KL Loss = 0.5172\n",
      "Stage 1 - Epoch 41: Recon Loss = 34.0440, KL Loss = 0.7372\n",
      "Stage 1 - Epoch 51: Recon Loss = 28.0889, KL Loss = 0.9363\n",
      "Stage 1 - Epoch 61: Recon Loss = 23.9218, KL Loss = 1.0969\n",
      "Stage 1 - Epoch 71: Recon Loss = 21.0408, KL Loss = 1.2214\n",
      "Stage 1 - Epoch 81: Recon Loss = 18.5022, KL Loss = 1.2087\n",
      "Stage 1 - Epoch 91: Recon Loss = 15.6513, KL Loss = 1.1028\n",
      "Stage 1 - Epoch 101: Recon Loss = 13.7511, KL Loss = 1.0474\n",
      "Stage 1 - Epoch 111: Recon Loss = 11.9622, KL Loss = 0.9727\n",
      "Stage 1 - Epoch 121: Recon Loss = 10.6702, KL Loss = 0.9213\n",
      "Stage 1 - Epoch 131: Recon Loss = 9.3408, KL Loss = 0.8726\n",
      "Stage 1 - Epoch 141: Recon Loss = 8.7654, KL Loss = 0.8246\n",
      "Stage 1 - Epoch 151: Recon Loss = 7.9190, KL Loss = 0.7673\n",
      "Stage 1 - Epoch 161: Recon Loss = 7.0401, KL Loss = 0.7078\n",
      "Stage 1 - Epoch 171: Recon Loss = 6.4778, KL Loss = 0.6631\n",
      "Stage 1 - Epoch 181: Recon Loss = 5.9924, KL Loss = 0.6527\n",
      "Stage 1 - Epoch 191: Recon Loss = 5.5890, KL Loss = 0.6014\n",
      "Stage 1 - Epoch 201: Recon Loss = 5.1706, KL Loss = 0.5527\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.8855, KL Loss = 0.5228\n",
      "Stage 1 - Epoch 221: Recon Loss = 4.7450, KL Loss = 0.4930\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.4724, KL Loss = 0.4688\n",
      "Stage 1 - Epoch 241: Recon Loss = 4.2531, KL Loss = 0.4382\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 13.9297\n",
      "h_value = 5.174091, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 7.8618\n",
      "h_value = 3.877618, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 4.4869\n",
      "h_value = 2.899315, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 2.7388\n",
      "h_value = 2.239155, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 1.8505\n",
      "h_value = 1.786298, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 1.3786\n",
      "h_value = 1.519250, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 1.0933\n",
      "h_value = 1.296924, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 0.9163\n",
      "h_value = 1.145587, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 0.7976\n",
      "h_value = 1.028933, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 0.7173\n",
      "h_value = 0.940085, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 0.6594\n",
      "h_value = 0.862920, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 0.6191\n",
      "h_value = 0.791911, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.5895\n",
      "h_value = 0.744325, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.5760\n",
      "h_value = 0.687768, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.5648\n",
      "h_value = 0.646646, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.5613\n",
      "h_value = 0.613944, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 0.5618\n",
      "h_value = 0.587066, Rho = 1.41, Alpha = 0.41\n",
      "Early stopping triggered at epoch 165\n",
      "Final Stage 2 - Lagrangian Loss = 0.5680\n",
      "Final h_value = 0.583615\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 19:30:34,752 INFO -- Edge Accuracy = 40.00, Instantaneous Accuracy = 30.00, Lagged Accuracy = 40.00, Counterfactual Accuracy = 20.00,  Blended Accuracy = 20.00,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 677.9729, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 105.2057, KL Loss = 0.1138\n",
      "Stage 1 - Epoch 21: Recon Loss = 65.8451, KL Loss = 0.3056\n",
      "Stage 1 - Epoch 31: Recon Loss = 49.2336, KL Loss = 0.5196\n",
      "Stage 1 - Epoch 41: Recon Loss = 38.6152, KL Loss = 0.7225\n",
      "Stage 1 - Epoch 51: Recon Loss = 31.4157, KL Loss = 0.9537\n",
      "Stage 1 - Epoch 61: Recon Loss = 26.8815, KL Loss = 1.1273\n",
      "Stage 1 - Epoch 71: Recon Loss = 22.8721, KL Loss = 1.2796\n",
      "Stage 1 - Epoch 81: Recon Loss = 19.5876, KL Loss = 1.2920\n",
      "Stage 1 - Epoch 91: Recon Loss = 17.5760, KL Loss = 1.2020\n",
      "Stage 1 - Epoch 101: Recon Loss = 14.8286, KL Loss = 1.1029\n",
      "Stage 1 - Epoch 111: Recon Loss = 13.1416, KL Loss = 1.0059\n",
      "Stage 1 - Epoch 121: Recon Loss = 11.5928, KL Loss = 0.9404\n",
      "Stage 1 - Epoch 131: Recon Loss = 10.3671, KL Loss = 0.8616\n",
      "Stage 1 - Epoch 141: Recon Loss = 9.4441, KL Loss = 0.8082\n",
      "Stage 1 - Epoch 151: Recon Loss = 8.4369, KL Loss = 0.7524\n",
      "Stage 1 - Epoch 161: Recon Loss = 7.7566, KL Loss = 0.7403\n",
      "Stage 1 - Epoch 171: Recon Loss = 6.9892, KL Loss = 0.6943\n",
      "Stage 1 - Epoch 181: Recon Loss = 6.5280, KL Loss = 0.6559\n",
      "Stage 1 - Epoch 191: Recon Loss = 6.0180, KL Loss = 0.6253\n",
      "Stage 1 - Epoch 201: Recon Loss = 5.5876, KL Loss = 0.5975\n",
      "Stage 1 - Epoch 211: Recon Loss = 5.2640, KL Loss = 0.5592\n",
      "Stage 1 - Epoch 221: Recon Loss = 5.0385, KL Loss = 0.5295\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.8155, KL Loss = 0.5141\n",
      "Stage 1 - Epoch 241: Recon Loss = 4.3924, KL Loss = 0.4928\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 32.8725\n",
      "h_value = 8.036831, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 25.5204\n",
      "h_value = 7.062663, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 18.3446\n",
      "h_value = 5.946634, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 11.4842\n",
      "h_value = 4.651472, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 7.0213\n",
      "h_value = 3.597411, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 4.5833\n",
      "h_value = 2.864609, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 3.2427\n",
      "h_value = 2.352201, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 2.4526\n",
      "h_value = 1.992235, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 1.9670\n",
      "h_value = 1.737815, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 1.6293\n",
      "h_value = 1.506176, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 1.4068\n",
      "h_value = 1.355916, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 1.2606\n",
      "h_value = 1.232974, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 1.1472\n",
      "h_value = 1.124852, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 1.0721\n",
      "h_value = 1.030849, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 1.0197\n",
      "h_value = 0.966762, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.9790\n",
      "h_value = 0.906283, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 0.9522\n",
      "h_value = 0.845782, Rho = 1.41, Alpha = 0.41\n",
      "Stage 2 - Epoch 171: Lagrangian Loss = 0.9341\n",
      "h_value = 0.802528, Rho = 1.46, Alpha = 0.46\n",
      "Stage 2 - Epoch 181: Lagrangian Loss = 0.9182\n",
      "h_value = 0.760037, Rho = 1.52, Alpha = 0.52\n",
      "Stage 2 - Epoch 191: Lagrangian Loss = 0.9041\n",
      "h_value = 0.722366, Rho = 1.58, Alpha = 0.58\n",
      "Early stopping triggered at epoch 194\n",
      "Final Stage 2 - Lagrangian Loss = 0.9209\n",
      "Final h_value = 0.716836\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 19:38:24,805 INFO -- Edge Accuracy = 45.45, Instantaneous Accuracy = 27.27, Lagged Accuracy = 45.45, Counterfactual Accuracy = 18.18,  Blended Accuracy = 18.18,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 366.8478, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 75.3432, KL Loss = 0.0916\n",
      "Stage 1 - Epoch 21: Recon Loss = 48.4685, KL Loss = 0.2540\n",
      "Stage 1 - Epoch 31: Recon Loss = 36.2173, KL Loss = 0.4408\n",
      "Stage 1 - Epoch 41: Recon Loss = 29.9370, KL Loss = 0.6314\n",
      "Stage 1 - Epoch 51: Recon Loss = 25.1597, KL Loss = 0.8097\n",
      "Stage 1 - Epoch 61: Recon Loss = 20.9262, KL Loss = 0.9790\n",
      "Stage 1 - Epoch 71: Recon Loss = 18.2317, KL Loss = 1.1016\n",
      "Stage 1 - Epoch 81: Recon Loss = 15.9988, KL Loss = 1.1249\n",
      "Stage 1 - Epoch 91: Recon Loss = 13.7954, KL Loss = 1.0421\n",
      "Stage 1 - Epoch 101: Recon Loss = 11.7918, KL Loss = 0.9843\n",
      "Stage 1 - Epoch 111: Recon Loss = 10.4004, KL Loss = 0.9214\n",
      "Stage 1 - Epoch 121: Recon Loss = 9.4600, KL Loss = 0.8792\n",
      "Stage 1 - Epoch 131: Recon Loss = 8.2212, KL Loss = 0.8207\n",
      "Stage 1 - Epoch 141: Recon Loss = 7.3966, KL Loss = 0.7909\n",
      "Stage 1 - Epoch 151: Recon Loss = 6.8521, KL Loss = 0.7364\n",
      "Stage 1 - Epoch 161: Recon Loss = 6.1902, KL Loss = 0.6841\n",
      "Stage 1 - Epoch 171: Recon Loss = 5.8127, KL Loss = 0.6528\n",
      "Stage 1 - Epoch 181: Recon Loss = 5.1780, KL Loss = 0.6209\n",
      "Stage 1 - Epoch 191: Recon Loss = 5.0460, KL Loss = 0.5747\n",
      "Stage 1 - Epoch 201: Recon Loss = 4.7255, KL Loss = 0.5509\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.6033, KL Loss = 0.5224\n",
      "Stage 1 - Epoch 221: Recon Loss = 4.4207, KL Loss = 0.5090\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.2142, KL Loss = 0.4957\n",
      "Stage 1 - Epoch 241: Recon Loss = 4.2066, KL Loss = 0.4670\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 32.7581\n",
      "h_value = 7.947239, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 19.1994\n",
      "h_value = 6.119597, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 12.0509\n",
      "h_value = 4.828279, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 7.5616\n",
      "h_value = 3.755571, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 4.8130\n",
      "h_value = 3.034766, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 3.2788\n",
      "h_value = 2.432775, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 2.4536\n",
      "h_value = 2.011292, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 1.9472\n",
      "h_value = 1.735693, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 1.6058\n",
      "h_value = 1.487383, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 1.3870\n",
      "h_value = 1.433430, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 1.2472\n",
      "h_value = 1.264817, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 1.1489\n",
      "h_value = 1.138816, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 1.0467\n",
      "h_value = 1.050732, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.9889\n",
      "h_value = 0.980342, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.9456\n",
      "h_value = 0.932657, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.8989\n",
      "h_value = 0.846025, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 0.8822\n",
      "h_value = 0.806634, Rho = 1.41, Alpha = 0.41\n",
      "Stage 2 - Epoch 171: Lagrangian Loss = 0.8809\n",
      "h_value = 0.760301, Rho = 1.46, Alpha = 0.46\n",
      "Early stopping triggered at epoch 179\n",
      "Final Stage 2 - Lagrangian Loss = 0.8705\n",
      "Final h_value = 0.746799\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 19:45:55,751 INFO -- Edge Accuracy = 50.00, Instantaneous Accuracy = 25.00, Lagged Accuracy = 50.00, Counterfactual Accuracy = 16.67,  Blended Accuracy = 16.67,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 538.5498, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 94.4005, KL Loss = 0.1457\n",
      "Stage 1 - Epoch 21: Recon Loss = 61.5018, KL Loss = 0.3527\n",
      "Stage 1 - Epoch 31: Recon Loss = 48.7352, KL Loss = 0.5870\n",
      "Stage 1 - Epoch 41: Recon Loss = 40.5967, KL Loss = 0.8162\n",
      "Stage 1 - Epoch 51: Recon Loss = 33.1479, KL Loss = 1.0439\n",
      "Stage 1 - Epoch 61: Recon Loss = 28.4166, KL Loss = 1.2596\n",
      "Stage 1 - Epoch 71: Recon Loss = 25.0319, KL Loss = 1.4187\n",
      "Stage 1 - Epoch 81: Recon Loss = 21.0515, KL Loss = 1.4504\n",
      "Stage 1 - Epoch 91: Recon Loss = 18.9076, KL Loss = 1.3416\n",
      "Stage 1 - Epoch 101: Recon Loss = 16.2406, KL Loss = 1.2001\n",
      "Stage 1 - Epoch 111: Recon Loss = 13.7948, KL Loss = 1.0859\n",
      "Stage 1 - Epoch 121: Recon Loss = 12.4526, KL Loss = 0.9993\n",
      "Stage 1 - Epoch 131: Recon Loss = 11.4643, KL Loss = 0.9188\n",
      "Stage 1 - Epoch 141: Recon Loss = 10.1286, KL Loss = 0.8610\n",
      "Stage 1 - Epoch 151: Recon Loss = 9.2519, KL Loss = 0.8075\n",
      "Stage 1 - Epoch 161: Recon Loss = 8.4308, KL Loss = 0.7524\n",
      "Stage 1 - Epoch 171: Recon Loss = 7.7269, KL Loss = 0.7103\n",
      "Stage 1 - Epoch 181: Recon Loss = 7.2081, KL Loss = 0.6720\n",
      "Stage 1 - Epoch 191: Recon Loss = 6.4057, KL Loss = 0.6309\n",
      "Stage 1 - Epoch 201: Recon Loss = 6.1199, KL Loss = 0.6082\n",
      "Stage 1 - Epoch 211: Recon Loss = 5.4929, KL Loss = 0.5613\n",
      "Stage 1 - Epoch 221: Recon Loss = 5.3216, KL Loss = 0.5298\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.9607, KL Loss = 0.5052\n",
      "Stage 1 - Epoch 241: Recon Loss = 4.8839, KL Loss = 0.4661\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 19.5091\n",
      "h_value = 6.117267, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 10.8194\n",
      "h_value = 4.557106, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 6.4271\n",
      "h_value = 3.458153, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 4.0125\n",
      "h_value = 2.716393, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 2.6392\n",
      "h_value = 2.159163, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 1.8505\n",
      "h_value = 1.771525, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 1.3942\n",
      "h_value = 1.500186, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 1.1284\n",
      "h_value = 1.316378, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 0.9441\n",
      "h_value = 1.138074, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 0.8328\n",
      "h_value = 1.026673, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 0.7537\n",
      "h_value = 0.928525, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 0.6915\n",
      "h_value = 0.857635, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.6510\n",
      "h_value = 0.787428, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.6297\n",
      "h_value = 0.726560, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.6133\n",
      "h_value = 0.694366, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.6073\n",
      "h_value = 0.655309, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 0.6096\n",
      "h_value = 0.630585, Rho = 1.41, Alpha = 0.41\n",
      "Stage 2 - Epoch 171: Lagrangian Loss = 0.6193\n",
      "h_value = 0.599142, Rho = 1.46, Alpha = 0.46\n",
      "Early stopping triggered at epoch 174\n",
      "Final Stage 2 - Lagrangian Loss = 0.6280\n",
      "Final h_value = 0.603853\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 19:53:18,025 INFO -- Edge Accuracy = 46.15, Instantaneous Accuracy = 23.08, Lagged Accuracy = 46.15, Counterfactual Accuracy = 23.08,  Blended Accuracy = 23.08,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 421.7207, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 81.5206, KL Loss = 0.0610\n",
      "Stage 1 - Epoch 21: Recon Loss = 56.5250, KL Loss = 0.1844\n",
      "Stage 1 - Epoch 31: Recon Loss = 42.4007, KL Loss = 0.3368\n",
      "Stage 1 - Epoch 41: Recon Loss = 34.7343, KL Loss = 0.5114\n",
      "Stage 1 - Epoch 51: Recon Loss = 28.6248, KL Loss = 0.6706\n",
      "Stage 1 - Epoch 61: Recon Loss = 23.1592, KL Loss = 0.8123\n",
      "Stage 1 - Epoch 71: Recon Loss = 18.8942, KL Loss = 0.9383\n",
      "Stage 1 - Epoch 81: Recon Loss = 16.7724, KL Loss = 0.9682\n",
      "Stage 1 - Epoch 91: Recon Loss = 13.2609, KL Loss = 0.9312\n",
      "Stage 1 - Epoch 101: Recon Loss = 11.5181, KL Loss = 0.8733\n",
      "Stage 1 - Epoch 111: Recon Loss = 9.7810, KL Loss = 0.8016\n",
      "Stage 1 - Epoch 121: Recon Loss = 8.6774, KL Loss = 0.7695\n",
      "Stage 1 - Epoch 131: Recon Loss = 7.8174, KL Loss = 0.7165\n",
      "Stage 1 - Epoch 141: Recon Loss = 6.6973, KL Loss = 0.6874\n",
      "Stage 1 - Epoch 151: Recon Loss = 6.1852, KL Loss = 0.6064\n",
      "Stage 1 - Epoch 161: Recon Loss = 5.7892, KL Loss = 0.5693\n",
      "Stage 1 - Epoch 171: Recon Loss = 5.2037, KL Loss = 0.5245\n",
      "Stage 1 - Epoch 181: Recon Loss = 4.6097, KL Loss = 0.5047\n",
      "Stage 1 - Epoch 191: Recon Loss = 4.3356, KL Loss = 0.4653\n",
      "Stage 1 - Epoch 201: Recon Loss = 4.2398, KL Loss = 0.4545\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.2047, KL Loss = 0.4212\n",
      "Stage 1 - Epoch 221: Recon Loss = 3.9909, KL Loss = 0.4043\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.0287, KL Loss = 0.3991\n",
      "Early stopping triggered at epoch 234\n",
      "Final Stage 1 - Recon Loss = 3.9527, KL Loss = 0.3860\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 34.4140\n",
      "h_value = 8.196778, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 23.1610\n",
      "h_value = 6.727867, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 16.2728\n",
      "h_value = 5.637022, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 10.9891\n",
      "h_value = 4.543280, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 6.6536\n",
      "h_value = 3.486060, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 4.3327\n",
      "h_value = 2.756438, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 3.0458\n",
      "h_value = 2.276690, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 2.3054\n",
      "h_value = 1.946353, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 1.8615\n",
      "h_value = 1.667221, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 1.5411\n",
      "h_value = 1.487316, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 1.3278\n",
      "h_value = 1.312989, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 1.1919\n",
      "h_value = 1.197747, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 1.0969\n",
      "h_value = 1.097252, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 1.0189\n",
      "h_value = 1.005823, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.9768\n",
      "h_value = 0.940527, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.9312\n",
      "h_value = 0.880013, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 0.9040\n",
      "h_value = 0.830336, Rho = 1.41, Alpha = 0.41\n",
      "Stage 2 - Epoch 171: Lagrangian Loss = 0.8999\n",
      "h_value = 0.770685, Rho = 1.46, Alpha = 0.46\n",
      "Stage 2 - Epoch 181: Lagrangian Loss = 0.8783\n",
      "h_value = 0.727834, Rho = 1.52, Alpha = 0.52\n",
      "Early stopping triggered at epoch 190\n",
      "Final Stage 2 - Lagrangian Loss = 0.9102\n",
      "Final h_value = 0.714938\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 20:00:45,292 INFO -- Edge Accuracy = 42.86, Instantaneous Accuracy = 28.57, Lagged Accuracy = 42.86, Counterfactual Accuracy = 21.43,  Blended Accuracy = 21.43,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 286.3210, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 73.4487, KL Loss = 0.1071\n",
      "Stage 1 - Epoch 21: Recon Loss = 48.5038, KL Loss = 0.2681\n",
      "Stage 1 - Epoch 31: Recon Loss = 37.1570, KL Loss = 0.4490\n",
      "Stage 1 - Epoch 41: Recon Loss = 29.4334, KL Loss = 0.6233\n",
      "Stage 1 - Epoch 51: Recon Loss = 23.7826, KL Loss = 0.7986\n",
      "Stage 1 - Epoch 61: Recon Loss = 19.7055, KL Loss = 0.9381\n",
      "Stage 1 - Epoch 71: Recon Loss = 16.9479, KL Loss = 1.0461\n",
      "Stage 1 - Epoch 81: Recon Loss = 13.5583, KL Loss = 1.0291\n",
      "Stage 1 - Epoch 91: Recon Loss = 11.2027, KL Loss = 0.9318\n",
      "Stage 1 - Epoch 101: Recon Loss = 9.7004, KL Loss = 0.8308\n",
      "Stage 1 - Epoch 111: Recon Loss = 8.6280, KL Loss = 0.7534\n",
      "Stage 1 - Epoch 121: Recon Loss = 7.3197, KL Loss = 0.6969\n",
      "Stage 1 - Epoch 131: Recon Loss = 6.6224, KL Loss = 0.6578\n",
      "Stage 1 - Epoch 141: Recon Loss = 6.1202, KL Loss = 0.5984\n",
      "Stage 1 - Epoch 151: Recon Loss = 5.5914, KL Loss = 0.5514\n",
      "Stage 1 - Epoch 161: Recon Loss = 5.1390, KL Loss = 0.5084\n",
      "Stage 1 - Epoch 171: Recon Loss = 4.6118, KL Loss = 0.4737\n",
      "Stage 1 - Epoch 181: Recon Loss = 4.4897, KL Loss = 0.4255\n",
      "Stage 1 - Epoch 191: Recon Loss = 4.1352, KL Loss = 0.3928\n",
      "Stage 1 - Epoch 201: Recon Loss = 3.8999, KL Loss = 0.3649\n",
      "Stage 1 - Epoch 211: Recon Loss = 3.8221, KL Loss = 0.3349\n",
      "Stage 1 - Epoch 221: Recon Loss = 3.6064, KL Loss = 0.3056\n",
      "Stage 1 - Epoch 231: Recon Loss = 3.4739, KL Loss = 0.2686\n",
      "Stage 1 - Epoch 241: Recon Loss = 3.2520, KL Loss = 0.2407\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 19.2581\n",
      "h_value = 6.131622, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 12.1641\n",
      "h_value = 4.849632, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 7.9271\n",
      "h_value = 3.934657, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 5.3328\n",
      "h_value = 3.102531, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 3.6423\n",
      "h_value = 2.609148, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 2.7422\n",
      "h_value = 2.194222, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 2.0474\n",
      "h_value = 1.863778, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 1.6397\n",
      "h_value = 1.621593, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 1.3469\n",
      "h_value = 1.439674, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 1.1802\n",
      "h_value = 1.299127, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 1.0348\n",
      "h_value = 1.089880, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 0.9260\n",
      "h_value = 1.028074, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.8679\n",
      "h_value = 0.934483, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.8356\n",
      "h_value = 0.915086, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.8028\n",
      "h_value = 0.834763, Rho = 1.31, Alpha = 0.31\n",
      "Early stopping triggered at epoch 146\n",
      "Final Stage 2 - Lagrangian Loss = 0.8039\n",
      "Final h_value = 0.808108\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 20:07:40,022 INFO -- Edge Accuracy = 40.00, Instantaneous Accuracy = 26.67, Lagged Accuracy = 40.00, Counterfactual Accuracy = 20.00,  Blended Accuracy = 20.00,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 367.5409, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 84.9381, KL Loss = 0.0866\n",
      "Stage 1 - Epoch 21: Recon Loss = 56.9613, KL Loss = 0.2138\n",
      "Stage 1 - Epoch 31: Recon Loss = 45.1865, KL Loss = 0.3700\n",
      "Stage 1 - Epoch 41: Recon Loss = 36.1473, KL Loss = 0.5383\n",
      "Stage 1 - Epoch 51: Recon Loss = 29.7249, KL Loss = 0.7128\n",
      "Stage 1 - Epoch 61: Recon Loss = 25.8573, KL Loss = 0.8490\n",
      "Stage 1 - Epoch 71: Recon Loss = 22.6099, KL Loss = 0.9734\n",
      "Stage 1 - Epoch 81: Recon Loss = 20.2324, KL Loss = 0.9920\n",
      "Stage 1 - Epoch 91: Recon Loss = 17.1737, KL Loss = 0.9582\n",
      "Stage 1 - Epoch 101: Recon Loss = 15.0070, KL Loss = 0.9019\n",
      "Stage 1 - Epoch 111: Recon Loss = 13.4716, KL Loss = 0.8530\n",
      "Stage 1 - Epoch 121: Recon Loss = 11.7085, KL Loss = 0.8143\n",
      "Stage 1 - Epoch 131: Recon Loss = 10.4818, KL Loss = 0.7735\n",
      "Stage 1 - Epoch 141: Recon Loss = 9.4927, KL Loss = 0.7243\n",
      "Stage 1 - Epoch 151: Recon Loss = 8.5515, KL Loss = 0.6789\n",
      "Stage 1 - Epoch 161: Recon Loss = 7.8248, KL Loss = 0.6562\n",
      "Stage 1 - Epoch 171: Recon Loss = 7.2198, KL Loss = 0.6177\n",
      "Stage 1 - Epoch 181: Recon Loss = 6.5968, KL Loss = 0.5789\n",
      "Stage 1 - Epoch 191: Recon Loss = 6.0274, KL Loss = 0.5306\n",
      "Stage 1 - Epoch 201: Recon Loss = 5.7269, KL Loss = 0.5009\n",
      "Stage 1 - Epoch 211: Recon Loss = 5.1500, KL Loss = 0.4730\n",
      "Stage 1 - Epoch 221: Recon Loss = 4.8416, KL Loss = 0.4339\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.6133, KL Loss = 0.4169\n",
      "Stage 1 - Epoch 241: Recon Loss = 4.3689, KL Loss = 0.3860\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 53.6782\n",
      "h_value = 10.221947, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 32.1400\n",
      "h_value = 7.901540, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 20.3864\n",
      "h_value = 6.286212, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 13.0053\n",
      "h_value = 4.999340, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 8.6293\n",
      "h_value = 4.016889, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 5.9638\n",
      "h_value = 3.270764, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 4.2776\n",
      "h_value = 2.733166, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 3.1153\n",
      "h_value = 2.276317, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 2.3590\n",
      "h_value = 1.904615, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 1.8984\n",
      "h_value = 1.651857, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 1.5869\n",
      "h_value = 1.446355, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 1.3731\n",
      "h_value = 1.286365, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 1.2439\n",
      "h_value = 1.177500, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 1.1166\n",
      "h_value = 1.060472, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 1.0560\n",
      "h_value = 0.984407, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 1.0096\n",
      "h_value = 0.903793, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 0.9600\n",
      "h_value = 0.862763, Rho = 1.41, Alpha = 0.41\n",
      "Stage 2 - Epoch 171: Lagrangian Loss = 0.9186\n",
      "h_value = 0.796960, Rho = 1.46, Alpha = 0.46\n",
      "Stage 2 - Epoch 181: Lagrangian Loss = 0.9145\n",
      "h_value = 0.750589, Rho = 1.52, Alpha = 0.52\n",
      "Stage 2 - Epoch 191: Lagrangian Loss = 0.9039\n",
      "h_value = 0.714372, Rho = 1.58, Alpha = 0.58\n",
      "Stage 2 - Epoch 201: Lagrangian Loss = 0.8848\n",
      "h_value = 0.679375, Rho = 1.64, Alpha = 0.64\n",
      "Stage 2 - Epoch 211: Lagrangian Loss = 0.8868\n",
      "h_value = 0.646717, Rho = 1.71, Alpha = 0.71\n",
      "Early stopping triggered at epoch 215\n",
      "Final Stage 2 - Lagrangian Loss = 0.8870\n",
      "Final h_value = 0.627734\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 20:15:53,006 INFO -- Edge Accuracy = 37.50, Instantaneous Accuracy = 25.00, Lagged Accuracy = 37.50, Counterfactual Accuracy = 18.75,  Blended Accuracy = 18.75,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 430.5973, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 85.2934, KL Loss = 0.0942\n",
      "Stage 1 - Epoch 21: Recon Loss = 54.3731, KL Loss = 0.2529\n",
      "Stage 1 - Epoch 31: Recon Loss = 41.6554, KL Loss = 0.4280\n",
      "Stage 1 - Epoch 41: Recon Loss = 34.7232, KL Loss = 0.6068\n",
      "Stage 1 - Epoch 51: Recon Loss = 28.9708, KL Loss = 0.7983\n",
      "Stage 1 - Epoch 61: Recon Loss = 25.4183, KL Loss = 0.9541\n",
      "Stage 1 - Epoch 71: Recon Loss = 21.5969, KL Loss = 1.0860\n",
      "Stage 1 - Epoch 81: Recon Loss = 18.2966, KL Loss = 1.1139\n",
      "Stage 1 - Epoch 91: Recon Loss = 15.9520, KL Loss = 1.0468\n",
      "Stage 1 - Epoch 101: Recon Loss = 14.0723, KL Loss = 1.0041\n",
      "Stage 1 - Epoch 111: Recon Loss = 12.4615, KL Loss = 0.9138\n",
      "Stage 1 - Epoch 121: Recon Loss = 10.7758, KL Loss = 0.8578\n",
      "Stage 1 - Epoch 131: Recon Loss = 9.4523, KL Loss = 0.8130\n",
      "Stage 1 - Epoch 141: Recon Loss = 8.6354, KL Loss = 0.7701\n",
      "Stage 1 - Epoch 151: Recon Loss = 7.4904, KL Loss = 0.7249\n",
      "Stage 1 - Epoch 161: Recon Loss = 6.9456, KL Loss = 0.6758\n",
      "Stage 1 - Epoch 171: Recon Loss = 6.3520, KL Loss = 0.6421\n",
      "Stage 1 - Epoch 181: Recon Loss = 5.8935, KL Loss = 0.6042\n",
      "Stage 1 - Epoch 191: Recon Loss = 5.2176, KL Loss = 0.5768\n",
      "Stage 1 - Epoch 201: Recon Loss = 5.0959, KL Loss = 0.5302\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.8131, KL Loss = 0.5093\n",
      "Stage 1 - Epoch 221: Recon Loss = 4.4456, KL Loss = 0.4769\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.2403, KL Loss = 0.4512\n",
      "Stage 1 - Epoch 241: Recon Loss = 4.2231, KL Loss = 0.4143\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 30.2243\n",
      "h_value = 7.690834, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 18.8162\n",
      "h_value = 6.027362, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 11.7977\n",
      "h_value = 4.748905, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 7.2135\n",
      "h_value = 3.663132, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 4.7158\n",
      "h_value = 2.930149, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 3.3484\n",
      "h_value = 2.450410, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 2.4842\n",
      "h_value = 2.042512, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 1.9813\n",
      "h_value = 1.765591, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 1.6282\n",
      "h_value = 1.532576, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 1.4176\n",
      "h_value = 1.426453, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 1.2517\n",
      "h_value = 1.278949, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 1.1365\n",
      "h_value = 1.164348, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 1.0602\n",
      "h_value = 1.081763, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 1.0040\n",
      "h_value = 0.989208, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.9536\n",
      "h_value = 0.934802, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.9279\n",
      "h_value = 0.887348, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 0.9059\n",
      "h_value = 0.829909, Rho = 1.41, Alpha = 0.41\n",
      "Stage 2 - Epoch 171: Lagrangian Loss = 0.8917\n",
      "h_value = 0.769053, Rho = 1.46, Alpha = 0.46\n",
      "Stage 2 - Epoch 181: Lagrangian Loss = 0.8797\n",
      "h_value = 0.736925, Rho = 1.52, Alpha = 0.52\n",
      "Stage 2 - Epoch 191: Lagrangian Loss = 0.8784\n",
      "h_value = 0.700988, Rho = 1.58, Alpha = 0.58\n",
      "Early stopping triggered at epoch 196\n",
      "Final Stage 2 - Lagrangian Loss = 0.8909\n",
      "Final h_value = 0.694864\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 20:23:40,440 INFO -- Edge Accuracy = 35.29, Instantaneous Accuracy = 29.41, Lagged Accuracy = 35.29, Counterfactual Accuracy = 17.65,  Blended Accuracy = 17.65,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 343.5347, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 83.6632, KL Loss = 0.1073\n",
      "Stage 1 - Epoch 21: Recon Loss = 55.2426, KL Loss = 0.2586\n",
      "Stage 1 - Epoch 31: Recon Loss = 41.4658, KL Loss = 0.4241\n",
      "Stage 1 - Epoch 41: Recon Loss = 33.2306, KL Loss = 0.5930\n",
      "Stage 1 - Epoch 51: Recon Loss = 26.8713, KL Loss = 0.7621\n",
      "Stage 1 - Epoch 61: Recon Loss = 22.3276, KL Loss = 0.9108\n",
      "Stage 1 - Epoch 71: Recon Loss = 18.7474, KL Loss = 1.0166\n",
      "Stage 1 - Epoch 81: Recon Loss = 16.0799, KL Loss = 1.0185\n",
      "Stage 1 - Epoch 91: Recon Loss = 13.2510, KL Loss = 0.9299\n",
      "Stage 1 - Epoch 101: Recon Loss = 11.3435, KL Loss = 0.8730\n",
      "Stage 1 - Epoch 111: Recon Loss = 9.8341, KL Loss = 0.7874\n",
      "Stage 1 - Epoch 121: Recon Loss = 8.3694, KL Loss = 0.7155\n",
      "Stage 1 - Epoch 131: Recon Loss = 7.6635, KL Loss = 0.6490\n",
      "Stage 1 - Epoch 141: Recon Loss = 6.7626, KL Loss = 0.6058\n",
      "Stage 1 - Epoch 151: Recon Loss = 6.0264, KL Loss = 0.5577\n",
      "Stage 1 - Epoch 161: Recon Loss = 5.4683, KL Loss = 0.5108\n",
      "Stage 1 - Epoch 171: Recon Loss = 5.1063, KL Loss = 0.4859\n",
      "Stage 1 - Epoch 181: Recon Loss = 4.6268, KL Loss = 0.4556\n",
      "Stage 1 - Epoch 191: Recon Loss = 4.3682, KL Loss = 0.4149\n",
      "Stage 1 - Epoch 201: Recon Loss = 4.0682, KL Loss = 0.3853\n",
      "Stage 1 - Epoch 211: Recon Loss = 3.8628, KL Loss = 0.3536\n",
      "Stage 1 - Epoch 221: Recon Loss = 3.6843, KL Loss = 0.3395\n",
      "Stage 1 - Epoch 231: Recon Loss = 3.4711, KL Loss = 0.3062\n",
      "Stage 1 - Epoch 241: Recon Loss = 3.4528, KL Loss = 0.2766\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 15.0047\n",
      "h_value = 5.396811, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 9.8105\n",
      "h_value = 4.353376, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 6.4454\n",
      "h_value = 3.492519, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 4.2709\n",
      "h_value = 2.815170, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 2.6765\n",
      "h_value = 2.169564, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 1.7241\n",
      "h_value = 1.690660, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 1.2322\n",
      "h_value = 1.395544, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 0.9709\n",
      "h_value = 1.204414, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 0.8132\n",
      "h_value = 1.039982, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 0.7158\n",
      "h_value = 0.943469, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 0.6486\n",
      "h_value = 0.847198, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 0.6100\n",
      "h_value = 0.791949, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.5793\n",
      "h_value = 0.714709, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.5667\n",
      "h_value = 0.672703, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.5525\n",
      "h_value = 0.645597, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.5521\n",
      "h_value = 0.605124, Rho = 1.36, Alpha = 0.36\n",
      "Early stopping triggered at epoch 155\n",
      "Final Stage 2 - Lagrangian Loss = 0.5615\n",
      "Final h_value = 0.604620\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 20:30:42,625 INFO -- Edge Accuracy = 33.33, Instantaneous Accuracy = 27.78, Lagged Accuracy = 33.33, Counterfactual Accuracy = 16.67,  Blended Accuracy = 16.67,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 299.7363, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 76.4435, KL Loss = 0.0960\n",
      "Stage 1 - Epoch 21: Recon Loss = 51.6670, KL Loss = 0.2648\n",
      "Stage 1 - Epoch 31: Recon Loss = 38.6773, KL Loss = 0.4558\n",
      "Stage 1 - Epoch 41: Recon Loss = 31.1783, KL Loss = 0.6328\n",
      "Stage 1 - Epoch 51: Recon Loss = 25.4726, KL Loss = 0.7901\n",
      "Stage 1 - Epoch 61: Recon Loss = 22.1236, KL Loss = 0.9137\n",
      "Stage 1 - Epoch 71: Recon Loss = 18.9822, KL Loss = 0.9790\n",
      "Stage 1 - Epoch 81: Recon Loss = 15.7533, KL Loss = 0.9590\n",
      "Stage 1 - Epoch 91: Recon Loss = 14.2379, KL Loss = 0.8621\n",
      "Stage 1 - Epoch 101: Recon Loss = 12.2637, KL Loss = 0.7972\n",
      "Stage 1 - Epoch 111: Recon Loss = 10.7056, KL Loss = 0.7433\n",
      "Stage 1 - Epoch 121: Recon Loss = 8.9922, KL Loss = 0.6952\n",
      "Stage 1 - Epoch 131: Recon Loss = 8.0614, KL Loss = 0.6525\n",
      "Stage 1 - Epoch 141: Recon Loss = 7.2066, KL Loss = 0.5978\n",
      "Stage 1 - Epoch 151: Recon Loss = 6.5691, KL Loss = 0.5524\n",
      "Stage 1 - Epoch 161: Recon Loss = 5.7915, KL Loss = 0.5087\n",
      "Stage 1 - Epoch 171: Recon Loss = 5.3533, KL Loss = 0.4658\n",
      "Stage 1 - Epoch 181: Recon Loss = 5.1547, KL Loss = 0.4261\n",
      "Stage 1 - Epoch 191: Recon Loss = 4.6902, KL Loss = 0.3925\n",
      "Stage 1 - Epoch 201: Recon Loss = 4.4164, KL Loss = 0.3552\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.1270, KL Loss = 0.3132\n",
      "Stage 1 - Epoch 221: Recon Loss = 3.9264, KL Loss = 0.2890\n",
      "Stage 1 - Epoch 231: Recon Loss = 3.7301, KL Loss = 0.2663\n",
      "Stage 1 - Epoch 241: Recon Loss = 3.6319, KL Loss = 0.2406\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 10.5764\n",
      "h_value = 4.505198, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 5.4918\n",
      "h_value = 3.240971, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 3.1202\n",
      "h_value = 2.416633, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 1.9144\n",
      "h_value = 1.875832, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 1.2680\n",
      "h_value = 1.493190, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 0.9278\n",
      "h_value = 1.204865, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 0.7534\n",
      "h_value = 1.069093, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 0.6380\n",
      "h_value = 0.922218, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 0.5700\n",
      "h_value = 0.852405, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 0.5242\n",
      "h_value = 0.792062, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 0.4978\n",
      "h_value = 0.697554, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 0.4816\n",
      "h_value = 0.658101, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.4715\n",
      "h_value = 0.635638, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.4706\n",
      "h_value = 0.589164, Rho = 1.27, Alpha = 0.27\n",
      "Early stopping triggered at epoch 139\n",
      "Final Stage 2 - Lagrangian Loss = 0.4764\n",
      "Final h_value = 0.577920\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 20:37:25,479 INFO -- Edge Accuracy = 31.58, Instantaneous Accuracy = 26.32, Lagged Accuracy = 31.58, Counterfactual Accuracy = 15.79,  Blended Accuracy = 15.79,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 482.2498, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 87.6261, KL Loss = 0.0719\n",
      "Stage 1 - Epoch 21: Recon Loss = 54.9400, KL Loss = 0.2057\n",
      "Stage 1 - Epoch 31: Recon Loss = 43.1431, KL Loss = 0.3651\n",
      "Stage 1 - Epoch 41: Recon Loss = 34.4451, KL Loss = 0.5255\n",
      "Stage 1 - Epoch 51: Recon Loss = 29.0674, KL Loss = 0.6911\n",
      "Stage 1 - Epoch 61: Recon Loss = 24.2838, KL Loss = 0.8505\n",
      "Stage 1 - Epoch 71: Recon Loss = 20.9373, KL Loss = 1.0115\n",
      "Stage 1 - Epoch 81: Recon Loss = 18.0916, KL Loss = 1.0518\n",
      "Stage 1 - Epoch 91: Recon Loss = 15.9983, KL Loss = 1.0271\n",
      "Stage 1 - Epoch 101: Recon Loss = 13.6834, KL Loss = 0.9820\n",
      "Stage 1 - Epoch 111: Recon Loss = 12.1115, KL Loss = 0.9428\n",
      "Stage 1 - Epoch 121: Recon Loss = 10.7626, KL Loss = 0.8901\n",
      "Stage 1 - Epoch 131: Recon Loss = 9.6308, KL Loss = 0.8244\n",
      "Stage 1 - Epoch 141: Recon Loss = 8.8297, KL Loss = 0.7727\n",
      "Stage 1 - Epoch 151: Recon Loss = 7.8727, KL Loss = 0.7205\n",
      "Stage 1 - Epoch 161: Recon Loss = 7.1636, KL Loss = 0.6767\n",
      "Stage 1 - Epoch 171: Recon Loss = 6.4454, KL Loss = 0.6333\n",
      "Stage 1 - Epoch 181: Recon Loss = 5.9694, KL Loss = 0.5983\n",
      "Stage 1 - Epoch 191: Recon Loss = 5.7146, KL Loss = 0.5680\n",
      "Stage 1 - Epoch 201: Recon Loss = 5.4432, KL Loss = 0.5222\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.9867, KL Loss = 0.4848\n",
      "Stage 1 - Epoch 221: Recon Loss = 4.5950, KL Loss = 0.4660\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.3989, KL Loss = 0.4313\n",
      "Stage 1 - Epoch 241: Recon Loss = 4.2261, KL Loss = 0.3946\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 34.5474\n",
      "h_value = 8.204988, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 23.8575\n",
      "h_value = 6.806380, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 15.2157\n",
      "h_value = 5.389592, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 9.3204\n",
      "h_value = 4.204381, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 5.9054\n",
      "h_value = 3.233362, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 3.8776\n",
      "h_value = 2.639557, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 2.8445\n",
      "h_value = 2.226566, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 2.2861\n",
      "h_value = 1.898778, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 1.8510\n",
      "h_value = 1.721732, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 1.5009\n",
      "h_value = 1.484491, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 1.3545\n",
      "h_value = 1.353357, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 1.2263\n",
      "h_value = 1.234268, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 1.1376\n",
      "h_value = 1.137363, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 1.0442\n",
      "h_value = 1.063730, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.9992\n",
      "h_value = 0.967145, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.9719\n",
      "h_value = 0.880623, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 0.9573\n",
      "h_value = 0.841551, Rho = 1.41, Alpha = 0.41\n",
      "Stage 2 - Epoch 171: Lagrangian Loss = 0.9379\n",
      "h_value = 0.832609, Rho = 1.46, Alpha = 0.46\n",
      "Early stopping triggered at epoch 177\n",
      "Final Stage 2 - Lagrangian Loss = 0.9368\n",
      "Final h_value = 0.786330\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 20:44:52,958 INFO -- Edge Accuracy = 30.00, Instantaneous Accuracy = 25.00, Lagged Accuracy = 30.00, Counterfactual Accuracy = 15.00,  Blended Accuracy = 15.00,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 419.8927, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 80.8449, KL Loss = 0.0578\n",
      "Stage 1 - Epoch 21: Recon Loss = 53.6111, KL Loss = 0.1798\n",
      "Stage 1 - Epoch 31: Recon Loss = 40.0979, KL Loss = 0.3258\n",
      "Stage 1 - Epoch 41: Recon Loss = 31.5250, KL Loss = 0.4829\n",
      "Stage 1 - Epoch 51: Recon Loss = 25.6975, KL Loss = 0.6493\n",
      "Stage 1 - Epoch 61: Recon Loss = 22.2069, KL Loss = 0.7754\n",
      "Stage 1 - Epoch 71: Recon Loss = 18.2168, KL Loss = 0.9068\n",
      "Stage 1 - Epoch 81: Recon Loss = 15.7239, KL Loss = 0.9262\n",
      "Stage 1 - Epoch 91: Recon Loss = 13.6099, KL Loss = 0.8738\n",
      "Stage 1 - Epoch 101: Recon Loss = 11.6896, KL Loss = 0.8509\n",
      "Stage 1 - Epoch 111: Recon Loss = 10.3501, KL Loss = 0.8005\n",
      "Stage 1 - Epoch 121: Recon Loss = 9.1924, KL Loss = 0.7535\n",
      "Stage 1 - Epoch 131: Recon Loss = 8.2578, KL Loss = 0.7242\n",
      "Stage 1 - Epoch 141: Recon Loss = 7.6614, KL Loss = 0.6781\n",
      "Stage 1 - Epoch 151: Recon Loss = 6.9786, KL Loss = 0.6417\n",
      "Stage 1 - Epoch 161: Recon Loss = 6.2774, KL Loss = 0.6003\n",
      "Stage 1 - Epoch 171: Recon Loss = 5.7117, KL Loss = 0.5718\n",
      "Stage 1 - Epoch 181: Recon Loss = 5.4748, KL Loss = 0.5519\n",
      "Stage 1 - Epoch 191: Recon Loss = 5.1092, KL Loss = 0.5250\n",
      "Stage 1 - Epoch 201: Recon Loss = 4.6531, KL Loss = 0.4886\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.4550, KL Loss = 0.4622\n",
      "Stage 1 - Epoch 221: Recon Loss = 4.3034, KL Loss = 0.4336\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.0396, KL Loss = 0.4122\n",
      "Stage 1 - Epoch 241: Recon Loss = 3.7743, KL Loss = 0.3883\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 19.8173\n",
      "h_value = 6.210047, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 13.3565\n",
      "h_value = 5.067942, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 8.8793\n",
      "h_value = 4.112062, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 5.7994\n",
      "h_value = 3.269329, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 3.8758\n",
      "h_value = 2.656461, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 2.7768\n",
      "h_value = 2.181863, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 2.1042\n",
      "h_value = 1.873127, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 1.6680\n",
      "h_value = 1.611061, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 1.3655\n",
      "h_value = 1.408456, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 1.1920\n",
      "h_value = 1.286157, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 1.0735\n",
      "h_value = 1.151834, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 0.9663\n",
      "h_value = 1.048173, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.8933\n",
      "h_value = 0.992037, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.8571\n",
      "h_value = 0.890532, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.8219\n",
      "h_value = 0.838392, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.7943\n",
      "h_value = 0.791534, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 0.7827\n",
      "h_value = 0.741383, Rho = 1.41, Alpha = 0.41\n",
      "Stage 2 - Epoch 171: Lagrangian Loss = 0.7810\n",
      "h_value = 0.710546, Rho = 1.46, Alpha = 0.46\n",
      "Early stopping triggered at epoch 174\n",
      "Final Stage 2 - Lagrangian Loss = 0.7867\n",
      "Final h_value = 0.695559\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 20:52:14,877 INFO -- Edge Accuracy = 33.33, Instantaneous Accuracy = 23.81, Lagged Accuracy = 33.33, Counterfactual Accuracy = 14.29,  Blended Accuracy = 14.29,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 420.0742, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 89.9151, KL Loss = 0.0609\n",
      "Stage 1 - Epoch 21: Recon Loss = 54.9613, KL Loss = 0.1823\n",
      "Stage 1 - Epoch 31: Recon Loss = 43.2471, KL Loss = 0.3200\n",
      "Stage 1 - Epoch 41: Recon Loss = 33.6641, KL Loss = 0.4732\n",
      "Stage 1 - Epoch 51: Recon Loss = 28.6951, KL Loss = 0.6217\n",
      "Stage 1 - Epoch 61: Recon Loss = 23.6511, KL Loss = 0.7480\n",
      "Stage 1 - Epoch 71: Recon Loss = 20.0394, KL Loss = 0.8712\n",
      "Stage 1 - Epoch 81: Recon Loss = 17.4055, KL Loss = 0.8774\n",
      "Stage 1 - Epoch 91: Recon Loss = 15.0709, KL Loss = 0.8385\n",
      "Stage 1 - Epoch 101: Recon Loss = 12.8754, KL Loss = 0.7763\n",
      "Stage 1 - Epoch 111: Recon Loss = 11.7413, KL Loss = 0.7377\n",
      "Stage 1 - Epoch 121: Recon Loss = 10.5302, KL Loss = 0.7047\n",
      "Stage 1 - Epoch 131: Recon Loss = 9.3433, KL Loss = 0.6507\n",
      "Stage 1 - Epoch 141: Recon Loss = 8.5007, KL Loss = 0.6208\n",
      "Stage 1 - Epoch 151: Recon Loss = 7.7327, KL Loss = 0.5711\n",
      "Stage 1 - Epoch 161: Recon Loss = 7.2225, KL Loss = 0.5351\n",
      "Stage 1 - Epoch 171: Recon Loss = 6.5004, KL Loss = 0.5079\n",
      "Stage 1 - Epoch 181: Recon Loss = 6.1656, KL Loss = 0.4638\n",
      "Stage 1 - Epoch 191: Recon Loss = 5.6722, KL Loss = 0.4322\n",
      "Stage 1 - Epoch 201: Recon Loss = 5.4446, KL Loss = 0.3884\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.9936, KL Loss = 0.3596\n",
      "Stage 1 - Epoch 221: Recon Loss = 4.6887, KL Loss = 0.3359\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.3020, KL Loss = 0.3109\n",
      "Stage 1 - Epoch 241: Recon Loss = 4.1231, KL Loss = 0.2797\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 21.5511\n",
      "h_value = 6.474302, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 13.4857\n",
      "h_value = 5.105508, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 7.7932\n",
      "h_value = 3.837478, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 4.5551\n",
      "h_value = 2.895562, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 2.8926\n",
      "h_value = 2.305309, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 1.9468\n",
      "h_value = 1.820349, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 1.4492\n",
      "h_value = 1.555385, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 1.1451\n",
      "h_value = 1.282239, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 0.9592\n",
      "h_value = 1.155616, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 0.8476\n",
      "h_value = 1.031260, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 0.7635\n",
      "h_value = 0.932402, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 0.7116\n",
      "h_value = 0.870022, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.6838\n",
      "h_value = 0.800686, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.6463\n",
      "h_value = 0.766774, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.6338\n",
      "h_value = 0.722885, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.6199\n",
      "h_value = 0.659666, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 0.6317\n",
      "h_value = 0.646524, Rho = 1.41, Alpha = 0.41\n",
      "Early stopping triggered at epoch 161\n",
      "Final Stage 2 - Lagrangian Loss = 0.6317\n",
      "Final h_value = 0.646524\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 20:59:23,828 INFO -- Edge Accuracy = 31.82, Instantaneous Accuracy = 22.73, Lagged Accuracy = 31.82, Counterfactual Accuracy = 18.18,  Blended Accuracy = 18.18,  RR Accuracy = 4.55  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 439.8472, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 89.6095, KL Loss = 0.0914\n",
      "Stage 1 - Epoch 21: Recon Loss = 57.3274, KL Loss = 0.2279\n",
      "Stage 1 - Epoch 31: Recon Loss = 42.5854, KL Loss = 0.3816\n",
      "Stage 1 - Epoch 41: Recon Loss = 34.0264, KL Loss = 0.5342\n",
      "Stage 1 - Epoch 51: Recon Loss = 28.4955, KL Loss = 0.6876\n",
      "Stage 1 - Epoch 61: Recon Loss = 23.3938, KL Loss = 0.8537\n",
      "Stage 1 - Epoch 71: Recon Loss = 19.6170, KL Loss = 0.9709\n",
      "Stage 1 - Epoch 81: Recon Loss = 17.0452, KL Loss = 0.9894\n",
      "Stage 1 - Epoch 91: Recon Loss = 13.9829, KL Loss = 0.9479\n",
      "Stage 1 - Epoch 101: Recon Loss = 12.5268, KL Loss = 0.9054\n",
      "Stage 1 - Epoch 111: Recon Loss = 10.5698, KL Loss = 0.8408\n",
      "Stage 1 - Epoch 121: Recon Loss = 9.6110, KL Loss = 0.7785\n",
      "Stage 1 - Epoch 131: Recon Loss = 8.2167, KL Loss = 0.7249\n",
      "Stage 1 - Epoch 141: Recon Loss = 7.4307, KL Loss = 0.6612\n",
      "Stage 1 - Epoch 151: Recon Loss = 6.6257, KL Loss = 0.6094\n",
      "Stage 1 - Epoch 161: Recon Loss = 6.1123, KL Loss = 0.5683\n",
      "Stage 1 - Epoch 171: Recon Loss = 5.7409, KL Loss = 0.5211\n",
      "Stage 1 - Epoch 181: Recon Loss = 5.1695, KL Loss = 0.4806\n",
      "Stage 1 - Epoch 191: Recon Loss = 4.8917, KL Loss = 0.4361\n",
      "Stage 1 - Epoch 201: Recon Loss = 4.6028, KL Loss = 0.4021\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.3385, KL Loss = 0.3891\n",
      "Stage 1 - Epoch 221: Recon Loss = 4.0584, KL Loss = 0.3454\n",
      "Stage 1 - Epoch 231: Recon Loss = 3.7853, KL Loss = 0.3335\n",
      "Stage 1 - Epoch 241: Recon Loss = 3.6862, KL Loss = 0.2953\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 21.6057\n",
      "h_value = 6.476854, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 14.2911\n",
      "h_value = 5.263479, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 9.6938\n",
      "h_value = 4.333337, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 6.3737\n",
      "h_value = 3.462601, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 4.1349\n",
      "h_value = 2.691983, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 2.8556\n",
      "h_value = 2.252999, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 2.0953\n",
      "h_value = 1.815216, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 1.6203\n",
      "h_value = 1.583190, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 1.3510\n",
      "h_value = 1.391655, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 1.1604\n",
      "h_value = 1.215396, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 1.0032\n",
      "h_value = 1.091369, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 0.9203\n",
      "h_value = 0.998415, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.8466\n",
      "h_value = 0.923119, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.7985\n",
      "h_value = 0.872597, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.7394\n",
      "h_value = 0.789643, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.7536\n",
      "h_value = 0.711699, Rho = 1.36, Alpha = 0.36\n",
      "Early stopping triggered at epoch 151\n",
      "Final Stage 2 - Lagrangian Loss = 0.7536\n",
      "Final h_value = 0.711699\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 21:06:23,002 INFO -- Edge Accuracy = 30.43, Instantaneous Accuracy = 21.74, Lagged Accuracy = 30.43, Counterfactual Accuracy = 17.39,  Blended Accuracy = 17.39,  RR Accuracy = 4.35  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 833.3541, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 99.1625, KL Loss = 0.0840\n",
      "Stage 1 - Epoch 21: Recon Loss = 64.5565, KL Loss = 0.2206\n",
      "Stage 1 - Epoch 31: Recon Loss = 45.9328, KL Loss = 0.3747\n",
      "Stage 1 - Epoch 41: Recon Loss = 38.1206, KL Loss = 0.5370\n",
      "Stage 1 - Epoch 51: Recon Loss = 33.7370, KL Loss = 0.7066\n",
      "Stage 1 - Epoch 61: Recon Loss = 28.3181, KL Loss = 0.8724\n",
      "Stage 1 - Epoch 71: Recon Loss = 24.7004, KL Loss = 1.0257\n",
      "Stage 1 - Epoch 81: Recon Loss = 21.1082, KL Loss = 1.0843\n",
      "Stage 1 - Epoch 91: Recon Loss = 18.5111, KL Loss = 1.0668\n",
      "Stage 1 - Epoch 101: Recon Loss = 16.1767, KL Loss = 1.0374\n",
      "Stage 1 - Epoch 111: Recon Loss = 14.0658, KL Loss = 0.9809\n",
      "Stage 1 - Epoch 121: Recon Loss = 12.4584, KL Loss = 0.9505\n",
      "Stage 1 - Epoch 131: Recon Loss = 11.6154, KL Loss = 0.8915\n",
      "Stage 1 - Epoch 141: Recon Loss = 10.2786, KL Loss = 0.8507\n",
      "Stage 1 - Epoch 151: Recon Loss = 9.2643, KL Loss = 0.7880\n",
      "Stage 1 - Epoch 161: Recon Loss = 8.7024, KL Loss = 0.7558\n",
      "Stage 1 - Epoch 171: Recon Loss = 7.7892, KL Loss = 0.7044\n",
      "Stage 1 - Epoch 181: Recon Loss = 7.2382, KL Loss = 0.6899\n",
      "Stage 1 - Epoch 191: Recon Loss = 6.8131, KL Loss = 0.6494\n",
      "Stage 1 - Epoch 201: Recon Loss = 6.3782, KL Loss = 0.6342\n",
      "Stage 1 - Epoch 211: Recon Loss = 5.7540, KL Loss = 0.5937\n",
      "Stage 1 - Epoch 221: Recon Loss = 5.5932, KL Loss = 0.5502\n",
      "Stage 1 - Epoch 231: Recon Loss = 5.2385, KL Loss = 0.5288\n",
      "Stage 1 - Epoch 241: Recon Loss = 4.8479, KL Loss = 0.4962\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 24.7755\n",
      "h_value = 6.906650, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 16.1951\n",
      "h_value = 5.575102, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 10.8631\n",
      "h_value = 4.568003, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 7.2341\n",
      "h_value = 3.690263, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 4.9537\n",
      "h_value = 2.986722, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 3.5531\n",
      "h_value = 2.486569, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 2.4325\n",
      "h_value = 2.022825, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 1.7314\n",
      "h_value = 1.645740, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 1.3491\n",
      "h_value = 1.390782, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 1.0945\n",
      "h_value = 1.208811, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 0.9537\n",
      "h_value = 1.065161, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 0.8572\n",
      "h_value = 0.994306, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.8042\n",
      "h_value = 0.898992, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.7399\n",
      "h_value = 0.824120, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.7201\n",
      "h_value = 0.774513, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.6992\n",
      "h_value = 0.727291, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 0.6932\n",
      "h_value = 0.695453, Rho = 1.41, Alpha = 0.41\n",
      "Stage 2 - Epoch 171: Lagrangian Loss = 0.7062\n",
      "h_value = 0.656405, Rho = 1.46, Alpha = 0.46\n",
      "Early stopping triggered at epoch 172\n",
      "Final Stage 2 - Lagrangian Loss = 0.6961\n",
      "Final h_value = 0.628372\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 21:13:46,541 INFO -- Edge Accuracy = 29.17, Instantaneous Accuracy = 20.83, Lagged Accuracy = 29.17, Counterfactual Accuracy = 16.67,  Blended Accuracy = 16.67,  RR Accuracy = 4.17  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 322.9245, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 76.4519, KL Loss = 0.1032\n",
      "Stage 1 - Epoch 21: Recon Loss = 50.9188, KL Loss = 0.2525\n",
      "Stage 1 - Epoch 31: Recon Loss = 38.9388, KL Loss = 0.4234\n",
      "Stage 1 - Epoch 41: Recon Loss = 32.7062, KL Loss = 0.5820\n",
      "Stage 1 - Epoch 51: Recon Loss = 28.2194, KL Loss = 0.7556\n",
      "Stage 1 - Epoch 61: Recon Loss = 22.4042, KL Loss = 0.8830\n",
      "Stage 1 - Epoch 71: Recon Loss = 19.1886, KL Loss = 0.9857\n",
      "Stage 1 - Epoch 81: Recon Loss = 16.1250, KL Loss = 0.9800\n",
      "Stage 1 - Epoch 91: Recon Loss = 13.6501, KL Loss = 0.9142\n",
      "Stage 1 - Epoch 101: Recon Loss = 11.5124, KL Loss = 0.8510\n",
      "Stage 1 - Epoch 111: Recon Loss = 10.3318, KL Loss = 0.7870\n",
      "Stage 1 - Epoch 121: Recon Loss = 8.9450, KL Loss = 0.7348\n",
      "Stage 1 - Epoch 131: Recon Loss = 7.9162, KL Loss = 0.6767\n",
      "Stage 1 - Epoch 141: Recon Loss = 6.9554, KL Loss = 0.6306\n",
      "Stage 1 - Epoch 151: Recon Loss = 6.3900, KL Loss = 0.5901\n",
      "Stage 1 - Epoch 161: Recon Loss = 6.1045, KL Loss = 0.5582\n",
      "Stage 1 - Epoch 171: Recon Loss = 5.5474, KL Loss = 0.5211\n",
      "Stage 1 - Epoch 181: Recon Loss = 4.9484, KL Loss = 0.4830\n",
      "Stage 1 - Epoch 191: Recon Loss = 4.7162, KL Loss = 0.4455\n",
      "Stage 1 - Epoch 201: Recon Loss = 4.4673, KL Loss = 0.4128\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.2004, KL Loss = 0.4084\n",
      "Stage 1 - Epoch 221: Recon Loss = 3.9892, KL Loss = 0.3686\n",
      "Stage 1 - Epoch 231: Recon Loss = 3.7928, KL Loss = 0.3423\n",
      "Stage 1 - Epoch 241: Recon Loss = 3.6376, KL Loss = 0.3096\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 16.0937\n",
      "h_value = 5.591885, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 8.9238\n",
      "h_value = 4.098302, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 4.9447\n",
      "h_value = 3.076861, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 3.0240\n",
      "h_value = 2.354967, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 2.0073\n",
      "h_value = 1.888189, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 1.4886\n",
      "h_value = 1.567505, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 1.1716\n",
      "h_value = 1.368805, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 0.9199\n",
      "h_value = 1.142146, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 0.7665\n",
      "h_value = 1.014678, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 0.6424\n",
      "h_value = 0.876221, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 0.5694\n",
      "h_value = 0.797922, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 0.5397\n",
      "h_value = 0.708523, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.5037\n",
      "h_value = 0.663775, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.4901\n",
      "h_value = 0.604173, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.4855\n",
      "h_value = 0.586682, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.4890\n",
      "h_value = 0.556307, Rho = 1.36, Alpha = 0.36\n",
      "Early stopping triggered at epoch 159\n",
      "Final Stage 2 - Lagrangian Loss = 0.5020\n",
      "Final h_value = 0.534617\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 21:20:52,982 INFO -- Edge Accuracy = 28.00, Instantaneous Accuracy = 20.00, Lagged Accuracy = 28.00, Counterfactual Accuracy = 16.00,  Blended Accuracy = 16.00,  RR Accuracy = 4.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 353.6123, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 81.4813, KL Loss = 0.1095\n",
      "Stage 1 - Epoch 21: Recon Loss = 53.0649, KL Loss = 0.2682\n",
      "Stage 1 - Epoch 31: Recon Loss = 38.4738, KL Loss = 0.4532\n",
      "Stage 1 - Epoch 41: Recon Loss = 31.1738, KL Loss = 0.6453\n",
      "Stage 1 - Epoch 51: Recon Loss = 24.9887, KL Loss = 0.8534\n",
      "Stage 1 - Epoch 61: Recon Loss = 20.5455, KL Loss = 1.0143\n",
      "Stage 1 - Epoch 71: Recon Loss = 16.9232, KL Loss = 1.1669\n",
      "Stage 1 - Epoch 81: Recon Loss = 14.6147, KL Loss = 1.1825\n",
      "Stage 1 - Epoch 91: Recon Loss = 12.6032, KL Loss = 1.0944\n",
      "Stage 1 - Epoch 101: Recon Loss = 10.8343, KL Loss = 1.0103\n",
      "Stage 1 - Epoch 111: Recon Loss = 9.4239, KL Loss = 0.9125\n",
      "Stage 1 - Epoch 121: Recon Loss = 8.6561, KL Loss = 0.8292\n",
      "Stage 1 - Epoch 131: Recon Loss = 7.1647, KL Loss = 0.7619\n",
      "Stage 1 - Epoch 141: Recon Loss = 6.5367, KL Loss = 0.6919\n",
      "Stage 1 - Epoch 151: Recon Loss = 5.9009, KL Loss = 0.6607\n",
      "Stage 1 - Epoch 161: Recon Loss = 5.5691, KL Loss = 0.6095\n",
      "Stage 1 - Epoch 171: Recon Loss = 5.2314, KL Loss = 0.5808\n",
      "Stage 1 - Epoch 181: Recon Loss = 4.8312, KL Loss = 0.5264\n",
      "Stage 1 - Epoch 191: Recon Loss = 4.5901, KL Loss = 0.5037\n",
      "Stage 1 - Epoch 201: Recon Loss = 4.5186, KL Loss = 0.4846\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.3865, KL Loss = 0.4837\n",
      "Stage 1 - Epoch 221: Recon Loss = 4.3571, KL Loss = 0.4712\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.3238, KL Loss = 0.4646\n",
      "Early stopping triggered at epoch 237\n",
      "Final Stage 1 - Recon Loss = 4.3702, KL Loss = 0.4665\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 20.1951\n",
      "h_value = 6.246786, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 12.1865\n",
      "h_value = 4.838569, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 7.8680\n",
      "h_value = 3.866907, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 5.2507\n",
      "h_value = 3.130742, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 3.6775\n",
      "h_value = 2.591042, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 2.7100\n",
      "h_value = 2.166815, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 2.1169\n",
      "h_value = 1.871692, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 1.7108\n",
      "h_value = 1.637625, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 1.4449\n",
      "h_value = 1.462109, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 1.2600\n",
      "h_value = 1.311596, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 1.1256\n",
      "h_value = 1.183708, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 1.0284\n",
      "h_value = 1.095277, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.9669\n",
      "h_value = 1.015217, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.9100\n",
      "h_value = 0.938593, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.8701\n",
      "h_value = 0.881959, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.8414\n",
      "h_value = 0.824470, Rho = 1.36, Alpha = 0.36\n",
      "Stage 2 - Epoch 161: Lagrangian Loss = 0.8269\n",
      "h_value = 0.780325, Rho = 1.41, Alpha = 0.41\n",
      "Stage 2 - Epoch 171: Lagrangian Loss = 0.8176\n",
      "h_value = 0.731406, Rho = 1.46, Alpha = 0.46\n",
      "Stage 2 - Epoch 181: Lagrangian Loss = 0.8152\n",
      "h_value = 0.693219, Rho = 1.52, Alpha = 0.52\n",
      "Stage 2 - Epoch 191: Lagrangian Loss = 0.8165\n",
      "h_value = 0.669814, Rho = 1.58, Alpha = 0.58\n",
      "Early stopping triggered at epoch 192\n",
      "Final Stage 2 - Lagrangian Loss = 0.8134\n",
      "Final h_value = 0.666815\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 21:28:23,888 INFO -- Edge Accuracy = 26.92, Instantaneous Accuracy = 23.08, Lagged Accuracy = 26.92, Counterfactual Accuracy = 15.38,  Blended Accuracy = 15.38,  RR Accuracy = 3.85  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 334.7579, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 83.4846, KL Loss = 0.0936\n",
      "Stage 1 - Epoch 21: Recon Loss = 55.8749, KL Loss = 0.2399\n",
      "Stage 1 - Epoch 31: Recon Loss = 43.4393, KL Loss = 0.4013\n",
      "Stage 1 - Epoch 41: Recon Loss = 35.3063, KL Loss = 0.5696\n",
      "Stage 1 - Epoch 51: Recon Loss = 28.9692, KL Loss = 0.7402\n",
      "Stage 1 - Epoch 61: Recon Loss = 24.8153, KL Loss = 0.8738\n",
      "Stage 1 - Epoch 71: Recon Loss = 20.9426, KL Loss = 1.0138\n",
      "Stage 1 - Epoch 81: Recon Loss = 17.9324, KL Loss = 1.0076\n",
      "Stage 1 - Epoch 91: Recon Loss = 15.4501, KL Loss = 0.9428\n",
      "Stage 1 - Epoch 101: Recon Loss = 12.9506, KL Loss = 0.8771\n",
      "Stage 1 - Epoch 111: Recon Loss = 11.4387, KL Loss = 0.8181\n",
      "Stage 1 - Epoch 121: Recon Loss = 10.0432, KL Loss = 0.7665\n",
      "Stage 1 - Epoch 131: Recon Loss = 8.9455, KL Loss = 0.7063\n",
      "Stage 1 - Epoch 141: Recon Loss = 7.9352, KL Loss = 0.6497\n",
      "Stage 1 - Epoch 151: Recon Loss = 7.1285, KL Loss = 0.6088\n",
      "Stage 1 - Epoch 161: Recon Loss = 6.5950, KL Loss = 0.5664\n",
      "Stage 1 - Epoch 171: Recon Loss = 6.1689, KL Loss = 0.5251\n",
      "Stage 1 - Epoch 181: Recon Loss = 5.5621, KL Loss = 0.4934\n",
      "Stage 1 - Epoch 191: Recon Loss = 5.0967, KL Loss = 0.4574\n",
      "Stage 1 - Epoch 201: Recon Loss = 4.8757, KL Loss = 0.4224\n",
      "Stage 1 - Epoch 211: Recon Loss = 4.6520, KL Loss = 0.3807\n",
      "Stage 1 - Epoch 221: Recon Loss = 4.4070, KL Loss = 0.3614\n",
      "Stage 1 - Epoch 231: Recon Loss = 4.1270, KL Loss = 0.3402\n",
      "Stage 1 - Epoch 241: Recon Loss = 3.9220, KL Loss = 0.3133\n",
      "\n",
      "=== Starting Stage 2: Training Causal Graph Discovery ===\n",
      "Stage 2 - Epoch 1: Lagrangian Loss = 9.2323\n",
      "h_value = 4.221696, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 11: Lagrangian Loss = 5.3745\n",
      "h_value = 3.175784, Rho = 1.00, Alpha = 0.00\n",
      "Stage 2 - Epoch 21: Lagrangian Loss = 3.2710\n",
      "h_value = 2.488149, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 31: Lagrangian Loss = 2.1506\n",
      "h_value = 1.977578, Rho = 1.01, Alpha = 0.01\n",
      "Stage 2 - Epoch 41: Lagrangian Loss = 1.5241\n",
      "h_value = 1.624996, Rho = 1.03, Alpha = 0.03\n",
      "Stage 2 - Epoch 51: Lagrangian Loss = 1.1648\n",
      "h_value = 1.391210, Rho = 1.04, Alpha = 0.04\n",
      "Stage 2 - Epoch 61: Lagrangian Loss = 0.9364\n",
      "h_value = 1.185899, Rho = 1.06, Alpha = 0.06\n",
      "Stage 2 - Epoch 71: Lagrangian Loss = 0.8133\n",
      "h_value = 1.082390, Rho = 1.08, Alpha = 0.08\n",
      "Stage 2 - Epoch 81: Lagrangian Loss = 0.7085\n",
      "h_value = 0.957964, Rho = 1.10, Alpha = 0.10\n",
      "Stage 2 - Epoch 91: Lagrangian Loss = 0.6081\n",
      "h_value = 0.860017, Rho = 1.13, Alpha = 0.13\n",
      "Stage 2 - Epoch 101: Lagrangian Loss = 0.5285\n",
      "h_value = 0.749027, Rho = 1.16, Alpha = 0.16\n",
      "Stage 2 - Epoch 111: Lagrangian Loss = 0.4902\n",
      "h_value = 0.670005, Rho = 1.19, Alpha = 0.19\n",
      "Stage 2 - Epoch 121: Lagrangian Loss = 0.4660\n",
      "h_value = 0.631780, Rho = 1.23, Alpha = 0.23\n",
      "Stage 2 - Epoch 131: Lagrangian Loss = 0.4527\n",
      "h_value = 0.587496, Rho = 1.27, Alpha = 0.27\n",
      "Stage 2 - Epoch 141: Lagrangian Loss = 0.4478\n",
      "h_value = 0.556338, Rho = 1.31, Alpha = 0.31\n",
      "Stage 2 - Epoch 151: Lagrangian Loss = 0.4512\n",
      "h_value = 0.532067, Rho = 1.36, Alpha = 0.36\n",
      "Early stopping triggered at epoch 154\n",
      "Final Stage 2 - Lagrangian Loss = 0.4531\n",
      "Final h_value = 0.517249\n",
      "Two-stage training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 21:35:26,706 INFO -- Edge Accuracy = 25.93, Instantaneous Accuracy = 22.22, Lagged Accuracy = 25.93, Counterfactual Accuracy = 14.81,  Blended Accuracy = 14.81,  RR Accuracy = 3.70  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Stage 1: Training Reconstruction Network ===\n",
      "Stage 1 - Epoch 1: Recon Loss = 457.2084, KL Loss = 0.0000\n",
      "Stage 1 - Epoch 11: Recon Loss = 94.6550, KL Loss = 0.0887\n",
      "Stage 1 - Epoch 21: Recon Loss = 60.4116, KL Loss = 0.2315\n",
      "Stage 1 - Epoch 31: Recon Loss = 48.8548, KL Loss = 0.3907\n",
      "Stage 1 - Epoch 41: Recon Loss = 40.0105, KL Loss = 0.5574\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Train on nominal data\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# print(\"Pretraining on nominal data...\")\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m mod \u001b[38;5;241m=\u001b[39m two_stage_training(fine_tuned, dataloader_bad, num_epochs_stage1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m, num_epochs_stage2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m, \n\u001b[0;32m     27\u001b[0m                    patience_stage1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, patience_stage2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, lr_stage1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, lr_stage2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m,\n\u001b[0;32m     28\u001b[0m                    batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, rho_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.0\u001b[39m, alpha_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\n\u001b[0;32m     30\u001b[0m X_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m,device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     31\u001b[0m T_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m,device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[45], line 46\u001b[0m, in \u001b[0;36mtwo_stage_training\u001b[1;34m(model, dataloader, num_epochs_stage1, num_epochs_stage2, patience_stage1, patience_stage2, lr_stage1, lr_stage2, batch_size, rho_max, alpha_max)\u001b[0m\n\u001b[0;32m     43\u001b[0m optimizer_stage1\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m recon_X, mu, logvar, adj_now, adj_lag \u001b[38;5;241m=\u001b[39m model(X_batch, time_batch)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Focus only on reconstruction and KL loss\u001b[39;00m\n\u001b[0;32m     49\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(recon_X, X_batch, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\Documents\\Doctorate\\praxis_research\\CARAT_CI\\src\\CARAT\\model.py:103\u001b[0m, in \u001b[0;36mCausalGraphVAE.forward\u001b[1;34m(self, X, time_context)\u001b[0m\n\u001b[0;32m    100\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Convert input X to float32\u001b[39;00m\n\u001b[0;32m    101\u001b[0m time_context \u001b[38;5;241m=\u001b[39m time_context\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Convert time context to float32\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m mu, logvar, adj_now, adj_lag, X_transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(X, time_context)\n\u001b[0;32m    104\u001b[0m Z \u001b[38;5;241m=\u001b[39m mu \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(logvar) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m logvar)\n\u001b[0;32m    105\u001b[0m recon_X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(Z, adj_now, adj_lag, X_transformed)\n",
      "File \u001b[1;32m~\\Documents\\Doctorate\\praxis_research\\CARAT_CI\\src\\CARAT\\model.py:74\u001b[0m, in \u001b[0;36mCausalGraphVAE.encode\u001b[1;34m(self, X, time_context)\u001b[0m\n\u001b[0;32m     71\u001b[0m mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu_layer(memory[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, :]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogvar_layer(memory[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, :])\n\u001b[0;32m     72\u001b[0m Z \u001b[38;5;241m=\u001b[39m mu \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(logvar) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m logvar)\n\u001b[1;32m---> 74\u001b[0m adj_now, adj_lag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcausal_graph(memory, time_context)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mu, logvar, adj_now, adj_lag, memory\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\Documents\\Doctorate\\praxis_research\\CARAT_CI\\src\\CARAT\\components.py:222\u001b[0m, in \u001b[0;36mTemporalCausalGraph.forward\u001b[1;34m(self, X_transformed, time_context)\u001b[0m\n\u001b[0;32m    219\u001b[0m avg_features \u001b[38;5;241m=\u001b[39m X_transformed[t]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [hidden_dim]\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# 1. Method: Correlation-based edges\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m corr_edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorrelation_based_edges(avg_features)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# 2. Method: Attention-based edges\u001b[39;00m\n\u001b[0;32m    225\u001b[0m attn_edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_based_edges(avg_features)\n",
      "File \u001b[1;32m~\\Documents\\Doctorate\\praxis_research\\CARAT_CI\\src\\CARAT\\components.py:106\u001b[0m, in \u001b[0;36mcorrelation_based_edges\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    100\u001b[0m if features.dim() == 1:\n\u001b[0;32m    101\u001b[0m     # For 1D features, we need a different approach\n\u001b[0;32m    102\u001b[0m     # Since we don't have true correlations from a single sample\n\u001b[0;32m    103\u001b[0m     # Create a sparse random matrix with values between 0.1 and 0.9\n\u001b[0;32m    104\u001b[0m     edge_scores = torch.rand(self.num_nodes, self.num_nodes, device=self.device) * 0.8 + 0.1\n\u001b[1;32m--> 106\u001b[0m     # Make it symmetric (for undirected connections)\n\u001b[0;32m    107\u001b[0m     edge_scores = (edge_scores + edge_scores.t()) / 2\n\u001b[0;32m    109\u001b[0m     # Add some structure based on the original feature values\n\u001b[0;32m    110\u001b[0m     # Nodes with similar values in the feature vector will have stronger connections\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TIME_STEPS = 7\n",
    "BATCH_SIZE = 25\n",
    "hidden_dim = 64\n",
    "latent_dim = 8\n",
    "edge_correct = 0\n",
    "instantaneous_correct = 0\n",
    "lagged_correct = 0\n",
    "counterfactual_correct = 0 \n",
    "rr_correct = 0\n",
    "total_correct = 0\n",
    "total_checked = 0\n",
    "for i in range(30):\n",
    "    total_checked+=1\n",
    "\n",
    "    dataset_bad = TimeSeriesDataset(bad, device=device, time_steps=TIME_STEPS)\n",
    "    dataloader_bad = DataLoader(dataset_bad, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    fine_tuned = CausalGraphVAE(input_dim=bad.shape[1], hidden_dim=hidden_dim,\n",
    "                            latent_dim=latent_dim, num_nodes=bad.shape[1],device=device,\n",
    "                            time_steps=TIME_STEPS, prior_adj=prior_adj,instantaneous_weight=0.05).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    \n",
    "    # Train on nominal data\n",
    "    # print(\"Pretraining on nominal data...\")\n",
    "    mod = two_stage_training(fine_tuned, dataloader_bad, num_epochs_stage1=250, num_epochs_stage2=250, \n",
    "                       patience_stage1=10, patience_stage2=10, lr_stage1=1e-4, lr_stage2=5e-4,\n",
    "                       batch_size=64, rho_max=4.0, alpha_max=2.0)\n",
    "    \n",
    "    X_data = torch.empty(0,device=device)\n",
    "    T_data = torch.empty(0,device=device)\n",
    "    for batch_idx, (X_batch, time_batch) in enumerate(dataloader_bad):\n",
    "        X_data = torch.cat((X_data[:batch_idx], X_batch, X_data[batch_idx:]))\n",
    "        T_data = torch.cat((T_data[:batch_idx], time_batch, T_data[batch_idx:]))\n",
    "    \n",
    "    causes= mod.infer_causal_effect(X_data,T_data,'x_4',cols,non_causal_indices=[])\n",
    "    \n",
    "    edge_cause_1 = causes.sort_values(by='causes',ascending=False)[0:3].index[0]\n",
    "    edge_cause_2 = causes.sort_values(by='causes',ascending=False)[0:3].index[1]\n",
    "    edge_cause_3 = causes.sort_values(by='causes',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    instant_cause_1 = causes.sort_values(by='instantaneous',ascending=False)[0:3].index[0]\n",
    "    instant_cause_2 = causes.sort_values(by='instantaneous',ascending=False)[0:3].index[1]\n",
    "    instant_cause_3 = causes.sort_values(by='instantaneous',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    lag_cause_1 = causes.sort_values(by='lagged',ascending=False)[0:3].index[0]\n",
    "    lag_cause_2 = causes.sort_values(by='lagged',ascending=False)[0:3].index[1]\n",
    "    lag_cause_3 = causes.sort_values(by='lagged',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    counterfactual_cause_1 = causes.sort_values(by='counterfactuals',ascending=False)[0:3].index[0]\n",
    "    counterfactual_cause_2 = causes.sort_values(by='counterfactuals',ascending=False)[0:3].index[1]\n",
    "    counterfactual_cause_3 = causes.sort_values(by='counterfactuals',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    rr_cause_1 = causes.sort_values(by='RootRank',ascending=False)[0:3].index[0]\n",
    "    rr_cause_2 = causes.sort_values(by='RootRank',ascending=False)[0:3].index[1]\n",
    "    rr_cause_3 = causes.sort_values(by='RootRank',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    total_score_cause_1=causes.sort_values(by='causal_strength',ascending=False)[0:3].index[0]\n",
    "    total_score_cause_2=causes.sort_values(by='causal_strength',ascending=False)[0:3].index[1]\n",
    "    total_score_cause_3=causes.sort_values(by='causal_strength',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    if (edge_cause_1 in ['x_1','x_44']) :\n",
    "        edge_correct+=1\n",
    "    \n",
    "    if (  total_score_cause_1 in ['x_1','x_44']):\n",
    "        total_correct+=1\n",
    "    \n",
    "    if (counterfactual_cause_1 in ['x_1','x_44']) :\n",
    "        counterfactual_correct+=1\n",
    "    \n",
    "    if (instant_cause_1 in ['x_1','x_44']):\n",
    "        instantaneous_correct+=1\n",
    "    \n",
    "    if (lag_cause_1 in ['x_1','x_44']) :\n",
    "        lagged_correct+=1\n",
    "    \n",
    "    if (rr_cause_1 in ['x_1','x_44']) :\n",
    "        rr_correct+=1\n",
    "        \n",
    "    total_accuracy = total_correct/total_checked* 100\n",
    "    edge_accuracy = edge_correct/total_checked* 100\n",
    "    cf_accuracy = counterfactual_correct/total_checked* 100\n",
    "    instant_accuracy = instantaneous_correct/total_checked* 100\n",
    "    lag_accuracy = lagged_correct/total_checked* 100\n",
    "    rr_accuracy = rr_correct/total_checked* 100\n",
    "    \n",
    "    \n",
    "    logger.info(f\"Edge Accuracy = {edge_accuracy:.2f}, Instantaneous Accuracy = {instant_accuracy:.2f}, Lagged Accuracy = {lag_accuracy:.2f}, Counterfactual Accuracy = {cf_accuracy:.2f},  Blended Accuracy = {total_accuracy:.2f},  RR Accuracy = {rr_accuracy:.2f}  \") \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6df2f38-5a3b-4fc7-a968-877416d2567f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 462.8883\n",
      "Recon Loss = 329.9856, KL Loss = 0.0000, Lagrangian Loss = 35.1801\n",
      "Early stopping triggered. Last Epoch: 33\n",
      "Recon Loss = 314.8075, KL Loss = 0.0940, Lagrangian Loss = 36.2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 17:51:49,001 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 100.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 0.00,  Blended Accuracy = 0.00,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 636.0375\n",
      "Recon Loss = 409.4594, KL Loss = 0.0000, Lagrangian Loss = 84.5073\n",
      "Early stopping triggered. Last Epoch: 36\n",
      "Recon Loss = 415.0301, KL Loss = 0.1863, Lagrangian Loss = 87.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 17:53:00,877 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 100.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 50.00,  Blended Accuracy = 50.00,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 445.9367\n",
      "Recon Loss = 328.4622, KL Loss = 0.0000, Lagrangian Loss = 10.7169\n",
      "Early stopping triggered. Last Epoch: 47\n",
      "Recon Loss = 333.6390, KL Loss = 0.2354, Lagrangian Loss = 11.5030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 17:54:33,057 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 100.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 66.67,  Blended Accuracy = 66.67,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 471.1117\n",
      "Recon Loss = 321.1597, KL Loss = 0.0000, Lagrangian Loss = 51.7619\n",
      "Early stopping triggered. Last Epoch: 44\n",
      "Recon Loss = 267.9358, KL Loss = 0.4612, Lagrangian Loss = 54.9309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 17:55:58,821 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 100.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 75.00,  Blended Accuracy = 75.00,  RR Accuracy = 0.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 487.2192\n",
      "Recon Loss = 298.9534, KL Loss = 0.0000, Lagrangian Loss = 62.3585\n",
      "Early stopping triggered. Last Epoch: 29\n",
      "Recon Loss = 301.6239, KL Loss = 0.0976, Lagrangian Loss = 63.8550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 17:56:57,973 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 100.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 80.00,  Blended Accuracy = 80.00,  RR Accuracy = 20.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 630.5689\n",
      "Recon Loss = 375.8698, KL Loss = 0.0000, Lagrangian Loss = 103.1679\n",
      "Early stopping triggered. Last Epoch: 27\n",
      "Recon Loss = 440.8449, KL Loss = 0.1301, Lagrangian Loss = 108.0715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 17:57:50,067 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 100.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 83.33,  Blended Accuracy = 83.33,  RR Accuracy = 33.33  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 611.9869\n",
      "Recon Loss = 455.1457, KL Loss = 0.0000, Lagrangian Loss = 27.2031\n",
      "Early stopping triggered. Last Epoch: 46\n",
      "Recon Loss = 395.2539, KL Loss = 0.3173, Lagrangian Loss = 29.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 17:59:19,475 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 100.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 85.71,  Blended Accuracy = 85.71,  RR Accuracy = 42.86  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 490.0683\n",
      "Recon Loss = 359.3690, KL Loss = 0.0000, Lagrangian Loss = 16.0919\n",
      "Early stopping triggered. Last Epoch: 49\n",
      "Recon Loss = 361.9183, KL Loss = 0.2135, Lagrangian Loss = 17.3508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:00:53,894 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 100.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 75.00,  Blended Accuracy = 75.00,  RR Accuracy = 37.50  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 566.5613\n",
      "Recon Loss = 380.3890, KL Loss = 0.0000, Lagrangian Loss = 28.4615\n",
      "Early stopping triggered. Last Epoch: 20\n",
      "Recon Loss = 413.3433, KL Loss = 0.1473, Lagrangian Loss = 28.8309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:01:33,415 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 100.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 66.67,  Blended Accuracy = 66.67,  RR Accuracy = 33.33  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 647.3255\n",
      "Recon Loss = 425.4727, KL Loss = 0.0000, Lagrangian Loss = 33.9801\n",
      "Early stopping triggered. Last Epoch: 36\n",
      "Recon Loss = 438.3860, KL Loss = 0.1574, Lagrangian Loss = 35.4048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:02:42,940 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 100.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 70.00,  Blended Accuracy = 70.00,  RR Accuracy = 40.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 514.8417\n",
      "Recon Loss = 335.0575, KL Loss = 0.0000, Lagrangian Loss = 47.6468\n",
      "Early stopping triggered. Last Epoch: 26\n",
      "Recon Loss = 310.2365, KL Loss = 0.1855, Lagrangian Loss = 49.0885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:03:33,687 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 100.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 63.64,  Blended Accuracy = 63.64,  RR Accuracy = 36.36  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 391.3974\n",
      "Recon Loss = 260.0343, KL Loss = 0.0000, Lagrangian Loss = 35.0737\n",
      "Early stopping triggered. Last Epoch: 39\n",
      "Recon Loss = 279.7959, KL Loss = 0.2130, Lagrangian Loss = 36.7177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:04:46,899 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 100.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 66.67,  Blended Accuracy = 66.67,  RR Accuracy = 41.67  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 535.7689\n",
      "Recon Loss = 392.7761, KL Loss = 0.0000, Lagrangian Loss = 22.0807\n",
      "Early stopping triggered. Last Epoch: 23\n",
      "Recon Loss = 402.8713, KL Loss = 0.0662, Lagrangian Loss = 22.3194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:05:33,426 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 100.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 61.54,  Blended Accuracy = 61.54,  RR Accuracy = 38.46  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 440.4711\n",
      "Recon Loss = 305.6276, KL Loss = 0.0000, Lagrangian Loss = 14.2175\n",
      "Early stopping triggered. Last Epoch: 25\n",
      "Recon Loss = 295.7242, KL Loss = 0.1042, Lagrangian Loss = 14.6112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:06:25,532 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 100.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 64.29,  Blended Accuracy = 64.29,  RR Accuracy = 35.71  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 592.4690\n",
      "Recon Loss = 440.5869, KL Loss = 0.0000, Lagrangian Loss = 35.2912\n",
      "Early stopping triggered. Last Epoch: 33\n",
      "Recon Loss = 424.5151, KL Loss = 0.1663, Lagrangian Loss = 36.5116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:07:31,149 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 100.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 60.00,  Blended Accuracy = 60.00,  RR Accuracy = 33.33  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 475.6707\n",
      "Recon Loss = 345.5203, KL Loss = 0.0000, Lagrangian Loss = 26.9438\n",
      "Early stopping triggered. Last Epoch: 29\n",
      "Recon Loss = 339.6374, KL Loss = 0.1186, Lagrangian Loss = 27.4741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:08:25,148 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 100.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 62.50,  Blended Accuracy = 62.50,  RR Accuracy = 31.25  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 664.3255\n",
      "Recon Loss = 446.9125, KL Loss = 0.0000, Lagrangian Loss = 53.2590\n",
      "Early stopping triggered. Last Epoch: 28\n",
      "Recon Loss = 471.0415, KL Loss = 0.1679, Lagrangian Loss = 54.5146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:09:18,587 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 94.12, Lagged Accuracy = 100.00, Counterfactual Accuracy = 58.82,  Blended Accuracy = 58.82,  RR Accuracy = 29.41  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 447.7123\n",
      "Recon Loss = 323.1378, KL Loss = 0.0000, Lagrangian Loss = 22.3760\n",
      "Epoch 51: Loss = 459.6946\n",
      "Recon Loss = 335.7008, KL Loss = 0.2298, Lagrangian Loss = 24.1539\n",
      "Early stopping triggered. Last Epoch: 64\n",
      "Recon Loss = 325.1032, KL Loss = 0.2958, Lagrangian Loss = 25.3038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:11:18,105 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 94.44, Lagged Accuracy = 100.00, Counterfactual Accuracy = 55.56,  Blended Accuracy = 55.56,  RR Accuracy = 27.78  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 406.3809\n",
      "Recon Loss = 313.0916, KL Loss = 0.0000, Lagrangian Loss = 41.0635\n",
      "Early stopping triggered. Last Epoch: 31\n",
      "Recon Loss = 252.3540, KL Loss = 0.1136, Lagrangian Loss = 42.5950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:12:19,738 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 94.74, Lagged Accuracy = 100.00, Counterfactual Accuracy = 57.89,  Blended Accuracy = 57.89,  RR Accuracy = 31.58  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 496.6887\n",
      "Recon Loss = 379.8770, KL Loss = 0.0000, Lagrangian Loss = 24.9470\n",
      "Early stopping triggered. Last Epoch: 43\n",
      "Recon Loss = 384.2023, KL Loss = 0.2281, Lagrangian Loss = 26.4468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:13:44,152 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 95.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 60.00,  Blended Accuracy = 60.00,  RR Accuracy = 35.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 686.2302\n",
      "Recon Loss = 478.5004, KL Loss = 0.0000, Lagrangian Loss = 33.9327\n",
      "Early stopping triggered. Last Epoch: 23\n",
      "Recon Loss = 537.1292, KL Loss = 0.0952, Lagrangian Loss = 35.6162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:14:29,922 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 95.24, Lagged Accuracy = 100.00, Counterfactual Accuracy = 61.90,  Blended Accuracy = 61.90,  RR Accuracy = 38.10  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 471.9480\n",
      "Recon Loss = 314.2253, KL Loss = 0.0000, Lagrangian Loss = 18.8466\n",
      "Early stopping triggered. Last Epoch: 22\n",
      "Recon Loss = 347.8652, KL Loss = 0.1172, Lagrangian Loss = 19.1653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:15:15,201 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 95.45, Lagged Accuracy = 100.00, Counterfactual Accuracy = 59.09,  Blended Accuracy = 59.09,  RR Accuracy = 36.36  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 602.9496\n",
      "Recon Loss = 465.5386, KL Loss = 0.0000, Lagrangian Loss = 26.4957\n",
      "Epoch 51: Loss = 606.7837\n",
      "Recon Loss = 451.5706, KL Loss = 0.3988, Lagrangian Loss = 28.6245\n",
      "Early stopping triggered. Last Epoch: 54\n",
      "Recon Loss = 395.2297, KL Loss = 0.4501, Lagrangian Loss = 29.0944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:17:00,217 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 95.65, Lagged Accuracy = 100.00, Counterfactual Accuracy = 60.87,  Blended Accuracy = 60.87,  RR Accuracy = 39.13  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 534.3744\n",
      "Recon Loss = 355.8215, KL Loss = 0.0000, Lagrangian Loss = 35.0015\n",
      "Early stopping triggered. Last Epoch: 22\n",
      "Recon Loss = 343.9540, KL Loss = 0.1114, Lagrangian Loss = 35.6596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:17:45,120 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 95.83, Lagged Accuracy = 100.00, Counterfactual Accuracy = 58.33,  Blended Accuracy = 58.33,  RR Accuracy = 37.50  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 425.4904\n",
      "Recon Loss = 297.2585, KL Loss = 0.0000, Lagrangian Loss = 24.8331\n",
      "Early stopping triggered. Last Epoch: 30\n",
      "Recon Loss = 281.1690, KL Loss = 0.1609, Lagrangian Loss = 25.7194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:18:43,885 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 96.00, Lagged Accuracy = 100.00, Counterfactual Accuracy = 56.00,  Blended Accuracy = 56.00,  RR Accuracy = 36.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 476.9857\n",
      "Recon Loss = 329.4189, KL Loss = 0.0000, Lagrangian Loss = 35.3203\n",
      "Early stopping triggered. Last Epoch: 26\n",
      "Recon Loss = 364.8055, KL Loss = 0.1724, Lagrangian Loss = 36.2249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:19:37,033 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 96.15, Lagged Accuracy = 100.00, Counterfactual Accuracy = 57.69,  Blended Accuracy = 57.69,  RR Accuracy = 34.62  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 589.2321\n",
      "Recon Loss = 384.2822, KL Loss = 0.0000, Lagrangian Loss = 54.0546\n",
      "Epoch 51: Loss = 596.5719\n",
      "Recon Loss = 383.7511, KL Loss = 0.3680, Lagrangian Loss = 58.1660\n",
      "Early stopping triggered. Last Epoch: 72\n",
      "Recon Loss = 394.1979, KL Loss = 0.4576, Lagrangian Loss = 62.5884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:21:53,120 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 96.30, Lagged Accuracy = 100.00, Counterfactual Accuracy = 55.56,  Blended Accuracy = 55.56,  RR Accuracy = 33.33  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 497.3699\n",
      "Recon Loss = 366.3078, KL Loss = 0.0000, Lagrangian Loss = 11.4307\n",
      "Early stopping triggered. Last Epoch: 37\n",
      "Recon Loss = 355.1627, KL Loss = 0.1683, Lagrangian Loss = 12.2730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:23:04,997 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 96.43, Lagged Accuracy = 100.00, Counterfactual Accuracy = 53.57,  Blended Accuracy = 53.57,  RR Accuracy = 32.14  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 445.3503\n",
      "Recon Loss = 304.2414, KL Loss = 0.0000, Lagrangian Loss = 24.1424\n",
      "Early stopping triggered. Last Epoch: 20\n",
      "Recon Loss = 336.4634, KL Loss = 0.0844, Lagrangian Loss = 24.4539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:23:45,441 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 96.55, Lagged Accuracy = 100.00, Counterfactual Accuracy = 55.17,  Blended Accuracy = 55.17,  RR Accuracy = 34.48  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 488.0731\n",
      "Recon Loss = 334.2580, KL Loss = 0.0000, Lagrangian Loss = 55.0326\n",
      "Early stopping triggered. Last Epoch: 37\n",
      "Recon Loss = 339.1972, KL Loss = 0.1518, Lagrangian Loss = 57.9265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:24:57,110 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 96.67, Lagged Accuracy = 100.00, Counterfactual Accuracy = 53.33,  Blended Accuracy = 53.33,  RR Accuracy = 33.33  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 615.1026\n",
      "Recon Loss = 428.4787, KL Loss = 0.0000, Lagrangian Loss = 64.2117\n",
      "Early stopping triggered. Last Epoch: 20\n",
      "Recon Loss = 432.7624, KL Loss = 0.0745, Lagrangian Loss = 64.8385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:25:37,439 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 96.77, Lagged Accuracy = 100.00, Counterfactual Accuracy = 51.61,  Blended Accuracy = 51.61,  RR Accuracy = 32.26  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 420.0882\n",
      "Recon Loss = 318.6779, KL Loss = 0.0000, Lagrangian Loss = 22.7115\n",
      "Early stopping triggered. Last Epoch: 36\n",
      "Recon Loss = 256.7640, KL Loss = 0.1289, Lagrangian Loss = 24.0657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:26:45,310 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 96.88, Lagged Accuracy = 100.00, Counterfactual Accuracy = 53.12,  Blended Accuracy = 53.12,  RR Accuracy = 34.38  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 518.4844\n",
      "Recon Loss = 368.3832, KL Loss = 0.0000, Lagrangian Loss = 30.7481\n",
      "Early stopping triggered. Last Epoch: 22\n",
      "Recon Loss = 386.1966, KL Loss = 0.0789, Lagrangian Loss = 31.4119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:27:29,098 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 96.97, Lagged Accuracy = 100.00, Counterfactual Accuracy = 54.55,  Blended Accuracy = 54.55,  RR Accuracy = 36.36  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 582.6207\n",
      "Recon Loss = 416.7713, KL Loss = 0.0000, Lagrangian Loss = 21.7059\n",
      "Early stopping triggered. Last Epoch: 25\n",
      "Recon Loss = 416.7998, KL Loss = 0.0814, Lagrangian Loss = 22.2144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:28:17,351 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 97.06, Lagged Accuracy = 100.00, Counterfactual Accuracy = 55.88,  Blended Accuracy = 55.88,  RR Accuracy = 35.29  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 558.1827\n",
      "Recon Loss = 388.1357, KL Loss = 0.0000, Lagrangian Loss = 27.3090\n",
      "Early stopping triggered. Last Epoch: 41\n",
      "Recon Loss = 423.4677, KL Loss = 0.1227, Lagrangian Loss = 28.8737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:29:34,784 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 97.14, Lagged Accuracy = 100.00, Counterfactual Accuracy = 54.29,  Blended Accuracy = 54.29,  RR Accuracy = 34.29  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 481.4133\n",
      "Recon Loss = 334.1935, KL Loss = 0.0000, Lagrangian Loss = 31.2385\n",
      "Early stopping triggered. Last Epoch: 33\n",
      "Recon Loss = 374.8163, KL Loss = 0.1721, Lagrangian Loss = 32.3018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:30:37,668 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 94.44, Lagged Accuracy = 100.00, Counterfactual Accuracy = 55.56,  Blended Accuracy = 55.56,  RR Accuracy = 36.11  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 536.1196\n",
      "Recon Loss = 358.0837, KL Loss = 0.0000, Lagrangian Loss = 32.6000\n",
      "Early stopping triggered. Last Epoch: 39\n",
      "Recon Loss = 359.6551, KL Loss = 0.1366, Lagrangian Loss = 34.7885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:31:53,017 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 94.59, Lagged Accuracy = 100.00, Counterfactual Accuracy = 54.05,  Blended Accuracy = 54.05,  RR Accuracy = 37.84  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 545.4722\n",
      "Recon Loss = 396.3150, KL Loss = 0.0000, Lagrangian Loss = 38.0944\n",
      "Early stopping triggered. Last Epoch: 42\n",
      "Recon Loss = 371.4050, KL Loss = 0.1622, Lagrangian Loss = 40.1938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:33:12,899 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 94.74, Lagged Accuracy = 100.00, Counterfactual Accuracy = 55.26,  Blended Accuracy = 55.26,  RR Accuracy = 39.47  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 467.6657\n",
      "Recon Loss = 265.6773, KL Loss = 0.0000, Lagrangian Loss = 90.5377\n",
      "Early stopping triggered. Last Epoch: 21\n",
      "Recon Loss = 264.4128, KL Loss = 0.0814, Lagrangian Loss = 92.2627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:33:53,858 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 94.87, Lagged Accuracy = 100.00, Counterfactual Accuracy = 53.85,  Blended Accuracy = 53.85,  RR Accuracy = 38.46  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 515.2365\n",
      "Recon Loss = 377.0325, KL Loss = 0.0000, Lagrangian Loss = 14.3959\n",
      "Early stopping triggered. Last Epoch: 35\n",
      "Recon Loss = 335.9560, KL Loss = 0.1823, Lagrangian Loss = 15.0916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:35:01,650 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 95.00, Lagged Accuracy = 97.50, Counterfactual Accuracy = 55.00,  Blended Accuracy = 55.00,  RR Accuracy = 37.50  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 611.2346\n",
      "Recon Loss = 497.8850, KL Loss = 0.0000, Lagrangian Loss = 57.2217\n",
      "Early stopping triggered. Last Epoch: 41\n",
      "Recon Loss = 447.1963, KL Loss = 0.2236, Lagrangian Loss = 59.7453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:36:21,580 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 95.12, Lagged Accuracy = 97.56, Counterfactual Accuracy = 56.10,  Blended Accuracy = 56.10,  RR Accuracy = 39.02  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 770.8433\n",
      "Recon Loss = 553.6843, KL Loss = 0.0000, Lagrangian Loss = 17.4607\n",
      "Early stopping triggered. Last Epoch: 24\n",
      "Recon Loss = 576.5599, KL Loss = 0.0966, Lagrangian Loss = 17.6594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:37:06,375 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 95.24, Lagged Accuracy = 97.62, Counterfactual Accuracy = 57.14,  Blended Accuracy = 57.14,  RR Accuracy = 38.10  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 507.9900\n",
      "Recon Loss = 366.0477, KL Loss = 0.0000, Lagrangian Loss = 29.8946\n",
      "Early stopping triggered. Last Epoch: 20\n",
      "Recon Loss = 357.0333, KL Loss = 0.0939, Lagrangian Loss = 30.5130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:37:46,386 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 95.35, Lagged Accuracy = 97.67, Counterfactual Accuracy = 58.14,  Blended Accuracy = 58.14,  RR Accuracy = 39.53  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 441.0878\n",
      "Recon Loss = 302.8326, KL Loss = 0.0000, Lagrangian Loss = 62.2390\n",
      "Early stopping triggered. Last Epoch: 35\n",
      "Recon Loss = 236.3150, KL Loss = 0.3041, Lagrangian Loss = 65.8071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:38:52,296 INFO -- Edge Accuracy = 97.73, Instantaneous Accuracy = 95.45, Lagged Accuracy = 95.45, Counterfactual Accuracy = 59.09,  Blended Accuracy = 59.09,  RR Accuracy = 38.64  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 626.8402\n",
      "Recon Loss = 500.8649, KL Loss = 0.0000, Lagrangian Loss = 30.7394\n",
      "Early stopping triggered. Last Epoch: 24\n",
      "Recon Loss = 415.9316, KL Loss = 0.1189, Lagrangian Loss = 31.3516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:39:39,651 INFO -- Edge Accuracy = 97.78, Instantaneous Accuracy = 95.56, Lagged Accuracy = 95.56, Counterfactual Accuracy = 60.00,  Blended Accuracy = 60.00,  RR Accuracy = 40.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 769.6055\n",
      "Recon Loss = 529.6830, KL Loss = 0.0000, Lagrangian Loss = 15.5577\n",
      "Early stopping triggered. Last Epoch: 30\n",
      "Recon Loss = 571.5584, KL Loss = 0.1022, Lagrangian Loss = 15.7854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:40:35,869 INFO -- Edge Accuracy = 97.83, Instantaneous Accuracy = 95.65, Lagged Accuracy = 95.65, Counterfactual Accuracy = 60.87,  Blended Accuracy = 60.87,  RR Accuracy = 39.13  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 605.7043\n",
      "Recon Loss = 416.3158, KL Loss = 0.0000, Lagrangian Loss = 35.7474\n",
      "Early stopping triggered. Last Epoch: 24\n",
      "Recon Loss = 407.5402, KL Loss = 0.0894, Lagrangian Loss = 37.4586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:41:23,432 INFO -- Edge Accuracy = 97.87, Instantaneous Accuracy = 93.62, Lagged Accuracy = 95.74, Counterfactual Accuracy = 59.57,  Blended Accuracy = 59.57,  RR Accuracy = 38.30  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 488.1645\n",
      "Recon Loss = 336.3693, KL Loss = 0.0000, Lagrangian Loss = 43.8603\n",
      "Early stopping triggered. Last Epoch: 25\n",
      "Recon Loss = 375.9398, KL Loss = 0.0790, Lagrangian Loss = 44.7951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:42:14,156 INFO -- Edge Accuracy = 97.92, Instantaneous Accuracy = 93.75, Lagged Accuracy = 95.83, Counterfactual Accuracy = 58.33,  Blended Accuracy = 60.42,  RR Accuracy = 37.50  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 606.3179\n",
      "Recon Loss = 424.4199, KL Loss = 0.0000, Lagrangian Loss = 61.8068\n",
      "Early stopping triggered. Last Epoch: 22\n",
      "Recon Loss = 408.7496, KL Loss = 0.1063, Lagrangian Loss = 62.6351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:42:57,131 INFO -- Edge Accuracy = 97.96, Instantaneous Accuracy = 93.88, Lagged Accuracy = 95.92, Counterfactual Accuracy = 57.14,  Blended Accuracy = 59.18,  RR Accuracy = 36.73  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 540.1215\n",
      "Recon Loss = 396.2502, KL Loss = 0.0000, Lagrangian Loss = 28.9626\n",
      "Early stopping triggered. Last Epoch: 25\n",
      "Recon Loss = 400.8777, KL Loss = 0.0981, Lagrangian Loss = 27.9741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:43:45,338 INFO -- Edge Accuracy = 98.00, Instantaneous Accuracy = 94.00, Lagged Accuracy = 96.00, Counterfactual Accuracy = 56.00,  Blended Accuracy = 60.00,  RR Accuracy = 38.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 414.1386\n",
      "Recon Loss = 274.5894, KL Loss = 0.0000, Lagrangian Loss = 39.9488\n",
      "Early stopping triggered. Last Epoch: 21\n",
      "Recon Loss = 280.7302, KL Loss = 0.2084, Lagrangian Loss = 40.5707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:44:26,951 INFO -- Edge Accuracy = 98.04, Instantaneous Accuracy = 94.12, Lagged Accuracy = 96.08, Counterfactual Accuracy = 56.86,  Blended Accuracy = 60.78,  RR Accuracy = 39.22  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 644.6608\n",
      "Recon Loss = 445.1179, KL Loss = 0.0000, Lagrangian Loss = 74.4982\n",
      "Early stopping triggered. Last Epoch: 26\n",
      "Recon Loss = 401.5475, KL Loss = 0.2028, Lagrangian Loss = 75.7209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:45:16,046 INFO -- Edge Accuracy = 98.08, Instantaneous Accuracy = 94.23, Lagged Accuracy = 96.15, Counterfactual Accuracy = 57.69,  Blended Accuracy = 61.54,  RR Accuracy = 40.38  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 660.0612\n",
      "Recon Loss = 486.7778, KL Loss = 0.0000, Lagrangian Loss = 41.5330\n",
      "Epoch 51: Loss = 689.5286\n",
      "Recon Loss = 537.8130, KL Loss = 0.1368, Lagrangian Loss = 44.5476\n",
      "Early stopping triggered. Last Epoch: 58\n",
      "Recon Loss = 460.4383, KL Loss = 0.1550, Lagrangian Loss = 44.7805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:47:02,910 INFO -- Edge Accuracy = 98.11, Instantaneous Accuracy = 94.34, Lagged Accuracy = 96.23, Counterfactual Accuracy = 56.60,  Blended Accuracy = 60.38,  RR Accuracy = 39.62  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 667.4881\n",
      "Recon Loss = 465.8947, KL Loss = 0.0000, Lagrangian Loss = 32.8725\n",
      "Early stopping triggered. Last Epoch: 21\n",
      "Recon Loss = 491.5247, KL Loss = 0.0658, Lagrangian Loss = 33.2088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:47:47,170 INFO -- Edge Accuracy = 98.15, Instantaneous Accuracy = 94.44, Lagged Accuracy = 96.30, Counterfactual Accuracy = 57.41,  Blended Accuracy = 61.11,  RR Accuracy = 38.89  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 619.5896\n",
      "Recon Loss = 433.8544, KL Loss = 0.0000, Lagrangian Loss = 26.9685\n",
      "Early stopping triggered. Last Epoch: 37\n",
      "Recon Loss = 436.5178, KL Loss = 0.0857, Lagrangian Loss = 28.3301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:49:00,890 INFO -- Edge Accuracy = 98.18, Instantaneous Accuracy = 94.55, Lagged Accuracy = 96.36, Counterfactual Accuracy = 56.36,  Blended Accuracy = 61.82,  RR Accuracy = 38.18  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 475.8673\n",
      "Recon Loss = 336.4672, KL Loss = 0.0000, Lagrangian Loss = 37.2670\n",
      "Early stopping triggered. Last Epoch: 42\n",
      "Recon Loss = 326.3718, KL Loss = 0.2697, Lagrangian Loss = 40.0622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:50:20,450 INFO -- Edge Accuracy = 98.21, Instantaneous Accuracy = 94.64, Lagged Accuracy = 96.43, Counterfactual Accuracy = 57.14,  Blended Accuracy = 62.50,  RR Accuracy = 39.29  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 532.5923\n",
      "Recon Loss = 366.6209, KL Loss = 0.0000, Lagrangian Loss = 43.1751\n",
      "Early stopping triggered. Last Epoch: 29\n",
      "Recon Loss = 353.9057, KL Loss = 0.1556, Lagrangian Loss = 44.1333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:51:15,219 INFO -- Edge Accuracy = 98.25, Instantaneous Accuracy = 94.74, Lagged Accuracy = 96.49, Counterfactual Accuracy = 57.89,  Blended Accuracy = 63.16,  RR Accuracy = 40.35  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 643.0036\n",
      "Recon Loss = 435.9946, KL Loss = 0.0000, Lagrangian Loss = 56.4443\n",
      "Early stopping triggered. Last Epoch: 23\n",
      "Recon Loss = 455.6764, KL Loss = 0.0949, Lagrangian Loss = 57.3371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:52:00,844 INFO -- Edge Accuracy = 98.28, Instantaneous Accuracy = 94.83, Lagged Accuracy = 96.55, Counterfactual Accuracy = 56.90,  Blended Accuracy = 62.07,  RR Accuracy = 39.66  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 510.9018\n",
      "Recon Loss = 352.6124, KL Loss = 0.0000, Lagrangian Loss = 26.2673\n",
      "Epoch 51: Loss = 514.5086\n",
      "Recon Loss = 344.1455, KL Loss = 0.2152, Lagrangian Loss = 28.6031\n",
      "Early stopping triggered. Last Epoch: 71\n",
      "Recon Loss = 351.9461, KL Loss = 0.2293, Lagrangian Loss = 30.8586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:54:11,744 INFO -- Edge Accuracy = 98.31, Instantaneous Accuracy = 93.22, Lagged Accuracy = 96.61, Counterfactual Accuracy = 57.63,  Blended Accuracy = 62.71,  RR Accuracy = 40.68  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 595.3419\n",
      "Recon Loss = 412.1265, KL Loss = 0.0000, Lagrangian Loss = 30.7073\n",
      "Early stopping triggered. Last Epoch: 22\n",
      "Recon Loss = 416.2978, KL Loss = 0.1172, Lagrangian Loss = 31.1763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:54:55,286 INFO -- Edge Accuracy = 98.33, Instantaneous Accuracy = 93.33, Lagged Accuracy = 96.67, Counterfactual Accuracy = 58.33,  Blended Accuracy = 63.33,  RR Accuracy = 41.67  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 576.8931\n",
      "Recon Loss = 396.6979, KL Loss = 0.0000, Lagrangian Loss = 49.0281\n",
      "Early stopping triggered. Last Epoch: 29\n",
      "Recon Loss = 389.1869, KL Loss = 0.2225, Lagrangian Loss = 50.6616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:55:51,499 INFO -- Edge Accuracy = 98.36, Instantaneous Accuracy = 93.44, Lagged Accuracy = 96.72, Counterfactual Accuracy = 59.02,  Blended Accuracy = 63.93,  RR Accuracy = 42.62  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 506.4777\n",
      "Recon Loss = 364.7623, KL Loss = 0.0000, Lagrangian Loss = 32.8492\n",
      "Early stopping triggered. Last Epoch: 33\n",
      "Recon Loss = 343.6104, KL Loss = 0.2255, Lagrangian Loss = 33.9014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:56:54,377 INFO -- Edge Accuracy = 98.39, Instantaneous Accuracy = 93.55, Lagged Accuracy = 96.77, Counterfactual Accuracy = 59.68,  Blended Accuracy = 64.52,  RR Accuracy = 43.55  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 554.4976\n",
      "Recon Loss = 386.0927, KL Loss = 0.0000, Lagrangian Loss = 29.1502\n",
      "Early stopping triggered. Last Epoch: 39\n",
      "Recon Loss = 381.6108, KL Loss = 0.1411, Lagrangian Loss = 29.5544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:58:09,659 INFO -- Edge Accuracy = 98.41, Instantaneous Accuracy = 92.06, Lagged Accuracy = 96.83, Counterfactual Accuracy = 60.32,  Blended Accuracy = 65.08,  RR Accuracy = 44.44  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 501.0309\n",
      "Recon Loss = 309.7285, KL Loss = 0.0000, Lagrangian Loss = 20.0032\n",
      "Early stopping triggered. Last Epoch: 26\n",
      "Recon Loss = 347.4255, KL Loss = 0.1212, Lagrangian Loss = 20.6896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 18:59:00,301 INFO -- Edge Accuracy = 98.44, Instantaneous Accuracy = 92.19, Lagged Accuracy = 96.88, Counterfactual Accuracy = 59.38,  Blended Accuracy = 64.06,  RR Accuracy = 43.75  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 591.4394\n",
      "Recon Loss = 386.9654, KL Loss = 0.0000, Lagrangian Loss = 88.6945\n",
      "Early stopping triggered. Last Epoch: 41\n",
      "Recon Loss = 373.6681, KL Loss = 0.1408, Lagrangian Loss = 92.1151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:00:17,828 INFO -- Edge Accuracy = 98.46, Instantaneous Accuracy = 92.31, Lagged Accuracy = 96.92, Counterfactual Accuracy = 60.00,  Blended Accuracy = 64.62,  RR Accuracy = 44.62  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 573.1170\n",
      "Recon Loss = 406.7724, KL Loss = 0.0000, Lagrangian Loss = 18.8880\n",
      "Early stopping triggered. Last Epoch: 35\n",
      "Recon Loss = 408.8212, KL Loss = 0.0901, Lagrangian Loss = 19.7564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:01:24,570 INFO -- Edge Accuracy = 98.48, Instantaneous Accuracy = 92.42, Lagged Accuracy = 96.97, Counterfactual Accuracy = 60.61,  Blended Accuracy = 65.15,  RR Accuracy = 43.94  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 527.9710\n",
      "Recon Loss = 297.8336, KL Loss = 0.0000, Lagrangian Loss = 100.8432\n",
      "Early stopping triggered. Last Epoch: 27\n",
      "Recon Loss = 328.0282, KL Loss = 0.0956, Lagrangian Loss = 103.5951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:02:14,995 INFO -- Edge Accuracy = 98.51, Instantaneous Accuracy = 91.04, Lagged Accuracy = 97.01, Counterfactual Accuracy = 61.19,  Blended Accuracy = 65.67,  RR Accuracy = 44.78  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 537.9344\n",
      "Recon Loss = 385.5340, KL Loss = 0.0000, Lagrangian Loss = 22.5581\n",
      "Early stopping triggered. Last Epoch: 20\n",
      "Recon Loss = 392.2213, KL Loss = 0.1393, Lagrangian Loss = 23.2453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:02:53,347 INFO -- Edge Accuracy = 98.53, Instantaneous Accuracy = 91.18, Lagged Accuracy = 97.06, Counterfactual Accuracy = 61.76,  Blended Accuracy = 66.18,  RR Accuracy = 44.12  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 714.3881\n",
      "Recon Loss = 439.6879, KL Loss = 0.0000, Lagrangian Loss = 95.0462\n",
      "Early stopping triggered. Last Epoch: 20\n",
      "Recon Loss = 444.3973, KL Loss = 0.0534, Lagrangian Loss = 94.4761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:03:31,477 INFO -- Edge Accuracy = 98.55, Instantaneous Accuracy = 89.86, Lagged Accuracy = 97.10, Counterfactual Accuracy = 60.87,  Blended Accuracy = 65.22,  RR Accuracy = 43.48  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 769.6033\n",
      "Recon Loss = 488.5280, KL Loss = 0.0000, Lagrangian Loss = 118.2248\n",
      "Early stopping triggered. Last Epoch: 41\n",
      "Recon Loss = 500.9627, KL Loss = 0.1226, Lagrangian Loss = 123.9152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:04:46,921 INFO -- Edge Accuracy = 98.57, Instantaneous Accuracy = 88.57, Lagged Accuracy = 97.14, Counterfactual Accuracy = 61.43,  Blended Accuracy = 65.71,  RR Accuracy = 44.29  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 552.7323\n",
      "Recon Loss = 386.5259, KL Loss = 0.0000, Lagrangian Loss = 54.6104\n",
      "Early stopping triggered. Last Epoch: 29\n",
      "Recon Loss = 360.9606, KL Loss = 0.0803, Lagrangian Loss = 55.7237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:05:42,217 INFO -- Edge Accuracy = 98.59, Instantaneous Accuracy = 88.73, Lagged Accuracy = 97.18, Counterfactual Accuracy = 61.97,  Blended Accuracy = 66.20,  RR Accuracy = 45.07  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 593.5636\n",
      "Recon Loss = 424.4420, KL Loss = 0.0000, Lagrangian Loss = 25.7960\n",
      "Early stopping triggered. Last Epoch: 38\n",
      "Recon Loss = 468.9685, KL Loss = 0.1128, Lagrangian Loss = 27.5973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:06:52,104 INFO -- Edge Accuracy = 98.61, Instantaneous Accuracy = 88.89, Lagged Accuracy = 97.22, Counterfactual Accuracy = 61.11,  Blended Accuracy = 65.28,  RR Accuracy = 44.44  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 559.0688\n",
      "Recon Loss = 383.3591, KL Loss = 0.0000, Lagrangian Loss = 36.7382\n",
      "Early stopping triggered. Last Epoch: 26\n",
      "Recon Loss = 363.0219, KL Loss = 0.1337, Lagrangian Loss = 37.1223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:07:41,702 INFO -- Edge Accuracy = 98.63, Instantaneous Accuracy = 89.04, Lagged Accuracy = 97.26, Counterfactual Accuracy = 60.27,  Blended Accuracy = 64.38,  RR Accuracy = 43.84  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 480.5243\n",
      "Recon Loss = 358.2636, KL Loss = 0.0000, Lagrangian Loss = 26.5530\n",
      "Early stopping triggered. Last Epoch: 26\n",
      "Recon Loss = 341.5189, KL Loss = 0.1099, Lagrangian Loss = 26.9903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:08:30,316 INFO -- Edge Accuracy = 98.65, Instantaneous Accuracy = 89.19, Lagged Accuracy = 97.30, Counterfactual Accuracy = 60.81,  Blended Accuracy = 64.86,  RR Accuracy = 44.59  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 485.8720\n",
      "Recon Loss = 350.2943, KL Loss = 0.0000, Lagrangian Loss = 56.4250\n",
      "Early stopping triggered. Last Epoch: 22\n",
      "Recon Loss = 283.3188, KL Loss = 0.0579, Lagrangian Loss = 57.5217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:09:11,740 INFO -- Edge Accuracy = 98.67, Instantaneous Accuracy = 89.33, Lagged Accuracy = 97.33, Counterfactual Accuracy = 61.33,  Blended Accuracy = 65.33,  RR Accuracy = 45.33  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 549.4829\n",
      "Recon Loss = 375.5226, KL Loss = 0.0000, Lagrangian Loss = 49.2799\n",
      "Early stopping triggered. Last Epoch: 34\n",
      "Recon Loss = 373.6573, KL Loss = 0.1478, Lagrangian Loss = 50.9185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:10:14,423 INFO -- Edge Accuracy = 98.68, Instantaneous Accuracy = 89.47, Lagged Accuracy = 97.37, Counterfactual Accuracy = 61.84,  Blended Accuracy = 65.79,  RR Accuracy = 46.05  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 456.0047\n",
      "Recon Loss = 328.6715, KL Loss = 0.0000, Lagrangian Loss = 42.8513\n",
      "Early stopping triggered. Last Epoch: 30\n",
      "Recon Loss = 333.8044, KL Loss = 0.1420, Lagrangian Loss = 43.8479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:11:12,582 INFO -- Edge Accuracy = 98.70, Instantaneous Accuracy = 89.61, Lagged Accuracy = 97.40, Counterfactual Accuracy = 61.04,  Blended Accuracy = 64.94,  RR Accuracy = 45.45  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 450.9995\n",
      "Recon Loss = 308.9011, KL Loss = 0.0000, Lagrangian Loss = 32.6826\n",
      "Early stopping triggered. Last Epoch: 40\n",
      "Recon Loss = 314.0178, KL Loss = 0.2099, Lagrangian Loss = 34.2526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:12:29,496 INFO -- Edge Accuracy = 98.72, Instantaneous Accuracy = 89.74, Lagged Accuracy = 97.44, Counterfactual Accuracy = 60.26,  Blended Accuracy = 64.10,  RR Accuracy = 44.87  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 628.6798\n",
      "Recon Loss = 445.3305, KL Loss = 0.0000, Lagrangian Loss = 19.1616\n",
      "Early stopping triggered. Last Epoch: 28\n",
      "Recon Loss = 462.0280, KL Loss = 0.0909, Lagrangian Loss = 19.5076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:13:22,593 INFO -- Edge Accuracy = 98.73, Instantaneous Accuracy = 89.87, Lagged Accuracy = 97.47, Counterfactual Accuracy = 60.76,  Blended Accuracy = 64.56,  RR Accuracy = 44.30  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 728.1488\n",
      "Recon Loss = 545.5403, KL Loss = 0.0000, Lagrangian Loss = 17.6431\n",
      "Early stopping triggered. Last Epoch: 33\n",
      "Recon Loss = 590.7711, KL Loss = 0.1059, Lagrangian Loss = 18.2840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:14:26,656 INFO -- Edge Accuracy = 98.75, Instantaneous Accuracy = 90.00, Lagged Accuracy = 97.50, Counterfactual Accuracy = 61.25,  Blended Accuracy = 65.00,  RR Accuracy = 45.00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 550.4279\n",
      "Recon Loss = 381.5872, KL Loss = 0.0000, Lagrangian Loss = 54.1067\n",
      "Early stopping triggered. Last Epoch: 32\n",
      "Recon Loss = 413.2181, KL Loss = 0.1488, Lagrangian Loss = 55.0212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:15:27,642 INFO -- Edge Accuracy = 98.77, Instantaneous Accuracy = 90.12, Lagged Accuracy = 97.53, Counterfactual Accuracy = 61.73,  Blended Accuracy = 65.43,  RR Accuracy = 45.68  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 600.4073\n",
      "Recon Loss = 419.3141, KL Loss = 0.0000, Lagrangian Loss = 43.8794\n",
      "Early stopping triggered. Last Epoch: 24\n",
      "Recon Loss = 444.9150, KL Loss = 0.1210, Lagrangian Loss = 44.4782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:16:13,451 INFO -- Edge Accuracy = 98.78, Instantaneous Accuracy = 90.24, Lagged Accuracy = 97.56, Counterfactual Accuracy = 62.20,  Blended Accuracy = 65.85,  RR Accuracy = 46.34  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 559.1038\n",
      "Recon Loss = 318.9182, KL Loss = 0.0000, Lagrangian Loss = 123.0319\n",
      "Early stopping triggered. Last Epoch: 27\n",
      "Recon Loss = 331.0604, KL Loss = 0.1387, Lagrangian Loss = 128.6401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:17:06,083 INFO -- Edge Accuracy = 98.80, Instantaneous Accuracy = 90.36, Lagged Accuracy = 97.59, Counterfactual Accuracy = 62.65,  Blended Accuracy = 66.27,  RR Accuracy = 45.78  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 519.2806\n",
      "Recon Loss = 323.7593, KL Loss = 0.0000, Lagrangian Loss = 74.3546\n",
      "Epoch 51: Loss = 514.1798\n",
      "Recon Loss = 312.6237, KL Loss = 0.1303, Lagrangian Loss = 78.7755\n",
      "Early stopping triggered. Last Epoch: 51\n",
      "Recon Loss = 345.3124, KL Loss = 0.1272, Lagrangian Loss = 79.4301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:18:42,207 INFO -- Edge Accuracy = 98.81, Instantaneous Accuracy = 90.48, Lagged Accuracy = 97.62, Counterfactual Accuracy = 63.10,  Blended Accuracy = 66.67,  RR Accuracy = 46.43  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 705.6585\n",
      "Recon Loss = 502.6906, KL Loss = 0.0000, Lagrangian Loss = 31.3439\n",
      "Early stopping triggered. Last Epoch: 44\n",
      "Recon Loss = 462.9668, KL Loss = 0.1474, Lagrangian Loss = 33.0872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:20:04,658 INFO -- Edge Accuracy = 98.82, Instantaneous Accuracy = 90.59, Lagged Accuracy = 97.65, Counterfactual Accuracy = 62.35,  Blended Accuracy = 65.88,  RR Accuracy = 45.88  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 664.7637\n",
      "Recon Loss = 433.9243, KL Loss = 0.0000, Lagrangian Loss = 28.1443\n",
      "Early stopping triggered. Last Epoch: 33\n",
      "Recon Loss = 459.3796, KL Loss = 0.1136, Lagrangian Loss = 29.8686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:21:05,887 INFO -- Edge Accuracy = 98.84, Instantaneous Accuracy = 90.70, Lagged Accuracy = 97.67, Counterfactual Accuracy = 62.79,  Blended Accuracy = 66.28,  RR Accuracy = 46.51  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 646.2260\n",
      "Recon Loss = 487.3109, KL Loss = 0.0000, Lagrangian Loss = 15.3174\n",
      "Early stopping triggered. Last Epoch: 20\n",
      "Recon Loss = 482.3143, KL Loss = 0.0752, Lagrangian Loss = 15.3431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:21:45,688 INFO -- Edge Accuracy = 98.85, Instantaneous Accuracy = 90.80, Lagged Accuracy = 97.70, Counterfactual Accuracy = 62.07,  Blended Accuracy = 65.52,  RR Accuracy = 45.98  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 410.1310\n",
      "Recon Loss = 275.4393, KL Loss = 0.0000, Lagrangian Loss = 45.8051\n",
      "Early stopping triggered. Last Epoch: 37\n",
      "Recon Loss = 297.0778, KL Loss = 0.1170, Lagrangian Loss = 47.7580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:22:53,733 INFO -- Edge Accuracy = 98.86, Instantaneous Accuracy = 90.91, Lagged Accuracy = 97.73, Counterfactual Accuracy = 62.50,  Blended Accuracy = 65.91,  RR Accuracy = 46.59  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 977.6728\n",
      "Recon Loss = 713.1372, KL Loss = 0.0000, Lagrangian Loss = 45.9253\n",
      "Early stopping triggered. Last Epoch: 37\n",
      "Recon Loss = 740.4094, KL Loss = 0.1619, Lagrangian Loss = 48.8793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:24:02,939 INFO -- Edge Accuracy = 98.88, Instantaneous Accuracy = 91.01, Lagged Accuracy = 97.75, Counterfactual Accuracy = 62.92,  Blended Accuracy = 66.29,  RR Accuracy = 46.07  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 397.5692\n",
      "Recon Loss = 285.4648, KL Loss = 0.0000, Lagrangian Loss = 28.5777\n",
      "Early stopping triggered. Last Epoch: 41\n",
      "Recon Loss = 266.0906, KL Loss = 0.2264, Lagrangian Loss = 29.8524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:25:23,285 INFO -- Edge Accuracy = 98.89, Instantaneous Accuracy = 91.11, Lagged Accuracy = 97.78, Counterfactual Accuracy = 62.22,  Blended Accuracy = 65.56,  RR Accuracy = 45.56  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 396.5988\n",
      "Recon Loss = 287.6496, KL Loss = 0.0000, Lagrangian Loss = 12.7383\n",
      "Early stopping triggered. Last Epoch: 20\n",
      "Recon Loss = 280.5601, KL Loss = 0.0526, Lagrangian Loss = 13.0554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:26:03,968 INFO -- Edge Accuracy = 98.90, Instantaneous Accuracy = 90.11, Lagged Accuracy = 97.80, Counterfactual Accuracy = 62.64,  Blended Accuracy = 65.93,  RR Accuracy = 46.15  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 685.7411\n",
      "Recon Loss = 527.6601, KL Loss = 0.0000, Lagrangian Loss = 13.9736\n",
      "Early stopping triggered. Last Epoch: 33\n",
      "Recon Loss = 473.6751, KL Loss = 0.1782, Lagrangian Loss = 14.1839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:27:07,119 INFO -- Edge Accuracy = 98.91, Instantaneous Accuracy = 89.13, Lagged Accuracy = 97.83, Counterfactual Accuracy = 63.04,  Blended Accuracy = 66.30,  RR Accuracy = 46.74  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 520.2110\n",
      "Recon Loss = 377.8938, KL Loss = 0.0000, Lagrangian Loss = 39.5514\n",
      "Early stopping triggered. Last Epoch: 36\n",
      "Recon Loss = 383.8396, KL Loss = 0.0898, Lagrangian Loss = 41.0970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:28:18,682 INFO -- Edge Accuracy = 98.92, Instantaneous Accuracy = 89.25, Lagged Accuracy = 97.85, Counterfactual Accuracy = 63.44,  Blended Accuracy = 66.67,  RR Accuracy = 47.31  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 791.2089\n",
      "Recon Loss = 574.8136, KL Loss = 0.0000, Lagrangian Loss = 24.7201\n",
      "Early stopping triggered. Last Epoch: 26\n",
      "Recon Loss = 592.9174, KL Loss = 0.1309, Lagrangian Loss = 25.5019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:29:09,938 INFO -- Edge Accuracy = 98.94, Instantaneous Accuracy = 88.30, Lagged Accuracy = 97.87, Counterfactual Accuracy = 63.83,  Blended Accuracy = 67.02,  RR Accuracy = 47.87  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 722.8752\n",
      "Recon Loss = 514.0704, KL Loss = 0.0000, Lagrangian Loss = 39.5263\n",
      "Early stopping triggered. Last Epoch: 35\n",
      "Recon Loss = 513.1834, KL Loss = 0.1207, Lagrangian Loss = 41.3170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:30:17,107 INFO -- Edge Accuracy = 98.95, Instantaneous Accuracy = 88.42, Lagged Accuracy = 97.89, Counterfactual Accuracy = 63.16,  Blended Accuracy = 67.37,  RR Accuracy = 47.37  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 701.0361\n",
      "Recon Loss = 542.2700, KL Loss = 0.0000, Lagrangian Loss = 42.1245\n",
      "Early stopping triggered. Last Epoch: 21\n",
      "Recon Loss = 466.8360, KL Loss = 0.1161, Lagrangian Loss = 42.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:30:57,874 INFO -- Edge Accuracy = 98.96, Instantaneous Accuracy = 88.54, Lagged Accuracy = 97.92, Counterfactual Accuracy = 63.54,  Blended Accuracy = 67.71,  RR Accuracy = 47.92  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 498.8139\n",
      "Recon Loss = 353.0919, KL Loss = 0.0000, Lagrangian Loss = 35.8278\n",
      "Epoch 51: Loss = 497.8549\n",
      "Recon Loss = 315.7294, KL Loss = 0.3226, Lagrangian Loss = 37.4623\n",
      "Early stopping triggered. Last Epoch: 58\n",
      "Recon Loss = 328.4999, KL Loss = 0.3180, Lagrangian Loss = 37.9204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:32:45,021 INFO -- Edge Accuracy = 98.97, Instantaneous Accuracy = 88.66, Lagged Accuracy = 97.94, Counterfactual Accuracy = 62.89,  Blended Accuracy = 67.01,  RR Accuracy = 47.42  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 459.6170\n",
      "Recon Loss = 324.8984, KL Loss = 0.0000, Lagrangian Loss = 37.3433\n",
      "Early stopping triggered. Last Epoch: 40\n",
      "Recon Loss = 314.9126, KL Loss = 0.2582, Lagrangian Loss = 39.3781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:33:59,228 INFO -- Edge Accuracy = 98.98, Instantaneous Accuracy = 88.78, Lagged Accuracy = 97.96, Counterfactual Accuracy = 62.24,  Blended Accuracy = 66.33,  RR Accuracy = 46.94  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 386.0467\n",
      "Recon Loss = 265.2920, KL Loss = 0.0000, Lagrangian Loss = 26.3052\n",
      "Early stopping triggered. Last Epoch: 37\n",
      "Recon Loss = 255.5908, KL Loss = 0.2465, Lagrangian Loss = 27.5172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:35:10,441 INFO -- Edge Accuracy = 98.99, Instantaneous Accuracy = 88.89, Lagged Accuracy = 97.98, Counterfactual Accuracy = 61.62,  Blended Accuracy = 65.66,  RR Accuracy = 46.46  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 567.9746\n",
      "Recon Loss = 389.1447, KL Loss = 0.0000, Lagrangian Loss = 52.5766\n",
      "Early stopping triggered. Last Epoch: 42\n",
      "Recon Loss = 369.8848, KL Loss = 0.1654, Lagrangian Loss = 54.9571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:36:33,396 INFO -- Edge Accuracy = 99.00, Instantaneous Accuracy = 89.00, Lagged Accuracy = 98.00, Counterfactual Accuracy = 62.00,  Blended Accuracy = 66.00,  RR Accuracy = 47.00  \n"
     ]
    }
   ],
   "source": [
    "TIME_STEPS = 7\n",
    "BATCH_SIZE = 25\n",
    "hidden_dim = 64\n",
    "latent_dim = 8\n",
    "edge_correct = 0\n",
    "instantaneous_correct = 0\n",
    "lagged_correct = 0\n",
    "counterfactual_correct = 0 \n",
    "rr_correct = 0\n",
    "total_correct = 0\n",
    "total_checked = 0\n",
    "for i in range(100):\n",
    "    total_checked+=1\n",
    "\n",
    "    dataset_bad = TimeSeriesDataset(bad, device=device, time_steps=TIME_STEPS)\n",
    "    dataloader_bad = DataLoader(dataset_bad, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    fine_tuned = CausalGraphVAE(input_dim=bad.shape[1], hidden_dim=hidden_dim,\n",
    "                            latent_dim=latent_dim, num_nodes=bad.shape[1],device=device,\n",
    "                            time_steps=TIME_STEPS, prior_adj=prior_adj.to(device),instantaneous_weight=0.1).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    \n",
    "    # Train on nominal data\n",
    "    # print(\"Pretraining on nominal data...\")\n",
    "    fine_tuned.train_model(dataloader_bad, optimizer, num_epochs=200, patience=20,\n",
    "                           BATCH_SIZE=BATCH_SIZE,rho_max=4,alpha_max=2)\n",
    "    \n",
    "    X_data = torch.empty(0,device=device)\n",
    "    T_data = torch.empty(0,device=device)\n",
    "    for batch_idx, (X_batch, time_batch) in enumerate(dataloader_bad):\n",
    "        X_data = torch.cat((X_data[:batch_idx], X_batch, X_data[batch_idx:]))\n",
    "        T_data = torch.cat((T_data[:batch_idx], time_batch, T_data[batch_idx:]))\n",
    "    \n",
    "    causes= fine_tuned.infer_causal_effect(X_data,T_data,'x_4',cols,non_causal_indices=[])\n",
    "    \n",
    "    edge_cause_1 = causes.sort_values(by='causes',ascending=False)[0:3].index[0]\n",
    "    edge_cause_2 = causes.sort_values(by='causes',ascending=False)[0:3].index[1]\n",
    "    edge_cause_3 = causes.sort_values(by='causes',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    instant_cause_1 = causes.sort_values(by='instantaneous',ascending=False)[0:3].index[0]\n",
    "    instant_cause_2 = causes.sort_values(by='instantaneous',ascending=False)[0:3].index[1]\n",
    "    instant_cause_3 = causes.sort_values(by='instantaneous',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    lag_cause_1 = causes.sort_values(by='lagged',ascending=False)[0:3].index[0]\n",
    "    lag_cause_2 = causes.sort_values(by='lagged',ascending=False)[0:3].index[1]\n",
    "    lag_cause_3 = causes.sort_values(by='lagged',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    counterfactual_cause_1 = causes.sort_values(by='counterfactuals',ascending=False)[0:3].index[0]\n",
    "    counterfactual_cause_2 = causes.sort_values(by='counterfactuals',ascending=False)[0:3].index[1]\n",
    "    counterfactual_cause_3 = causes.sort_values(by='counterfactuals',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    rr_cause_1 = causes.sort_values(by='RootRank',ascending=False)[0:3].index[0]\n",
    "    rr_cause_2 = causes.sort_values(by='RootRank',ascending=False)[0:3].index[1]\n",
    "    rr_cause_3 = causes.sort_values(by='RootRank',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    total_score_cause_1=causes.sort_values(by='causal_strength',ascending=False)[0:3].index[0]\n",
    "    total_score_cause_2=causes.sort_values(by='causal_strength',ascending=False)[0:3].index[1]\n",
    "    total_score_cause_3=causes.sort_values(by='causal_strength',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    if (edge_cause_1 in ['x_1','x_44']) | (edge_cause_2 in ['x_1','x_44']) | (edge_cause_3 in ['x_1','x_44']):\n",
    "        edge_correct+=1\n",
    "    \n",
    "    if (  total_score_cause_1 in ['x_1','x_44']) | (total_score_cause_2 in ['x_1','x_44']) | ( total_score_cause_3 in ['x_1','x_44']):\n",
    "        total_correct+=1\n",
    "    \n",
    "    if (counterfactual_cause_1 in ['x_1','x_44']) | (counterfactual_cause_2 in ['x_1','x_44']) | (counterfactual_cause_3 in ['x_1','x_44']):\n",
    "        counterfactual_correct+=1\n",
    "    \n",
    "    if (instant_cause_1 in ['x_1','x_44']) | (instant_cause_2 in ['x_1','x_44']) |( instant_cause_3 in ['x_1','x_44'] ):\n",
    "        instantaneous_correct+=1\n",
    "    \n",
    "    if (lag_cause_1 in ['x_1','x_44']) | (lag_cause_2 in ['x_1','x_44'])  | (lag_cause_3 in ['x_1','x_44']):\n",
    "        lagged_correct+=1\n",
    "    \n",
    "    if (rr_cause_1 in ['x_1','x_44']) | (rr_cause_2 in ['x_1','x_44']) | (rr_cause_3 in ['x_1','x_44']):\n",
    "        rr_correct+=1\n",
    "        \n",
    "    total_accuracy = total_correct/total_checked* 100\n",
    "    edge_accuracy = edge_correct/total_checked* 100\n",
    "    cf_accuracy = counterfactual_correct/total_checked* 100\n",
    "    instant_accuracy = instantaneous_correct/total_checked* 100\n",
    "    lag_accuracy = lagged_correct/total_checked* 100\n",
    "    rr_accuracy = rr_correct/total_checked* 100\n",
    "    \n",
    "    \n",
    "    logger.info(f\"Edge Accuracy = {edge_accuracy:.2f}, Instantaneous Accuracy = {instant_accuracy:.2f}, Lagged Accuracy = {lag_accuracy:.2f}, Counterfactual Accuracy = {cf_accuracy:.2f},  Blended Accuracy = {total_accuracy:.2f},  RR Accuracy = {rr_accuracy:.2f}  \") \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2083fb-9501-47ad-92d3-7f0fd528e049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
