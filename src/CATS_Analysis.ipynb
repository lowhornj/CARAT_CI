{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a97d3321-4427-4837-8269-048dc64686f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from torch_geometric.nn import GATv2Conv, GATConv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch.distributions import Normal, Laplace, RelaxedOneHotCategorical\n",
    "from torchdiffeq import odeint  # For continuous-time normalizing flows\n",
    "\n",
    "from feature.scalers import ranged_scaler\n",
    "from feature.engineering import *\n",
    "from CARAT.model_utils import *\n",
    "from CARAT.model import CausalGraphVAE\n",
    "from CARAT.components import *\n",
    "from utils.utils import set_seed, logger\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define the seed value\n",
    "seed = 42\n",
    "\n",
    "# Set seed for PyTorch\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Set seed for CUDA (if using GPUs)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "\n",
    "# Set seed for Python's random module\n",
    "random.seed(seed)\n",
    "\n",
    "# Set seed for NumPy\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Ensure deterministic behavior for PyTorch operations\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Set device\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device( \"cpu\")\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6981e0a1-e1c0-409d-ab6a-cf7d34bf1793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000000, 20)\n"
     ]
    }
   ],
   "source": [
    "cats_df = pl.read_csv(\"data/data.csv\", separator=\",\")  \n",
    "print(cats_df.shape)\n",
    "metadata = pl.read_csv('data/metadata.csv',separator=',')\n",
    "potential_causes = metadata['root_cause'].unique().to_list()\n",
    "\n",
    "for col in cats_df.columns:\n",
    "    unique_vals = cats_df[col].n_unique()\n",
    "    data_type = cats_df[col].dtype\n",
    "    bad_dtypes = [pl.Date,pl.Datetime,pl.Utf8]\n",
    "    if ((unique_vals >= 50) & (data_type not in bad_dtypes) ):\n",
    "        cats_df = cats_df.with_columns(ranged_scaler(cats_df[col]))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df0f5a4d-1cf3-4a87-a3fb-751607296c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_df = cats_df.with_columns(\n",
    "    pl.col('timestamp').str.to_datetime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    pl.Series(\"entity_id\",range(len(cats_df)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48b9af15-9315-41d4-86fe-907430746a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000000, 21)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_rows_list = metadata.rows(named=True)\n",
    "cats_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52959448-4391-42ad-a59e-e0f732b1971f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping column timestamp (not a float column)\n",
      "Column: aimp | Final ADF: -2211.6294 | p-value: 0.0000 | Diffs: 0\n",
      "Column: amud | Final ADF: -8.0091 | p-value: 0.0000 | Diffs: 0\n",
      "Column: arnd | Final ADF: -18.1248 | p-value: 0.0000 | Diffs: 0\n",
      "Column: asin1 | Final ADF: -5.7565 | p-value: 0.0000 | Diffs: 1\n",
      "Column: asin2 | Final ADF: -39.8976 | p-value: 0.0000 | Diffs: 3\n",
      "Column: adbr | Final ADF: -111.5305 | p-value: 0.0000 | Diffs: 0\n",
      "Column: adfl | Final ADF: -217.2031 | p-value: 0.0000 | Diffs: 0\n",
      "Column: bed1 | Final ADF: -35.0214 | p-value: 0.0000 | Diffs: 0\n",
      "Column: bed2 | Final ADF: -44.5328 | p-value: 0.0000 | Diffs: 0\n",
      "Column: bfo1 | Final ADF: -8.5614 | p-value: 0.0000 | Diffs: 0\n",
      "Column: bfo2 | Final ADF: -13.3015 | p-value: 0.0000 | Diffs: 0\n",
      "Column: bso1 | Final ADF: -2164.1026 | p-value: 0.0000 | Diffs: 1\n",
      "Column: bso2 | Final ADF: -13.5707 | p-value: 0.0000 | Diffs: 0\n",
      "Column: bso3 | Final ADF: -14.7270 | p-value: 0.0000 | Diffs: 0\n",
      "Column: ced1 | Final ADF: -727.3289 | p-value: 0.0000 | Diffs: 1\n",
      "Column: cfo1 | Final ADF: -6.7640 | p-value: 0.0000 | Diffs: 0\n",
      "Column: cso1 | Final ADF: -7.8526 | p-value: 0.0000 | Diffs: 0\n",
      "Column: y | Final ADF: -51.2989 | p-value: 0.0000 | Diffs: 0\n",
      "Column: category | Final ADF: -53.9463 | p-value: 0.0000 | Diffs: 0\n",
      "⚠️ Skipping column entity_id (not a float column)\n"
     ]
    }
   ],
   "source": [
    "cats_df = make_stationary(cats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6b4cd0e-cacc-4d93-b0e9-a0cdca28b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_df = cats_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e03dcda-4108-4fe2-9ec3-8ccbdeaf85f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['cfo1']\", \"['ced1']\", \"['cso1']\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata['affected'].unique().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc297da3-97f8-4077-898e-cc23b8c2e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_causes = metadata['root_cause'].unique().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8be12f8-8ca1-49a7-81aa-5e05325023a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aimp</th>\n",
       "      <th>amud</th>\n",
       "      <th>arnd</th>\n",
       "      <th>asin1</th>\n",
       "      <th>asin2</th>\n",
       "      <th>adbr</th>\n",
       "      <th>adfl</th>\n",
       "      <th>bed1</th>\n",
       "      <th>bed2</th>\n",
       "      <th>bfo1</th>\n",
       "      <th>bfo2</th>\n",
       "      <th>bso1</th>\n",
       "      <th>bso2</th>\n",
       "      <th>bso3</th>\n",
       "      <th>ced1</th>\n",
       "      <th>cfo1</th>\n",
       "      <th>cso1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:00:06</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.444480</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>-7.999823e-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.32802</td>\n",
       "      <td>-0.369237</td>\n",
       "      <td>-0.738163</td>\n",
       "      <td>-0.767181</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>-0.507953</td>\n",
       "      <td>-0.716059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100401</td>\n",
       "      <td>-0.186461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:00:07</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.446078</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>-7.999823e-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.32802</td>\n",
       "      <td>-0.369237</td>\n",
       "      <td>-0.738163</td>\n",
       "      <td>-0.767181</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>-0.507953</td>\n",
       "      <td>-0.716059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100408</td>\n",
       "      <td>-0.186406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:00:08</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.447166</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>-8.000267e-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.32802</td>\n",
       "      <td>-0.369237</td>\n",
       "      <td>-0.738163</td>\n",
       "      <td>-0.767181</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>-0.507953</td>\n",
       "      <td>-0.716059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100416</td>\n",
       "      <td>-0.186345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:00:09</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.442843</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>-7.999823e-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.32802</td>\n",
       "      <td>-0.369237</td>\n",
       "      <td>-0.738163</td>\n",
       "      <td>-0.767181</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>-0.507953</td>\n",
       "      <td>-0.716059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100426</td>\n",
       "      <td>-0.186278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:00:10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.441320</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>-8.000045e-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.32802</td>\n",
       "      <td>-0.369237</td>\n",
       "      <td>-0.738163</td>\n",
       "      <td>-0.767181</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-0.507953</td>\n",
       "      <td>-0.716059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100438</td>\n",
       "      <td>-0.186205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     aimp      amud      arnd    asin1         asin2  adbr  \\\n",
       "timestamp                                                                    \n",
       "2023-01-01 00:00:06   0.0  0.142857 -0.444480  0.00002 -7.999823e-12   0.0   \n",
       "2023-01-01 00:00:07   0.0  0.142857 -0.446078  0.00002 -7.999823e-12   0.0   \n",
       "2023-01-01 00:00:08   0.0  0.142857 -0.447166  0.00002 -8.000267e-12   0.0   \n",
       "2023-01-01 00:00:09   0.0  0.142857 -0.442843  0.00002 -7.999823e-12   0.0   \n",
       "2023-01-01 00:00:10   0.0  0.142857 -0.441320  0.00002 -8.000045e-12   0.0   \n",
       "\n",
       "                     adfl     bed1      bed2      bfo1      bfo2      bso1  \\\n",
       "timestamp                                                                    \n",
       "2023-01-01 00:00:06   0.0 -0.32802 -0.369237 -0.738163 -0.767181  0.000072   \n",
       "2023-01-01 00:00:07   0.0 -0.32802 -0.369237 -0.738163 -0.767181  0.000083   \n",
       "2023-01-01 00:00:08   0.0 -0.32802 -0.369237 -0.738163 -0.767181  0.000094   \n",
       "2023-01-01 00:00:09   0.0 -0.32802 -0.369237 -0.738163 -0.767181  0.000104   \n",
       "2023-01-01 00:00:10   0.0 -0.32802 -0.369237 -0.738163 -0.767181  0.000113   \n",
       "\n",
       "                         bso2      bso3  ced1      cfo1      cso1  \n",
       "timestamp                                                          \n",
       "2023-01-01 00:00:06 -0.507953 -0.716059   0.0  0.100401 -0.186461  \n",
       "2023-01-01 00:00:07 -0.507953 -0.716059   0.0  0.100408 -0.186406  \n",
       "2023-01-01 00:00:08 -0.507953 -0.716059   0.0  0.100416 -0.186345  \n",
       "2023-01-01 00:00:09 -0.507953 -0.716059   0.0  0.100426 -0.186278  \n",
       "2023-01-01 00:00:10 -0.507953 -0.716059   0.0  0.100438 -0.186205  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_df=cats_df.set_index('timestamp')\n",
    "cats_df = cats_df.drop(['y','category','entity_id'],axis=1)\n",
    "cats_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c816fec8-c143-4f7e-a78c-1b1b18f32a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4999994, 17)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6b15ba7-0bab-44e8-95aa-c6b534da9748",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = cats_df[0:1000000]\n",
    "test_df = cats_df[1000000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7eaa704-a473-421e-acaf-0aff6efe4fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab4ff270-6892-43e6-8571-b623acdd105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_df = train_df.drop('time',axis=1)\n",
    "except:\n",
    "    None\n",
    "try:\n",
    "    test_df = test_df.drop('time',axis=1)\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfc22130-f5dd-4ef8-8d1b-e29c4f26b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(test_df.columns)\n",
    "non_causal_columns = list(set(cols).difference(set(potential_causes)))\n",
    "causal_indices = [train_df.columns.get_loc(col) for col in potential_causes]\n",
    "non_causal_indices = [train_df.columns.get_loc(col) for col in non_causal_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3198cc09-56e4-4f5b-8f20-aea6df75647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define the seed value\n",
    "seed = 42\n",
    "\n",
    "# Set seed for PyTorch\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Set seed for CUDA (if using GPUs)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "\n",
    "# Set seed for Python's random module\n",
    "random.seed(seed)\n",
    "\n",
    "# Set seed for NumPy\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Ensure deterministic behavior for PyTorch operations\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eebb432e-a758-4045-ad93-c5ad7cb19bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining on nominal data...\n",
      "Epoch 1: Loss = 184355.7912\n",
      "Recon Loss = 50394.9375, KL Loss = 0.0000, Lagrangian Loss = 19456.3281\n",
      "Epoch 51: Loss = 749.3755\n",
      "Recon Loss = 717.3883, KL Loss = 3.6515, Lagrangian Loss = 13.9407\n",
      "Epoch 101: Loss = 283.9353\n",
      "Recon Loss = 281.1418, KL Loss = 5.7118, Lagrangian Loss = 8.6988\n",
      "Epoch 151: Loss = 205.1650\n",
      "Recon Loss = 176.3573, KL Loss = 5.5914, Lagrangian Loss = 10.7432\n",
      "Epoch 201: Loss = 184.4385\n",
      "Recon Loss = 186.6451, KL Loss = 5.4285, Lagrangian Loss = 13.7190\n"
     ]
    }
   ],
   "source": [
    "TIME_STEPS = 15\n",
    "BATCH_SIZE = 10000\n",
    "hidden_dim = 64\n",
    "latent_dim = 16\n",
    "num_nodes = 17\n",
    "\n",
    "dataset_nominal = TimeSeriesDataset(train_df, device=device, time_steps=TIME_STEPS)\n",
    "dataloader_nominal = DataLoader(dataset_nominal, batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = CausalGraphVAE(input_dim=train_df.shape[1], hidden_dim=hidden_dim,\n",
    "                        latent_dim=latent_dim, num_nodes=train_df.shape[1],device=device,\n",
    "                        time_steps=TIME_STEPS, prior_adj=None,instantaneous_weight=0.4)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Train on nominal data\n",
    "print(\"Pretraining on nominal data...\")\n",
    "model.train_model(dataloader_nominal, optimizer, num_epochs=250, patience=25,rho_max=3,alpha_max=1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62199d32-95e0-464f-8a31-d074cc60fff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277,\n",
       "         0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277],\n",
       "        [0.2277, 0.0000, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277,\n",
       "         0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277],\n",
       "        [0.2277, 0.2277, 0.0000, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277,\n",
       "         0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277],\n",
       "        [0.2277, 0.2277, 0.2277, 0.0000, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277,\n",
       "         0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277],\n",
       "        [0.2277, 0.2277, 0.2277, 0.2277, 0.0000, 0.2277, 0.2277, 0.2277, 0.2277,\n",
       "         0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277],\n",
       "        [0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.0000, 0.2277, 0.2277, 0.2277,\n",
       "         0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277],\n",
       "        [0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.0000, 0.2277, 0.2277,\n",
       "         0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277],\n",
       "        [0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.0000, 0.2277,\n",
       "         0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277],\n",
       "        [0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.0000,\n",
       "         0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277],\n",
       "        [0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277,\n",
       "         0.0000, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277],\n",
       "        [0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277,\n",
       "         0.2277, 0.0000, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277],\n",
       "        [0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277,\n",
       "         0.2277, 0.2277, 0.0000, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277],\n",
       "        [0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277,\n",
       "         0.2277, 0.2277, 0.2277, 0.0000, 0.2277, 0.2277, 0.2277, 0.2277],\n",
       "        [0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277,\n",
       "         0.2277, 0.2277, 0.2277, 0.2277, 0.0000, 0.2277, 0.2277, 0.2277],\n",
       "        [0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277,\n",
       "         0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.0000, 0.2277, 0.2277],\n",
       "        [0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277,\n",
       "         0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.0000, 0.2277],\n",
       "        [0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277,\n",
       "         0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.2277, 0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_adj = model.causal_graph.adj_mat.clone().detach()\n",
    "prior_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d35de27-37ce-4711-9b1f-c052549cdbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_adj = model.causal_graph.adj_mat.clone().detach()\n",
    "#scaled_prior = scale_tensor(prior_adj)\n",
    "for i, row in enumerate(prior_adj):\n",
    "    for j, column in enumerate(row):\n",
    "        if (j in non_causal_indices) and (i in causal_indices) & (i!=j):\n",
    "            continue\n",
    "        else:\n",
    "            prior_adj[i,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1969b33-7fd1-471c-b21a-3b6c42c157ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aimp</th>\n",
       "      <th>amud</th>\n",
       "      <th>arnd</th>\n",
       "      <th>asin1</th>\n",
       "      <th>asin2</th>\n",
       "      <th>adbr</th>\n",
       "      <th>adfl</th>\n",
       "      <th>bed1</th>\n",
       "      <th>bed2</th>\n",
       "      <th>bfo1</th>\n",
       "      <th>bfo2</th>\n",
       "      <th>bso1</th>\n",
       "      <th>bso2</th>\n",
       "      <th>bso3</th>\n",
       "      <th>ced1</th>\n",
       "      <th>cfo1</th>\n",
       "      <th>cso1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aimp</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amud</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arnd</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asin1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asin2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adbr</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adfl</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bed1</th>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bed2</th>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bfo1</th>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bfo2</th>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bso1</th>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bso2</th>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bso3</th>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>0.227679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ced1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cfo1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cso1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           aimp      amud      arnd     asin1     asin2      adbr      adfl  \\\n",
       "aimp   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "amud   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "arnd   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "asin1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "asin2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "adbr   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "adfl   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "bed1   0.227679  0.227679  0.227679  0.227679  0.227679  0.227679  0.227679   \n",
       "bed2   0.227679  0.227679  0.227679  0.227679  0.227679  0.227679  0.227679   \n",
       "bfo1   0.227679  0.227679  0.227679  0.227679  0.227679  0.227679  0.227679   \n",
       "bfo2   0.227679  0.227679  0.227679  0.227679  0.227679  0.227679  0.227679   \n",
       "bso1   0.227679  0.227679  0.227679  0.227679  0.227679  0.227679  0.227679   \n",
       "bso2   0.227679  0.227679  0.227679  0.227679  0.227679  0.227679  0.227679   \n",
       "bso3   0.227679  0.227679  0.227679  0.227679  0.227679  0.227679  0.227679   \n",
       "ced1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "cfo1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "cso1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       bed1  bed2  bfo1  bfo2  bso1  bso2  bso3      ced1      cfo1      cso1  \n",
       "aimp    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "amud    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "arnd    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "asin1   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "asin2   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "adbr    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "adfl    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "bed1    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.227679  0.227679  0.227679  \n",
       "bed2    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.227679  0.227679  0.227679  \n",
       "bfo1    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.227679  0.227679  0.227679  \n",
       "bfo2    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.227679  0.227679  0.227679  \n",
       "bso1    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.227679  0.227679  0.227679  \n",
       "bso2    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.227679  0.227679  0.227679  \n",
       "bso3    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.227679  0.227679  0.227679  \n",
       "ced1    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "cfo1    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  \n",
       "cso1    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(prior_adj.cpu().detach().numpy(),index=cols,columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7d362f-6a8a-4a3c-b03f-3839ca30f4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 15:59:35,941 INFO -- Model: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly: cfo1\n",
      "Epoch 1: Loss = 52125.9680\n",
      "Recon Loss = 2567.2012, KL Loss = 0.0000, Lagrangian Loss = 54007.7461\n",
      "Epoch 51: Loss = 27180.5833\n",
      "Recon Loss = 180.6280, KL Loss = 0.0594, Lagrangian Loss = 29511.3438\n",
      "Epoch 101: Loss = 13462.0625\n",
      "Recon Loss = 99.3084, KL Loss = 0.1562, Lagrangian Loss = 14516.2598\n",
      "Epoch 151: Loss = 5184.7607\n",
      "Recon Loss = 105.9923, KL Loss = 0.2191, Lagrangian Loss = 5563.6338\n",
      "Epoch 201: Loss = 2618.2565\n",
      "Recon Loss = 63.0276, KL Loss = 0.3447, Lagrangian Loss = 2798.8047\n",
      "Epoch 251: Loss = 1521.6651\n",
      "Recon Loss = 61.2843, KL Loss = 0.4547, Lagrangian Loss = 1594.9978\n",
      "Epoch 301: Loss = 971.8837\n",
      "Recon Loss = 59.0003, KL Loss = 0.5661, Lagrangian Loss = 995.8981\n",
      "Epoch 351: Loss = 659.2572\n",
      "Recon Loss = 55.3028, KL Loss = 0.6419, Lagrangian Loss = 660.7437\n",
      "Epoch 401: Loss = 472.5373\n",
      "Recon Loss = 52.8800, KL Loss = 0.6873, Lagrangian Loss = 466.2163\n",
      "Epoch 451: Loss = 346.8952\n",
      "Recon Loss = 42.3673, KL Loss = 0.7178, Lagrangian Loss = 331.1893\n",
      "Epoch 501: Loss = 266.3982\n",
      "Recon Loss = 36.2033, KL Loss = 0.7427, Lagrangian Loss = 249.1686\n",
      "Epoch 551: Loss = 208.2825\n",
      "Recon Loss = 36.5745, KL Loss = 0.7164, Lagrangian Loss = 193.7726\n",
      "Epoch 601: Loss = 165.6443\n",
      "Recon Loss = 43.6517, KL Loss = 0.6950, Lagrangian Loss = 151.1123\n",
      "Epoch 651: Loss = 134.5839\n",
      "Recon Loss = 25.9473, KL Loss = 0.6648, Lagrangian Loss = 120.7036\n",
      "Epoch 701: Loss = 111.2484\n",
      "Recon Loss = 29.3948, KL Loss = 0.6149, Lagrangian Loss = 96.3736\n",
      "Epoch 751: Loss = 93.0056\n",
      "Recon Loss = 19.6291, KL Loss = 0.6364, Lagrangian Loss = 78.5984\n",
      "Epoch 801: Loss = 78.4525\n",
      "Recon Loss = 24.2039, KL Loss = 0.5625, Lagrangian Loss = 65.5949\n",
      "Epoch 851: Loss = 67.5562\n",
      "Recon Loss = 20.8351, KL Loss = 0.5236, Lagrangian Loss = 54.5643\n",
      "Epoch 901: Loss = 58.6766\n",
      "Recon Loss = 14.4730, KL Loss = 0.4718, Lagrangian Loss = 46.4545\n",
      "Epoch 951: Loss = 52.5516\n",
      "Recon Loss = 12.6368, KL Loss = 0.4397, Lagrangian Loss = 39.8855\n",
      "Epoch 1001: Loss = 46.3147\n",
      "Recon Loss = 16.1038, KL Loss = 0.4289, Lagrangian Loss = 34.5577\n",
      "Epoch 1051: Loss = 42.4189\n",
      "Recon Loss = 16.0347, KL Loss = 0.3996, Lagrangian Loss = 31.3597\n",
      "Epoch 1101: Loss = 38.7416\n",
      "Recon Loss = 17.0470, KL Loss = 0.3495, Lagrangian Loss = 28.0858\n",
      "Epoch 1151: Loss = 36.5770\n",
      "Recon Loss = 17.5796, KL Loss = 0.3450, Lagrangian Loss = 25.4292\n",
      "Epoch 1201: Loss = 33.7363\n",
      "Recon Loss = 18.0142, KL Loss = 0.3553, Lagrangian Loss = 23.4452\n",
      "Epoch 1251: Loss = 31.4854\n",
      "Recon Loss = 11.9533, KL Loss = 0.3637, Lagrangian Loss = 21.8479\n",
      "Epoch 1301: Loss = 29.8279\n",
      "Recon Loss = 13.1216, KL Loss = 0.3817, Lagrangian Loss = 20.8591\n",
      "Epoch 1351: Loss = 27.8665\n",
      "Recon Loss = 8.1909, KL Loss = 0.3603, Lagrangian Loss = 19.6115\n",
      "Epoch 1401: Loss = 27.3459\n",
      "Recon Loss = 12.7863, KL Loss = 0.3307, Lagrangian Loss = 19.1107\n",
      "Epoch 1451: Loss = 26.3665\n",
      "Recon Loss = 7.1528, KL Loss = 0.3437, Lagrangian Loss = 18.6546\n",
      "Epoch 1501: Loss = 25.9422\n",
      "Recon Loss = 9.9811, KL Loss = 0.3583, Lagrangian Loss = 18.4255\n",
      "Early stopping triggered. Last Epoch: 1519\n",
      "Recon Loss = 8.1014, KL Loss = 0.2940, Lagrangian Loss = 18.3619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 18:27:26,672 INFO -- Edge Accuracy = 100.00, Instantaneous Accuracy = 100.00, Lagged Accuracy = 0.00, Counterfactual Accuracy = 0.00,  Blended Accuracy = 100.00,  RR Accuracy = 0.00  \n",
      "2025-03-27 18:27:26,672 INFO -- Model: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly: cfo1\n",
      "Epoch 1: Loss = 56903.4561\n",
      "Recon Loss = 549.4895, KL Loss = 0.0000, Lagrangian Loss = 50390.7344\n",
      "Epoch 51: Loss = 479.4792\n",
      "Recon Loss = 48.0876, KL Loss = 0.0964, Lagrangian Loss = 413.0755\n",
      "Epoch 101: Loss = 85.9704\n",
      "Recon Loss = 42.6763, KL Loss = 0.2127, Lagrangian Loss = 46.8242\n",
      "Epoch 151: Loss = 29.6169\n",
      "Recon Loss = 16.6747, KL Loss = 0.3053, Lagrangian Loss = 14.7006\n",
      "Epoch 201: Loss = 19.7449\n",
      "Recon Loss = 7.5022, KL Loss = 0.3045, Lagrangian Loss = 10.9338\n",
      "Epoch 251: Loss = 15.0762\n",
      "Recon Loss = 4.5496, KL Loss = 0.2922, Lagrangian Loss = 10.7476\n",
      "Epoch 301: Loss = 14.2192\n",
      "Recon Loss = 3.6823, KL Loss = 0.2518, Lagrangian Loss = 10.8021\n",
      "Epoch 351: Loss = 13.9130\n",
      "Recon Loss = 3.9199, KL Loss = 0.2520, Lagrangian Loss = 10.8803\n",
      "Early stopping triggered. Last Epoch: 398\n",
      "Recon Loss = 2.0534, KL Loss = 0.1879, Lagrangian Loss = 10.9675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 01:02:37,938 INFO -- Edge Accuracy = 50.00, Instantaneous Accuracy = 50.00, Lagged Accuracy = 0.00, Counterfactual Accuracy = 50.00,  Blended Accuracy = 100.00,  RR Accuracy = 0.00  \n",
      "2025-03-28 01:02:37,939 INFO -- Model: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly: cso1\n",
      "Epoch 1: Loss = 54001.8270\n",
      "Recon Loss = 1605.6938, KL Loss = 0.0000, Lagrangian Loss = 52950.0469\n",
      "Epoch 51: Loss = 9126.7521\n",
      "Recon Loss = 179.4910, KL Loss = 0.0467, Lagrangian Loss = 9220.9893\n",
      "Epoch 101: Loss = 2367.8082\n",
      "Recon Loss = 109.7033, KL Loss = 0.1702, Lagrangian Loss = 2378.1079\n",
      "Epoch 151: Loss = 946.7933\n",
      "Recon Loss = 126.2305, KL Loss = 0.2813, Lagrangian Loss = 851.8746\n",
      "Epoch 201: Loss = 511.4558\n",
      "Recon Loss = 105.8886, KL Loss = 0.4381, Lagrangian Loss = 426.8864\n",
      "Epoch 251: Loss = 311.8470\n",
      "Recon Loss = 86.5833, KL Loss = 0.5592, Lagrangian Loss = 239.9332\n",
      "Epoch 301: Loss = 202.7741\n",
      "Recon Loss = 64.6303, KL Loss = 0.6268, Lagrangian Loss = 146.8194\n",
      "Epoch 351: Loss = 142.4176\n",
      "Recon Loss = 46.9491, KL Loss = 0.6245, Lagrangian Loss = 94.3344\n",
      "Epoch 401: Loss = 104.5419\n",
      "Recon Loss = 43.6413, KL Loss = 0.5828, Lagrangian Loss = 64.0704\n",
      "Epoch 451: Loss = 79.8141\n",
      "Recon Loss = 37.0138, KL Loss = 0.6021, Lagrangian Loss = 44.4538\n",
      "Epoch 501: Loss = 59.5836\n",
      "Recon Loss = 31.3602, KL Loss = 0.6610, Lagrangian Loss = 33.3350\n",
      "Epoch 551: Loss = 48.7282\n",
      "Recon Loss = 24.5818, KL Loss = 0.7587, Lagrangian Loss = 26.2677\n",
      "Epoch 601: Loss = 38.9472\n",
      "Recon Loss = 18.0049, KL Loss = 0.7976, Lagrangian Loss = 20.7974\n",
      "Epoch 651: Loss = 33.6618\n",
      "Recon Loss = 15.0298, KL Loss = 0.8500, Lagrangian Loss = 17.7192\n",
      "Epoch 701: Loss = 29.6826\n",
      "Recon Loss = 8.0122, KL Loss = 0.8350, Lagrangian Loss = 15.7119\n",
      "Epoch 751: Loss = 27.9257\n",
      "Recon Loss = 13.9273, KL Loss = 0.8776, Lagrangian Loss = 14.4310\n",
      "Epoch 801: Loss = 26.1578\n",
      "Recon Loss = 19.0822, KL Loss = 0.8415, Lagrangian Loss = 13.6343\n",
      "Epoch 851: Loss = 24.6569\n",
      "Recon Loss = 12.7258, KL Loss = 0.8094, Lagrangian Loss = 13.2071\n",
      "Epoch 901: Loss = 23.5425\n",
      "Recon Loss = 6.7961, KL Loss = 0.7595, Lagrangian Loss = 13.0784\n",
      "Epoch 951: Loss = 22.8513\n",
      "Recon Loss = 8.4723, KL Loss = 0.7210, Lagrangian Loss = 13.0841\n",
      "Epoch 1001: Loss = 20.7471\n",
      "Recon Loss = 7.5024, KL Loss = 0.7458, Lagrangian Loss = 13.1897\n",
      "Epoch 1051: Loss = 19.5238\n",
      "Recon Loss = 6.9542, KL Loss = 0.7205, Lagrangian Loss = 13.3619\n",
      "Early stopping triggered. Last Epoch: 1070\n",
      "Recon Loss = 7.2012, KL Loss = 0.6918, Lagrangian Loss = 13.4505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 04:32:34,255 INFO -- Edge Accuracy = 33.33, Instantaneous Accuracy = 33.33, Lagged Accuracy = 33.33, Counterfactual Accuracy = 66.67,  Blended Accuracy = 100.00,  RR Accuracy = 33.33  \n",
      "2025-03-28 04:32:34,255 INFO -- Model: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly: ced1\n",
      "Epoch 1: Loss = 56480.9665\n",
      "Recon Loss = 3095.9280, KL Loss = 0.0000, Lagrangian Loss = 61934.1914\n",
      "Epoch 51: Loss = 33586.4381\n",
      "Recon Loss = 238.4113, KL Loss = 0.0243, Lagrangian Loss = 38828.3320\n",
      "Epoch 101: Loss = 22125.9127\n",
      "Recon Loss = 151.5302, KL Loss = 0.0763, Lagrangian Loss = 25559.7188\n",
      "Epoch 151: Loss = 13553.6137\n",
      "Recon Loss = 93.6425, KL Loss = 0.1426, Lagrangian Loss = 15647.6338\n",
      "Epoch 201: Loss = 8222.6306\n",
      "Recon Loss = 69.0229, KL Loss = 0.2086, Lagrangian Loss = 9551.6074\n",
      "Epoch 251: Loss = 5294.5523\n",
      "Recon Loss = 53.8044, KL Loss = 0.2682, Lagrangian Loss = 6046.8496\n",
      "Epoch 301: Loss = 3567.1948\n",
      "Recon Loss = 45.7596, KL Loss = 0.3373, Lagrangian Loss = 4056.2471\n",
      "Epoch 351: Loss = 2553.5840\n",
      "Recon Loss = 48.9694, KL Loss = 0.4041, Lagrangian Loss = 2868.8315\n",
      "Epoch 401: Loss = 1889.5274\n",
      "Recon Loss = 43.5106, KL Loss = 0.4530, Lagrangian Loss = 2113.3955\n",
      "Epoch 451: Loss = 1433.5232\n",
      "Recon Loss = 42.2229, KL Loss = 0.5087, Lagrangian Loss = 1621.8904\n",
      "Epoch 501: Loss = 1113.6240\n",
      "Recon Loss = 38.5786, KL Loss = 0.5465, Lagrangian Loss = 1229.3873\n",
      "Epoch 551: Loss = 875.6145\n",
      "Recon Loss = 42.8290, KL Loss = 0.5477, Lagrangian Loss = 1003.9441\n",
      "Epoch 601: Loss = 661.7451\n",
      "Recon Loss = 38.2063, KL Loss = 0.5743, Lagrangian Loss = 737.4946\n",
      "Epoch 651: Loss = 510.2848\n",
      "Recon Loss = 24.3759, KL Loss = 0.5805, Lagrangian Loss = 562.1501\n",
      "Epoch 701: Loss = 400.5565\n",
      "Recon Loss = 32.0355, KL Loss = 0.5628, Lagrangian Loss = 447.4052\n",
      "Epoch 751: Loss = 325.3821\n",
      "Recon Loss = 36.3132, KL Loss = 0.5149, Lagrangian Loss = 334.9836\n",
      "Epoch 801: Loss = 269.7481\n",
      "Recon Loss = 30.2720, KL Loss = 0.4640, Lagrangian Loss = 278.1429\n",
      "Epoch 851: Loss = 229.0124\n",
      "Recon Loss = 48.1546, KL Loss = 0.4264, Lagrangian Loss = 237.3621\n",
      "Epoch 901: Loss = 196.1308\n",
      "Recon Loss = 29.0139, KL Loss = 0.4007, Lagrangian Loss = 191.3160\n",
      "Epoch 951: Loss = 170.3665\n",
      "Recon Loss = 24.0527, KL Loss = 0.3617, Lagrangian Loss = 164.5017\n",
      "Epoch 1001: Loss = 150.1763\n",
      "Recon Loss = 26.9773, KL Loss = 0.3284, Lagrangian Loss = 145.2748\n",
      "Epoch 1051: Loss = 135.6479\n",
      "Recon Loss = 29.0021, KL Loss = 0.3111, Lagrangian Loss = 128.6059\n",
      "Epoch 1101: Loss = 119.9348\n",
      "Recon Loss = 30.0298, KL Loss = 0.3002, Lagrangian Loss = 110.4064\n",
      "Epoch 1151: Loss = 111.6895\n",
      "Recon Loss = 27.0568, KL Loss = 0.2780, Lagrangian Loss = 99.3048\n",
      "Epoch 1201: Loss = 100.4150\n",
      "Recon Loss = 29.9434, KL Loss = 0.2654, Lagrangian Loss = 87.9189\n",
      "Epoch 1251: Loss = 91.5437\n",
      "Recon Loss = 30.6505, KL Loss = 0.2581, Lagrangian Loss = 78.1643\n",
      "Epoch 1301: Loss = 85.4528\n",
      "Recon Loss = 27.7174, KL Loss = 0.2514, Lagrangian Loss = 70.9967\n",
      "Epoch 1351: Loss = 80.8720\n",
      "Recon Loss = 39.3716, KL Loss = 0.2380, Lagrangian Loss = 66.2354\n",
      "Epoch 1401: Loss = 73.8672\n",
      "Recon Loss = 30.4209, KL Loss = 0.2501, Lagrangian Loss = 59.5400\n",
      "Epoch 1451: Loss = 69.4046\n",
      "Recon Loss = 33.4277, KL Loss = 0.2212, Lagrangian Loss = 53.9758\n",
      "Epoch 1501: Loss = 67.6721\n",
      "Recon Loss = 36.3985, KL Loss = 0.2014, Lagrangian Loss = 50.3522\n",
      "Epoch 1551: Loss = 62.7914\n",
      "Recon Loss = 28.8256, KL Loss = 0.2138, Lagrangian Loss = 45.8703\n",
      "Epoch 1601: Loss = 61.0181\n",
      "Recon Loss = 27.7566, KL Loss = 0.2027, Lagrangian Loss = 43.2863\n",
      "Epoch 1651: Loss = 58.3853\n",
      "Recon Loss = 32.6473, KL Loss = 0.1854, Lagrangian Loss = 40.0338\n",
      "Epoch 1701: Loss = 54.3851\n",
      "Recon Loss = 36.1515, KL Loss = 0.2142, Lagrangian Loss = 38.0486\n",
      "Epoch 1751: Loss = 52.1777\n",
      "Recon Loss = 21.2414, KL Loss = 0.1981, Lagrangian Loss = 36.1807\n",
      "Epoch 1801: Loss = 51.1201\n",
      "Recon Loss = 30.8881, KL Loss = 0.2275, Lagrangian Loss = 34.8985\n",
      "Epoch 1851: Loss = 49.5511\n",
      "Recon Loss = 23.8308, KL Loss = 0.2230, Lagrangian Loss = 33.3826\n",
      "Epoch 1901: Loss = 47.4323\n",
      "Recon Loss = 17.6470, KL Loss = 0.2202, Lagrangian Loss = 31.9087\n",
      "Epoch 1951: Loss = 45.1687\n",
      "Recon Loss = 25.4678, KL Loss = 0.2084, Lagrangian Loss = 30.8549\n",
      "Epoch 2001: Loss = 45.1595\n",
      "Recon Loss = 21.1929, KL Loss = 0.2106, Lagrangian Loss = 30.0541\n",
      "Early stopping triggered. Last Epoch: 2019\n",
      "Recon Loss = 16.9267, KL Loss = 0.2139, Lagrangian Loss = 29.9567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 06:32:15,022 INFO -- Edge Accuracy = 50.00, Instantaneous Accuracy = 25.00, Lagged Accuracy = 50.00, Counterfactual Accuracy = 75.00,  Blended Accuracy = 100.00,  RR Accuracy = 50.00  \n",
      "2025-03-28 06:32:15,023 INFO -- Model: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly: cfo1\n",
      "Epoch 1: Loss = 25480.3125\n",
      "Recon Loss = 2223.8022, KL Loss = 0.0000, Lagrangian Loss = 27126.4414\n",
      "Epoch 51: Loss = 15322.9607\n",
      "Recon Loss = 291.0033, KL Loss = 0.0430, Lagrangian Loss = 17559.3125\n",
      "Epoch 101: Loss = 9150.1963\n",
      "Recon Loss = 162.7530, KL Loss = 0.1057, Lagrangian Loss = 10365.7695\n",
      "Epoch 151: Loss = 5618.0132\n",
      "Recon Loss = 118.6054, KL Loss = 0.1904, Lagrangian Loss = 6402.7637\n",
      "Epoch 201: Loss = 3623.9147\n",
      "Recon Loss = 91.5971, KL Loss = 0.2791, Lagrangian Loss = 4039.7654\n",
      "Epoch 251: Loss = 2496.5702\n",
      "Recon Loss = 79.9366, KL Loss = 0.3845, Lagrangian Loss = 2894.7932\n",
      "Epoch 301: Loss = 1800.7126\n",
      "Recon Loss = 73.6155, KL Loss = 0.5096, Lagrangian Loss = 2050.0212\n",
      "Epoch 351: Loss = 1335.9071\n",
      "Recon Loss = 57.2595, KL Loss = 0.5860, Lagrangian Loss = 1529.8342\n",
      "Epoch 401: Loss = 1023.0784\n",
      "Recon Loss = 62.2438, KL Loss = 0.7112, Lagrangian Loss = 1125.5670\n",
      "Epoch 451: Loss = 803.4954\n",
      "Recon Loss = 58.6227, KL Loss = 0.8200, Lagrangian Loss = 875.5508\n",
      "Epoch 501: Loss = 640.0473\n",
      "Recon Loss = 47.4889, KL Loss = 0.8933, Lagrangian Loss = 710.2193\n",
      "Epoch 551: Loss = 529.2065\n",
      "Recon Loss = 44.0148, KL Loss = 0.9195, Lagrangian Loss = 563.1849\n",
      "Epoch 601: Loss = 431.8619\n",
      "Recon Loss = 47.1024, KL Loss = 0.9790, Lagrangian Loss = 466.9411\n",
      "Epoch 651: Loss = 363.2488\n",
      "Recon Loss = 44.1140, KL Loss = 0.9645, Lagrangian Loss = 383.5587\n",
      "Epoch 701: Loss = 308.9840\n",
      "Recon Loss = 37.9402, KL Loss = 1.0274, Lagrangian Loss = 317.8083\n",
      "Epoch 751: Loss = 263.1673\n",
      "Recon Loss = 37.9071, KL Loss = 0.9669, Lagrangian Loss = 259.3363\n",
      "Epoch 801: Loss = 226.3074\n",
      "Recon Loss = 33.0919, KL Loss = 0.9178, Lagrangian Loss = 223.7194\n",
      "Epoch 851: Loss = 198.8022\n",
      "Recon Loss = 36.9472, KL Loss = 0.8650, Lagrangian Loss = 192.4114\n",
      "Epoch 901: Loss = 173.9875\n",
      "Recon Loss = 36.5465, KL Loss = 0.8219, Lagrangian Loss = 171.9594\n",
      "Epoch 951: Loss = 152.3937\n",
      "Recon Loss = 35.5864, KL Loss = 0.8145, Lagrangian Loss = 144.6406\n",
      "Epoch 1001: Loss = 135.1373\n",
      "Recon Loss = 24.2338, KL Loss = 0.7603, Lagrangian Loss = 129.0443\n",
      "Epoch 1051: Loss = 122.0370\n",
      "Recon Loss = 27.6931, KL Loss = 0.7268, Lagrangian Loss = 112.0694\n",
      "Epoch 1101: Loss = 109.2066\n",
      "Recon Loss = 22.0553, KL Loss = 0.6903, Lagrangian Loss = 101.4656\n",
      "Epoch 1151: Loss = 98.6703\n",
      "Recon Loss = 29.8380, KL Loss = 0.6404, Lagrangian Loss = 87.4935\n",
      "Epoch 1201: Loss = 88.7623\n",
      "Recon Loss = 23.4059, KL Loss = 0.6118, Lagrangian Loss = 78.2904\n",
      "Epoch 1251: Loss = 81.1275\n",
      "Recon Loss = 18.4562, KL Loss = 0.5882, Lagrangian Loss = 73.8846\n",
      "Epoch 1301: Loss = 74.7629\n",
      "Recon Loss = 23.4610, KL Loss = 0.5678, Lagrangian Loss = 66.1209\n",
      "Epoch 1351: Loss = 67.6393\n",
      "Recon Loss = 21.3697, KL Loss = 0.5516, Lagrangian Loss = 59.0387\n",
      "Epoch 1401: Loss = 62.3258\n",
      "Recon Loss = 16.1384, KL Loss = 0.5161, Lagrangian Loss = 55.1924\n",
      "Epoch 1451: Loss = 58.8878\n",
      "Recon Loss = 19.0571, KL Loss = 0.5101, Lagrangian Loss = 51.1722\n",
      "Epoch 1501: Loss = 55.5731\n",
      "Recon Loss = 16.9821, KL Loss = 0.5223, Lagrangian Loss = 46.6022\n",
      "Epoch 1551: Loss = 50.8393\n",
      "Recon Loss = 15.2357, KL Loss = 0.5139, Lagrangian Loss = 43.3791\n",
      "Epoch 1601: Loss = 46.8305\n",
      "Recon Loss = 15.2709, KL Loss = 0.5292, Lagrangian Loss = 40.9887\n",
      "Epoch 1651: Loss = 44.8745\n",
      "Recon Loss = 18.5557, KL Loss = 0.5327, Lagrangian Loss = 37.8656\n",
      "Epoch 1701: Loss = 42.2650\n",
      "Recon Loss = 13.2292, KL Loss = 0.5151, Lagrangian Loss = 36.0910\n",
      "Epoch 1751: Loss = 40.2505\n",
      "Recon Loss = 7.4292, KL Loss = 0.5000, Lagrangian Loss = 34.4455\n",
      "Epoch 1801: Loss = 38.6266\n",
      "Recon Loss = 15.6491, KL Loss = 0.5521, Lagrangian Loss = 32.6946\n",
      "Epoch 1851: Loss = 36.2223\n",
      "Recon Loss = 7.3319, KL Loss = 0.5043, Lagrangian Loss = 31.2437\n",
      "Epoch 1901: Loss = 35.2519\n",
      "Recon Loss = 13.1300, KL Loss = 0.5691, Lagrangian Loss = 30.2044\n",
      "Epoch 1951: Loss = 34.2376\n",
      "Recon Loss = 10.3860, KL Loss = 0.5261, Lagrangian Loss = 29.6215\n",
      "Early stopping triggered. Last Epoch: 1962\n",
      "Recon Loss = 9.5683, KL Loss = 0.4683, Lagrangian Loss = 29.5631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 08:28:18,296 INFO -- Edge Accuracy = 60.00, Instantaneous Accuracy = 40.00, Lagged Accuracy = 60.00, Counterfactual Accuracy = 80.00,  Blended Accuracy = 100.00,  RR Accuracy = 60.00  \n",
      "2025-03-28 08:28:18,297 INFO -- Model: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly: cfo1\n",
      "Epoch 1: Loss = 40493.1500\n",
      "Recon Loss = 2348.5393, KL Loss = 0.0000, Lagrangian Loss = 39699.7070\n",
      "Epoch 51: Loss = 11206.3636\n",
      "Recon Loss = 134.2639, KL Loss = 0.0513, Lagrangian Loss = 11746.1641\n",
      "Epoch 101: Loss = 4107.0412\n",
      "Recon Loss = 59.0447, KL Loss = 0.1301, Lagrangian Loss = 4296.3052\n",
      "Epoch 151: Loss = 1686.3780\n",
      "Recon Loss = 47.6314, KL Loss = 0.1466, Lagrangian Loss = 1747.9731\n",
      "Epoch 201: Loss = 841.2046\n",
      "Recon Loss = 45.8567, KL Loss = 0.2183, Lagrangian Loss = 850.1748\n",
      "Epoch 251: Loss = 482.7141\n",
      "Recon Loss = 42.4154, KL Loss = 0.3256, Lagrangian Loss = 471.8865\n",
      "Epoch 301: Loss = 314.9741\n",
      "Recon Loss = 31.9251, KL Loss = 0.4045, Lagrangian Loss = 294.0592\n",
      "Epoch 351: Loss = 221.1140\n",
      "Recon Loss = 47.2993, KL Loss = 0.4129, Lagrangian Loss = 196.0555\n",
      "Epoch 401: Loss = 162.7727\n",
      "Recon Loss = 32.7919, KL Loss = 0.4087, Lagrangian Loss = 139.1293\n",
      "Epoch 451: Loss = 124.2073\n",
      "Recon Loss = 28.7994, KL Loss = 0.4012, Lagrangian Loss = 100.3740\n",
      "Epoch 501: Loss = 97.9532\n",
      "Recon Loss = 30.3830, KL Loss = 0.3834, Lagrangian Loss = 74.5570\n",
      "Epoch 551: Loss = 79.9687\n",
      "Recon Loss = 27.6113, KL Loss = 0.3576, Lagrangian Loss = 57.0351\n",
      "Epoch 601: Loss = 66.7351\n",
      "Recon Loss = 28.2331, KL Loss = 0.3550, Lagrangian Loss = 45.3070\n",
      "Epoch 651: Loss = 57.1405\n",
      "Recon Loss = 19.8203, KL Loss = 0.3330, Lagrangian Loss = 36.8946\n",
      "Epoch 701: Loss = 48.8747\n",
      "Recon Loss = 23.2592, KL Loss = 0.3126, Lagrangian Loss = 30.6117\n",
      "Epoch 751: Loss = 43.0908\n",
      "Recon Loss = 14.1008, KL Loss = 0.3016, Lagrangian Loss = 25.7981\n",
      "Epoch 801: Loss = 38.9206\n",
      "Recon Loss = 18.4392, KL Loss = 0.2983, Lagrangian Loss = 22.2862\n",
      "Epoch 851: Loss = 34.6268\n",
      "Recon Loss = 18.8182, KL Loss = 0.2744, Lagrangian Loss = 20.1373\n",
      "Epoch 901: Loss = 31.7077\n",
      "Recon Loss = 15.4046, KL Loss = 0.2463, Lagrangian Loss = 18.0424\n",
      "Epoch 951: Loss = 30.2199\n",
      "Recon Loss = 15.0340, KL Loss = 0.3691, Lagrangian Loss = 16.8123\n",
      "Epoch 1001: Loss = 27.2181\n",
      "Recon Loss = 12.0773, KL Loss = 0.3021, Lagrangian Loss = 15.9469\n",
      "Epoch 1051: Loss = 25.5092\n",
      "Recon Loss = 10.2771, KL Loss = 0.3088, Lagrangian Loss = 15.3751\n",
      "Epoch 1101: Loss = 24.2164\n",
      "Recon Loss = 13.1268, KL Loss = 0.2839, Lagrangian Loss = 14.9883\n",
      "Epoch 1151: Loss = 23.3782\n",
      "Recon Loss = 10.6622, KL Loss = 0.2660, Lagrangian Loss = 14.8422\n",
      "Early stopping triggered. Last Epoch: 1173\n",
      "Recon Loss = 9.1850, KL Loss = 0.3047, Lagrangian Loss = 14.8155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 11:10:49,471 INFO -- Edge Accuracy = 66.67, Instantaneous Accuracy = 50.00, Lagged Accuracy = 50.00, Counterfactual Accuracy = 66.67,  Blended Accuracy = 100.00,  RR Accuracy = 50.00  \n",
      "2025-03-28 11:10:49,472 INFO -- Model: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly: cso1\n",
      "Epoch 1: Loss = 37500.1094\n",
      "Recon Loss = 2737.5425, KL Loss = 0.0000, Lagrangian Loss = 40637.1797\n",
      "Epoch 51: Loss = 31147.4046\n",
      "Recon Loss = 258.9304, KL Loss = 0.0484, Lagrangian Loss = 36042.4609\n",
      "Epoch 101: Loss = 27147.9037\n",
      "Recon Loss = 152.1610, KL Loss = 0.1264, Lagrangian Loss = 31456.7324\n",
      "Epoch 151: Loss = 22384.7804\n",
      "Recon Loss = 122.6846, KL Loss = 0.2073, Lagrangian Loss = 25928.9785\n",
      "Epoch 201: Loss = 17066.4051\n",
      "Recon Loss = 90.3979, KL Loss = 0.3293, Lagrangian Loss = 19804.0605\n",
      "Epoch 251: Loss = 12522.3465\n",
      "Recon Loss = 85.9347, KL Loss = 0.4476, Lagrangian Loss = 14393.8584\n",
      "Epoch 301: Loss = 8623.7861\n",
      "Recon Loss = 73.8880, KL Loss = 0.5582, Lagrangian Loss = 9799.8887\n",
      "Epoch 351: Loss = 5974.3387\n",
      "Recon Loss = 68.4404, KL Loss = 0.6780, Lagrangian Loss = 6908.8325\n",
      "Epoch 401: Loss = 4231.7289\n",
      "Recon Loss = 61.8500, KL Loss = 0.7681, Lagrangian Loss = 4831.4316\n",
      "Epoch 451: Loss = 3140.3872\n",
      "Recon Loss = 65.2844, KL Loss = 0.8569, Lagrangian Loss = 3656.6807\n",
      "Epoch 501: Loss = 2311.9948\n",
      "Recon Loss = 54.4233, KL Loss = 0.9008, Lagrangian Loss = 2645.8328\n",
      "Epoch 551: Loss = 1798.7975\n",
      "Recon Loss = 56.7139, KL Loss = 0.9485, Lagrangian Loss = 2065.9436\n",
      "Epoch 601: Loss = 1407.3813\n",
      "Recon Loss = 64.2870, KL Loss = 0.9535, Lagrangian Loss = 1613.9688\n",
      "Epoch 651: Loss = 1119.5557\n",
      "Recon Loss = 46.3143, KL Loss = 0.9450, Lagrangian Loss = 1220.6219\n",
      "Epoch 701: Loss = 913.5467\n",
      "Recon Loss = 51.0317, KL Loss = 0.8953, Lagrangian Loss = 1012.9037\n",
      "Epoch 751: Loss = 745.8596\n",
      "Recon Loss = 57.2684, KL Loss = 0.8620, Lagrangian Loss = 816.7711\n",
      "Epoch 801: Loss = 607.8429\n",
      "Recon Loss = 42.2542, KL Loss = 0.7724, Lagrangian Loss = 681.9973\n",
      "Epoch 851: Loss = 479.0063\n",
      "Recon Loss = 57.4347, KL Loss = 0.7047, Lagrangian Loss = 507.8268\n",
      "Epoch 901: Loss = 378.7078\n",
      "Recon Loss = 51.4595, KL Loss = 0.6325, Lagrangian Loss = 397.3244\n",
      "Epoch 951: Loss = 308.7921\n",
      "Recon Loss = 48.3810, KL Loss = 0.5912, Lagrangian Loss = 319.4460\n",
      "Epoch 1001: Loss = 256.8029\n",
      "Recon Loss = 64.0948, KL Loss = 0.5345, Lagrangian Loss = 261.9517\n",
      "Epoch 1051: Loss = 215.7973\n",
      "Recon Loss = 42.5913, KL Loss = 0.4908, Lagrangian Loss = 211.7091\n",
      "Epoch 1101: Loss = 185.1298\n",
      "Recon Loss = 33.9845, KL Loss = 0.4597, Lagrangian Loss = 171.9113\n",
      "Epoch 1151: Loss = 161.6965\n",
      "Recon Loss = 34.5998, KL Loss = 0.4247, Lagrangian Loss = 149.2484\n",
      "Epoch 1201: Loss = 143.6161\n",
      "Recon Loss = 40.2871, KL Loss = 0.3930, Lagrangian Loss = 129.6181\n"
     ]
    }
   ],
   "source": [
    "cols = list(test_df.columns)\n",
    "non_causal_columns = list(set(cols).difference(set(potential_causes)))\n",
    "causal_indices = [train_df.columns.get_loc(col) for col in potential_causes]\n",
    "non_causal_indices = [train_df.columns.get_loc(col) for col in non_causal_columns]\n",
    "num_nodes = len(train_df.columns)\n",
    "BATCH_SIZE = 50\n",
    "device = torch.device('cpu')\n",
    "\n",
    "\n",
    "new_metadata = []\n",
    "edge_correct = 0\n",
    "instantaneous_correct = 0\n",
    "lagged_correct = 0\n",
    "counterfactual_correct = 0 \n",
    "rr_correct = 0\n",
    "total_correct = 0\n",
    "total_checked = 0\n",
    "incorrect = []\n",
    "\n",
    "for i, row in enumerate(cats_rows_list):\n",
    "    total_checked +=1 \n",
    "    logger.info('Model: '+ str(i))\n",
    "    anomaly = eval(row['affected'])[0]\n",
    "    print('Anomaly: ' + anomaly)\n",
    "    anomaly_time = datetime.strptime(row['start_time'],\"%Y-%m-%d %H:%M:%S\")\n",
    "    #start_time = datetime.strptime(row['start_time'],\"%Y-%m-%d %H:%M:%S\")\n",
    "    end_time = datetime.strptime(row['end_time'],\"%Y-%m-%d %H:%M:%S\")\n",
    "    root_cause = row['root_cause']\n",
    "    #start_len = mod_df.shape[0]\n",
    "    #if start_len >1000:\n",
    "        #start_len = 1000\n",
    "    start_len = 8\n",
    "    start_time = anomaly_time- timedelta(seconds=start_len)\n",
    "    finish_time = end_time + timedelta(seconds=start_len)\n",
    "    mod_df = test_df[(test_df.index>= start_time) & (test_df.index<= finish_time)]\n",
    "    mod_df = mod_df[['aimp', 'amud', 'arnd', 'asin1', 'asin2', 'adbr', 'adfl', 'bed1',\n",
    "       'bed2', 'bfo1', 'bfo2', 'bso1', 'bso2', 'bso3', 'ced1', 'cfo1', 'cso1']]\n",
    "\n",
    "    \"\"\"\n",
    "    FIND THE OPTIMAL NUMBER OF LAGS\n",
    "    \"\"\"\n",
    "    TIME_STEPS = 8#max(most_frequent(find_optimal_lags_for_dataframe(mod_df))+1,3)\n",
    "\n",
    "    dataset = TimeSeriesDataset(mod_df,device=device, time_steps=TIME_STEPS)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    X_data = torch.empty(0,device=device)\n",
    "    T_data = torch.empty(0,device=device)\n",
    "    for batch_idx, (X_batch, time_batch) in enumerate(dataloader):\n",
    "        X_data = torch.cat((X_data[:batch_idx], X_batch, X_data[batch_idx:]))\n",
    "        T_data = torch.cat((T_data[:batch_idx], time_batch, T_data[batch_idx:]))\n",
    "    \n",
    "    fine_tuned = CausalGraphVAE(input_dim=num_nodes, hidden_dim=hidden_dim,\n",
    "                           latent_dim=latent_dim, num_nodes=num_nodes,device=device,\n",
    "                           time_steps=TIME_STEPS,prior_adj=prior_adj.to(device),instantaneous_weight=0.5)\n",
    "    optimizer = torch.optim.AdamW(fine_tuned.parameters(), lr=0.0001, weight_decay=1e-3)  # L2 Regularization\n",
    "\n",
    "    \n",
    "    fine_tuned.train_model(dataloader, optimizer, num_epochs=2500, patience=20,BATCH_SIZE=BATCH_SIZE,rho_max=2.0,alpha_max=2.0)\n",
    "\n",
    "    causes = fine_tuned.infer_causal_effect(X_data.to(torch.float32).to(device),T_data.to(torch.float32).to(device),anomaly,cols,non_causal_indices=non_causal_indices)\n",
    "    \n",
    "    causes = causes.filter(items=potential_causes, axis=0)\n",
    "    edge_cause_1 = causes.sort_values(by='causes',ascending=False)[0:3].index[0]\n",
    "    edge_cause_2 = causes.sort_values(by='causes',ascending=False)[0:3].index[1]\n",
    "    edge_cause_3 = causes.sort_values(by='causes',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    instant_cause_1 = causes.sort_values(by='instantaneous',ascending=False)[0:3].index[0]\n",
    "    instant_cause_2 = causes.sort_values(by='instantaneous',ascending=False)[0:3].index[1]\n",
    "    instant_cause_3 = causes.sort_values(by='instantaneous',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    lag_cause_1 = causes.sort_values(by='lagged',ascending=False)[0:3].index[0]\n",
    "    lag_cause_2 = causes.sort_values(by='lagged',ascending=False)[0:3].index[1]\n",
    "    lag_cause_3 = causes.sort_values(by='lagged',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    counterfactual_cause_1 = causes.sort_values(by='counterfactuals',ascending=False)[0:3].index[0]\n",
    "    counterfactual_cause_2 = causes.sort_values(by='counterfactuals',ascending=False)[0:3].index[1]\n",
    "    counterfactual_cause_3 = causes.sort_values(by='counterfactuals',ascending=False)[0:3].index[2]\n",
    "\n",
    "    rr_cause_1 = causes.sort_values(by='RootRank',ascending=False)[0:3].index[0]\n",
    "    rr_cause_2 = causes.sort_values(by='RootRank',ascending=False)[0:3].index[1]\n",
    "    rr_cause_3 = causes.sort_values(by='RootRank',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    total_score_cause_1=causes.sort_values(by='causal_strength',ascending=False)[0:3].index[0]\n",
    "    total_score_cause_2=causes.sort_values(by='causal_strength',ascending=False)[0:3].index[1]\n",
    "    total_score_cause_3=causes.sort_values(by='causal_strength',ascending=False)[0:3].index[2]\n",
    "    \n",
    "    if root_cause == edge_cause_1:\n",
    "        row['edge_cause_1'] = 1\n",
    "    if root_cause == edge_cause_1:\n",
    "        row['edge_cause_2'] = 1\n",
    "    if root_cause == edge_cause_1:\n",
    "        row['edge_cause_3'] = 1\n",
    "    \n",
    "    if root_cause == counterfactual_cause_1:\n",
    "        row['counterfactual_cause_1'] = 1\n",
    "    if root_cause == counterfactual_cause_2:\n",
    "        row['counterfactual_cause_2'] = 1\n",
    "    if root_cause == counterfactual_cause_3:\n",
    "        row['counterfactual_cause_3'] = 1\n",
    "    \n",
    "    if root_cause == total_score_cause_1:\n",
    "        row['total_score_cause_1'] = 1\n",
    "    if root_cause == total_score_cause_2:\n",
    "        row['total_score_cause_2'] = 1\n",
    "    if root_cause == total_score_cause_3:\n",
    "        row['total_score_cause_3'] = 1\n",
    "    \n",
    "    if root_cause == instant_cause_1:\n",
    "        row['instant_cause_1'] = 1\n",
    "    if root_cause == instant_cause_2:\n",
    "        row['instant_cause_2'] = 1\n",
    "    if root_cause == instant_cause_3:\n",
    "        row['instant_cause_3'] = 1\n",
    "    \n",
    "    if root_cause == lag_cause_1:\n",
    "        row['lag_cause_1'] = 1\n",
    "    if root_cause == lag_cause_2:\n",
    "        row['lag_cause_2'] = 1\n",
    "    if root_cause == lag_cause_3:\n",
    "        row['lag_cause_3'] = 1\n",
    "\n",
    "    if root_cause == rr_cause_1:\n",
    "        row['rr_cause_1'] = 1\n",
    "    if root_cause == rr_cause_2:\n",
    "        row['rr_cause_2'] = 1\n",
    "    if root_cause == rr_cause_3:\n",
    "        row['rr_cause_3'] = 1\n",
    "    \n",
    "    new_metadata.append(row)\n",
    "    \n",
    "    if root_cause in [total_score_cause_1 , total_score_cause_2 , total_score_cause_3]:\n",
    "        total_correct+=1\n",
    "    if root_cause in [edge_cause_1 , edge_cause_2 , edge_cause_3]:\n",
    "        edge_correct+=1\n",
    "    if root_cause in [counterfactual_cause_1 , counterfactual_cause_2 , counterfactual_cause_3]:\n",
    "        counterfactual_correct+=1\n",
    "    if root_cause in [instant_cause_1 , instant_cause_2 , instant_cause_3]:\n",
    "        instantaneous_correct+=1\n",
    "    if root_cause in [lag_cause_1 , lag_cause_2 , lag_cause_3]:\n",
    "        lagged_correct+=1\n",
    "    if root_cause in [rr_cause_1 , rr_cause_2 , rr_cause_3]:\n",
    "        rr_correct+=1\n",
    "    \n",
    "    total_accuracy = total_correct/total_checked* 100\n",
    "    edge_accuracy = edge_correct/total_checked* 100\n",
    "    cf_accuracy = counterfactual_correct/total_checked* 100\n",
    "    instant_accuracy = instantaneous_correct/total_checked* 100\n",
    "    lag_accuracy = lagged_correct/total_checked* 100\n",
    "    rr_accuracy = rr_correct/total_checked* 100\n",
    "    \n",
    "    if root_cause not in [total_score_cause_1 , total_score_cause_2 , total_score_cause_3,edge_cause_1 , edge_cause_2 , edge_cause_3 ]:\n",
    "        incorrect.append(i) \n",
    "    logger.info(f\"Edge Accuracy = {edge_accuracy:.2f}, Instantaneous Accuracy = {instant_accuracy:.2f}, Lagged Accuracy = {lag_accuracy:.2f}, Counterfactual Accuracy = {cf_accuracy:.2f},  Blended Accuracy = {total_accuracy:.2f},  RR Accuracy = {rr_accuracy:.2f}  \") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab3a638-f7c1-4ba2-ba66-380c14575be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20161b14-2938-4fa8-98c8-b34e7ed63141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eabbf0-6cbb-4b3b-8a2e-e92cfec228bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
